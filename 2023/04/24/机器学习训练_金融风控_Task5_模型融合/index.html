<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="序模型融合是比赛后期上分的重要手段，特别是多人组队学习的比赛中，将不同队友的模型进行融合，可能会收获意想不到的效果哦，往往模型相差越大且模型表现都不错的前提下，模型融合后结果会有大幅提升。 # 简单加权平均-结果直接融合123456789&amp;#x27;&amp;#x27;&amp;#x27;生成一些简单的样本数据,test_prei - 代表第i个模型的预测值y_test_true - 代表真实值&amp;#x27;&amp;#x">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习训练_金融风控_Task5_模型融合">
<meta property="og:url" content="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/index.html">
<meta property="og:site_name" content="大宝的树屋">
<meta property="og:description" content="序模型融合是比赛后期上分的重要手段，特别是多人组队学习的比赛中，将不同队友的模型进行融合，可能会收获意想不到的效果哦，往往模型相差越大且模型表现都不错的前提下，模型融合后结果会有大幅提升。 # 简单加权平均-结果直接融合123456789&amp;#x27;&amp;#x27;&amp;#x27;生成一些简单的样本数据,test_prei - 代表第i个模型的预测值y_test_true - 代表真实值&amp;#x27;&amp;#x">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2023-04-24T08:04:28.388Z">
<meta property="article:modified_time" content="2023-04-24T08:18:27.375Z">
<meta property="article:author" content="小书包">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="金融风控">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习训练_金融风控_Task5_模型融合 | 大宝的树屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

	<a target="_blank" rel="noopener" href="https://your-url" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">大宝的树屋</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="小书包">
      <meta itemprop="description" content="种一棵树最好的时间是十年前，其次是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大宝的树屋">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习训练_金融风控_Task5_模型融合
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-24 16:04:28 / 修改时间：16:18:27" itemprop="dateCreated datePublished" datetime="2023-04-24T16:04:28+08:00">2023-04-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">金融风控</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>32 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><p>模型融合是比赛后期上分的重要手段，特别是多人组队学习的比赛中，将不同队友的模型进行融合，可能会收获意想不到的效果哦，往往模型相差越大且模型表现都不错的前提下，模型融合后结果会有大幅提升。</p>
<h2 id="简单加权平均-结果直接融合"><a href="#简单加权平均-结果直接融合" class="headerlink" title="# 简单加权平均-结果直接融合"></a># 简单加权平均-结果直接融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">生成一些简单的样本数据,</span></span><br><span class="line"><span class="string">test_prei - 代表第i个模型的预测值</span></span><br><span class="line"><span class="string">y_test_true - 代表真实值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<h2 id="定义结果的加权平均函数-根据加权计算"><a href="#定义结果的加权平均函数-根据加权计算" class="headerlink" title="# 定义结果的加权平均函数 - 根据加权计算"></a># 定义结果的加权平均函数 - 根据加权计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_method</span>(<span class="params">test_pre1, test_pre2, test_pre3, w=[<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>]</span>):</span><br><span class="line">    weighted_result = w[<span class="number">0</span>] * pd.Series(test_pre1) + w[<span class="number">1</span>] * pd.Series(test_pre2) + w[<span class="number">2</span>] * pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> weighted_result</span><br></pre></td></tr></table></figure>
<h2 id="根据各模型的预测结果计算MAE"><a href="#根据各模型的预测结果计算MAE" class="headerlink" title="# 根据各模型的预测结果计算MAE"></a># 根据各模型的预测结果计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">metrics.mean_absolute_error - 多维数组MAE的计算方法</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred1 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred2 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred3 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre3))</span><br><span class="line"></span><br><span class="line">Pred1 MAE: <span class="number">0.1750000000000001</span></span><br><span class="line">Pred2 MAE: <span class="number">0.07499999999999993</span></span><br><span class="line">Pred3 MAE: <span class="number">0.10000000000000009</span></span><br></pre></td></tr></table></figure>
<h2 id="根据加权计算MAE"><a href="#根据加权计算MAE" class="headerlink" title="# 根据加权计算MAE"></a># 根据加权计算MAE</h2><h2 id="定义比重权值"><a href="#定义比重权值" class="headerlink" title="## 定义比重权值"></a>## 定义比重权值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]</span><br><span class="line">weighted_pre = weighted_method(test_pre1, test_pre2, test_pre3, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Weighted_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, weighted_pre))</span><br><span class="line"></span><br><span class="line">Weighted_pre MAE: <span class="number">0.05750000000000027</span></span><br></pre></td></tr></table></figure>
<h3 id="定义结果的加权平均函数-mean平均"><a href="#定义结果的加权平均函数-mean平均" class="headerlink" title="# 定义结果的加权平均函数 - mean平均"></a># 定义结果的加权平均函数 - mean平均</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    mean_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                             pd.Series(test_pre2),</span><br><span class="line">                             pd.Series(test_pre3)], axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mean_result</span><br></pre></td></tr></table></figure>
<h2 id="根据均值计算MAE"><a href="#根据均值计算MAE" class="headerlink" title="# 根据均值计算MAE"></a># 根据均值计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Mean_pre = mean_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Mean_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Mean_pre MAE: <span class="number">0.06666666666666693</span></span><br></pre></td></tr></table></figure>
<h2 id="定义结果的加权平均函数-median平均"><a href="#定义结果的加权平均函数-median平均" class="headerlink" title="# 定义结果的加权平均函数 - median平均"></a># 定义结果的加权平均函数 - median平均</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">median_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    median_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                               pd.Series(test_pre2),</span><br><span class="line">                               pd.Series(test_pre3)], axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> median_result</span><br></pre></td></tr></table></figure>
<h2 id="根据中位数计算MAE"><a href="#根据中位数计算MAE" class="headerlink" title="# 根据中位数计算MAE"></a># 根据中位数计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Median_pre = median_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Median_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Median_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Median_pre MAE: <span class="number">0.07500000000000007</span></span><br></pre></td></tr></table></figure>
<h1 id="Stacking融合-回归"><a href="#Stacking融合-回归" class="headerlink" title="# Stacking融合(回归)"></a># Stacking融合(回归)</h1><h2 id="定义Stacking融合函数"><a href="#定义Stacking融合函数" class="headerlink" title="# 定义Stacking融合函数"></a># 定义Stacking融合函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Stacking_method</span>(<span class="params">train_reg1, train_reg2, train_reg3,</span></span><br><span class="line"><span class="params">                    y_train_true,</span></span><br><span class="line"><span class="params">                    test_pre1, test_pre2, test_pre3,</span></span><br><span class="line"><span class="params">                    model_L2=linear_model.LinearRegression(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param train_reg1:  第一个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg2:  第二个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg3:  第三个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param y_train_true:    train真实的标签</span></span><br><span class="line"><span class="string">    :param test_pre1:   第一个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre2:   第二个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre3:   第三个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param model_L2:    次级模型:以真实训练集的标签为标签,以多个模型训练训练集后得到的标签合并后的数据集为特征进行训练</span></span><br><span class="line"><span class="string">                        注意:次级模型不宜选取的太复杂,这样会导致模型在训练集上过拟合,测试集泛化效果差</span></span><br><span class="line"><span class="string">    :return:            训练好的次机模型预测test数据集得到的预测值 - Stacking_result</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), pd.Series(train_reg3)], axis=<span class="number">1</span>).values,</span><br><span class="line">                 y_train_true)      <span class="comment"># 次级模型训练</span></span><br><span class="line">    stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),</span><br><span class="line">                                                  pd.Series(test_pre2), pd.Series(test_pre3)], axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> stacking_result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值"><a href="#生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值" class="headerlink" title="# 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值"></a># 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<h2 id="看一下Stacking融合的效果"><a href="#看一下Stacking融合的效果" class="headerlink" title="# 看一下Stacking融合的效果"></a># 看一下Stacking融合的效果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_L2 = linear_model.LinearRegression()      <span class="comment"># 不设定这个参数也可以,创建函数的时候默认了</span></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,</span><br><span class="line">                               test_pre1, test_pre2, test_pre3, model_L2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Stacking_pre MAE: &#x27;</span>, metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Stacking_pre MAE:  <span class="number">0.042134831460675204</span></span><br><span class="line"><span class="comment"># 发现模型效果相对于之前有了更近一步的提升</span></span><br></pre></td></tr></table></figure>
<h1 id="分类模型融合-Voting-Stacking…"><a href="#分类模型融合-Voting-Stacking…" class="headerlink" title="# 分类模型融合 - Voting,Stacking…"></a># 分类模型融合 - Voting,Stacking…</h1><h2 id="Voting投票机制"><a href="#Voting投票机制" class="headerlink" title="# Voting投票机制"></a># Voting投票机制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Voting - 投票机制</span></span><br><span class="line"><span class="string">        1.硬投票 - 对多个模型直接进行投票,不区分模型结果的相对重要度,最终投票数最多的类为最终被预测的类</span></span><br><span class="line"><span class="string">        2.软投票 - 和硬投票原理相同,增加了设置权重的功能,可以为不同模型设置不同权重,进而区别模型不同的重要度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 硬投票</span></span><br><span class="line">iris = datasets.load_iris()     <span class="comment"># 读取鸢尾花数据集 - 分类问题</span></span><br><span class="line"></span><br><span class="line">x = iris.data   <span class="comment"># 分离特征集和标签</span></span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)    <span class="comment"># 训练集和测试集按照7:3比例切分</span></span><br></pre></td></tr></table></figure>
<h2 id="用XGB分类模型训练数据"><a href="#用XGB分类模型训练数据" class="headerlink" title="# 用XGB分类模型训练数据"></a># 用XGB分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">colsample_bytree - 训练每棵树时，使用的特征占全部特征的比例</span></span><br><span class="line"><span class="string">objective - 目标函数</span></span><br><span class="line"><span class="string">            二分类问题 - binary:logistic - 返回概率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="用随机森林分类模型训练数据"><a href="#用随机森林分类模型训练数据" class="headerlink" title="# 用随机森林分类模型训练数据"></a># 用随机森林分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">n_estimators - 随机森林中决策树的个数</span></span><br><span class="line"><span class="string">max_depth - 决策树的最大深度</span></span><br><span class="line"><span class="string">            如果值为None,那么会扩展节点,直到所有的叶子是纯净的,或者直到所有叶子包含少于min_sample_split的样本</span></span><br><span class="line"><span class="string">min_samples_split - 分割内部节点所需要的最小样本数量</span></span><br><span class="line"><span class="string">min_samples_leaf - 需要在叶子结点上的最小样本数量</span></span><br><span class="line"><span class="string">oob_score - 是否使用袋外样本来估计泛化精度</span></span><br><span class="line"><span class="string">            树的生成过程并不会使用所有的样本,未使用的样本就叫(out_of_bag)oob袋外样本,通过袋外样本,可以评估这个树的准确度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="用SVC训练数据"><a href="#用SVC训练数据" class="headerlink" title="# 用SVC训练数据"></a># 用SVC训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">支持向量机 - 分类算法，但是也可以做回归,根据输入的数据不同可做不同的模型</span></span><br><span class="line"><span class="string">            1.若输入标签为连续值则做回归</span></span><br><span class="line"><span class="string">            2.若输入标签为分类值则用SVC()做分类</span></span><br><span class="line"><span class="string">            支持向量机的学习策略是间隔最大化，最终可转化为一个凸二次规划问题的求解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">C - 惩罚参数;   值越大,对误分类的惩罚大,不容犯错,于是训练集测试准确率高,但是泛化能力弱</span></span><br><span class="line"><span class="string">                值越小,对误分类的惩罚小,允许犯错,泛化能力较强</span></span><br><span class="line"><span class="string">probability - 是否采用概率估计,默认为False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="硬投票"><a href="#硬投票" class="headerlink" title="# 硬投票"></a># 硬投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">eclf - 其实就是三个模型的集成算法,硬投票决定最终被预测的类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)     <span class="comment"># 本质是Ensemble</span></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.05</span>) [Ensemble]</span><br></pre></td></tr></table></figure>
<h2 id="软投票"><a href="#软投票" class="headerlink" title="# 软投票"></a># 软投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.8</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.8</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)</span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;soft&#x27;</span>, weights=[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [Ensemble]</span><br></pre></td></tr></table></figure>
<h1 id="分类的Stacking-x2F-Blending融合"><a href="#分类的Stacking-x2F-Blending融合" class="headerlink" title="# 分类的Stacking&#x2F;Blending融合"></a># 分类的Stacking&#x2F;Blending融合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Stacking是一种分层模型集成框架,以两层为例</span></span><br><span class="line"><span class="string">        第一层由多个基学习器组成,其输入为原始训练集</span></span><br><span class="line"><span class="string">        第二层的模型则是以第一层学习器的输出作为训练集进行再训练,从而得到完整的stacking模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># ## 创建训练用的数据集</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]  <span class="comment"># 100个样本</span></span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 模型融合中使用到的各个单模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LogisticRegression()</span></span><br><span class="line"><span class="string">            solver - 用来优化权重     &#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;,默认adam,</span></span><br><span class="line"><span class="string">                                        lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好</span></span><br><span class="line"><span class="string">                                        sgd - 随机梯度下降 </span></span><br><span class="line"><span class="string">                                        adam - 机遇随机梯度的优化器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RandomForestClassifier()</span></span><br><span class="line"><span class="string">            n_estimators - 决策树个数</span></span><br><span class="line"><span class="string">            n_jobs - 用于拟合和预测的并行运行的工作数量,如果值为-1,那么工作数量被设置为核的数量</span></span><br><span class="line"><span class="string">            criterion - 衡量分裂质量的性能</span></span><br><span class="line"><span class="string">                        1.gini - Gini impurity衡量的是从一个集合中随机选择一个元素</span></span><br><span class="line"><span class="string">                                基于该集合中标签的概率分布为元素分配标签的错误率</span></span><br><span class="line"><span class="string">                                Gini impurity的计算就非常简单了,即1减去所有分类正确的概率,得到的就是分类不正确的概率</span></span><br><span class="line"><span class="string">                                若元素数量非常多,且所有元素单独属于一个分类时，Gini不纯度达到极小值0</span></span><br><span class="line"><span class="string">                        2.entropy - 信息增益熵</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">ExtraTreesClassifier() - 极端随机树</span></span><br><span class="line"><span class="string">    该算法与随机森林算法十分相似,都是由许多决策树构成,但该算法与随机森林有两点主要的区别:</span></span><br><span class="line"><span class="string">        1.随机森林应用的是Bagging模型,而ET是使用所有的训练样本得到每棵决策树,也就是每棵决策树应用的是相同的全部训练样本</span></span><br><span class="line"><span class="string">            关于Bagging和Boosting的差别,可以参考 https://www.cnblogs.com/earendil/p/8872001.html</span></span><br><span class="line"><span class="string">        2.随机森林是在一个随机子集内得到最佳分叉属性,而ET是完全随机的得到分叉值,从而实现对决策树进行分叉的</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"> Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                            1.lad - 绝对损失</span></span><br><span class="line"><span class="string">                                            2.huber - Huber损失</span></span><br><span class="line"><span class="string">                                            3.quantile - 分位数损失</span></span><br><span class="line"><span class="string">                                            4.ls - 均方差损失(默认)</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">                                    alpha - 当我们使用Huber损失和分位数损失&#x27;quantile&#x27;时,需要指定分位数的值,只有regressor有</span></span><br><span class="line"><span class="string">                                    </span></span><br><span class="line"><span class="string">                            GradientBoostingClassifier() - 参数绝大多数和Regressor相同,不同的是loss函数</span></span><br><span class="line"><span class="string">                                            1.deviance - 对数似然损失函数(默认)</span></span><br><span class="line"><span class="string">                                            2.exponential - 指数损失函数       </span></span><br><span class="line"><span class="string">参考网址: https://www.cnblogs.com/pinard/p/6143927.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((X.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))  <span class="comment"># 全零数组,行取训练集的个数,列取模型个数</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 全零数组,行取测试集的个数,列取模型个数</span></span><br></pre></td></tr></table></figure>
<h2 id="5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练"><a href="#5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练" class="headerlink" title="# 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练"></a># 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)     <span class="comment"># # 分层交叉验证,每一折中都保持着原始数据中各个类别的比例关系(测试集和训练集分离)</span></span><br><span class="line">skf = skf.split(X, y)     <span class="comment"># 把特征和标签分离</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">enumerate() - 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列,同时列出数据和数据下标,一般用在for循环当中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 30行5列的全0数组</span></span><br><span class="line">    <span class="comment"># 五折交叉训练,使用第i个部分作为预测集,剩余部分为验证集,获得的预测值成为第i部分的新特征</span></span><br><span class="line">    <span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf):</span><br><span class="line">        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        <span class="comment"># 将对测试集的概率预测第二列(也就是结果为1)的概率装进y_submission中</span></span><br><span class="line">        y_submission = clf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">        dataset_blend_train[test, j] = y_submission     <span class="comment"># 把预测验证集(比如第一折)的结果依次对应装进dataset_blend_train中</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        predict_proba() - 返回的是一个n行k列的数组</span></span><br><span class="line"><span class="string">                            第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率,并且每一行的概率和为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        因为我们采取到的数据集的标签只有0或1,所以predict_proba返回的概率只有两个</span></span><br><span class="line"><span class="string">                    如果左边的概率大于0.5,那么预测值为0</span></span><br><span class="line"><span class="string">                    如果右边的概率大于0.5,那么预测值为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># # 将对测试集的概率预测的第二列(也就是结果为1)的概率装进dataset_blend_test_j中</span></span><br><span class="line">        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对于测试集,直接用这5个模型的预测值均值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = dataset_blend_test_j.mean(<span class="number">1</span>)     <span class="comment"># mean(1) - 求每行数的平均值(五折预测测试集的平均值)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>)    <span class="comment"># 次级学习器再次训练</span></span><br><span class="line">clf.fit(dataset_blend_train, y)     <span class="comment"># 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_blend_test)[:, <span class="number">1</span>]  <span class="comment"># 把第一层预测测试集的结果作为新特征,预测测试集的标签</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ROC曲线和AUC - 用来评价一个二值分类器(binary classifier)的优劣,用于衡量&#x27;二分类问题&#x27;机器学习算法性能(泛化能力)</span></span><br><span class="line"><span class="string">AUC - ROC曲线下的面积</span></span><br><span class="line"><span class="string">      AUC的取值范围在0.5和1之间</span></span><br><span class="line"><span class="string">      使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好</span></span><br><span class="line"><span class="string">      而作为一个数值,对应AUC更大的分类器效果更好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h2 id="Blending-和Stacking类似-不同点在于"><a href="#Blending-和Stacking类似-不同点在于" class="headerlink" title="# Blending - 和Stacking类似,不同点在于:"></a># Blending - 和Stacking类似,不同点在于:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.Stacking - 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string">2.Blending - 把第一层得到训练集中的30%的验证集的结果作为新特征继续训练,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Blending优点 - 比stacking简单,因为不用进行k次的交叉验证来获得stacker feature</span></span><br><span class="line"><span class="string">                避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</span></span><br><span class="line"><span class="string">Blending缺点- 使用了很少的数据,可能会过拟合,没有stacking使用多次的交叉验证来的稳健</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把训练数据分成d1(子训练集),d2(验证集)两部分 - 对半分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))   <span class="comment"># 35行5列的全0数组</span></span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))      <span class="comment"># 30行5列的全0数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 用子训练集依次训练各个模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    <span class="comment"># 返回模型对验证集的预测值为1的概率</span></span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 结果装进dataset_d1中 - 表示用子训练集训练的模型预测验证集标签的结果 - 就是上文说的30%的数据</span></span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment"># 建立第二层模型的特征 - 用第一层模型预测测试集的结果作为新的特征</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 看一下预测的预测集标签和真实的预测集标签的roc_auc_score</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用第二层模型训练特征</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)   <span class="comment"># 用验证集的第一层模型预测结果作为特征,用验证集的真实标签作为标签,再次训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, <span class="number">1</span>]  <span class="comment"># 用第一层模型预测测试集的结果作为特征,用第二层模型预测训练集返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Blending: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">Val auc Score of Blending: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h2 id="利用mlxtend进行分类的Stacking融合"><a href="#利用mlxtend进行分类的Stacking融合" class="headerlink" title="# 利用mlxtend进行分类的Stacking融合"></a># 利用mlxtend进行分类的Stacking融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, <span class="number">1</span>:<span class="number">3</span>], iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=<span class="number">1</span>)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">StackingClassifier() - 快速Stacking融合的方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">classifiers - 一级分类器列表</span></span><br><span class="line"><span class="string">meta_classifier - 二级分类器(元分类器)</span></span><br><span class="line"><span class="string">use_probas - 如果为True,则基于预测的概率而不是类标签来训练元分类器,默认为False</span></span><br><span class="line"><span class="string">average_probas - 如果为真,将概率平均为元特征,默认为False</span></span><br><span class="line"><span class="string">verbose - 是否输出到日志</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                          meta_classifier=lr)</span><br><span class="line">label = [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>, <span class="string">&#x27;Stacking Classifier&#x27;</span>]</span><br><span class="line">clf_list = [clf1, clf2, clf3, sclf]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># 网格布局,每行2个,每列2个</span></span><br><span class="line">grid = itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)  <span class="comment"># 求多个可迭代对象的笛卡尔积,其实就是更加灵活调整网格的大小</span></span><br><span class="line"></span><br><span class="line">clf_cv_mean = []    <span class="comment"># 存放每个模型的准确率的均值</span></span><br><span class="line">clf_cv_std = []     <span class="comment"># 存放每个模型的准确率的标准差</span></span><br><span class="line"><span class="keyword">for</span> clf, label, grd <span class="keyword">in</span> <span class="built_in">zip</span>(clf_list, label, grid):</span><br><span class="line"></span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 3折交叉验证,评分标准为模型准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.2f (+/- %.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line">    clf_cv_mean.append(scores.mean())</span><br><span class="line">    clf_cv_std.append(scores.std())</span><br><span class="line"></span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(label)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.02</span>) [Stacking Classifier]</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出,融合后的曲线更加优秀</p>
<h2 id="一些其它方法"><a href="#一些其它方法" class="headerlink" title="# 一些其它方法"></a># 一些其它方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将特征放进模型中预测,并将预测结果变换并作为新的特征加入原有特征中,再经过模型预测结果(Stacking变化)</span></span><br><span class="line"><span class="string">可以反复预测多次将结果加入最后的特征中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ensemble_add_feature</span>(<span class="params">train, test, target, clfs</span>):</span><br><span class="line">    <span class="comment"># n_folds = 5</span></span><br><span class="line">    <span class="comment"># skf = list(StratifiedKFold(y, n_folds=n_folds))</span></span><br><span class="line">    train_ = np.zeros((train.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    test_ = np.zeros((test.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="comment"># 依次训练单个模型</span></span><br><span class="line">        <span class="built_in">print</span>(j, clf)</span><br><span class="line">        <span class="comment"># 使用第1部分作为预测,第2部分来训练模型(第1部分预测的输出作为第2部分的新特征)</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train]</span></span><br><span class="line">        clf.fit(train, target)  <span class="comment"># 训练模型</span></span><br><span class="line">        y_train = clf.predict(train)    <span class="comment"># 模型在训练集中的预测值</span></span><br><span class="line">        y_test = clf.predict(test)      <span class="comment"># 模型在测试集中的预测值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成新特征</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        j 从0开始递增,构建新的特征集,特征为训练集和测试集各自的预测值的平方</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_[:, j*<span class="number">2</span>] = y_train ** <span class="number">2</span></span><br><span class="line">        test_[:, j*<span class="number">2</span>] = y_test ** <span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)    <span class="comment"># np.exp(a) - 返回e的a次方</span></span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method:&#x27;</span>, j)</span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf = LogisticRegression()  <span class="comment"># 次级模型</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.3</span>)</span><br><span class="line">x_train = pd.DataFrame(x_train)     <span class="comment"># 转换成DataFrame格式,方便后续构造新特征</span></span><br><span class="line">x_test = pd.DataFrame(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给出模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 新特征的构造 - 用上面的各个单模型预测训练集和测试集的结果,作为新特征</span></span><br><span class="line">New_train, New_test = ensemble_add_feature(x_train, x_test, y_train, clfs)</span><br><span class="line">clf.fit(New_train, y_train)     <span class="comment"># 用训练集的新特征和训练集的真实标签训练数据</span></span><br><span class="line">y_emb = clf.predict_proba(New_test)[:, <span class="number">1</span>]   <span class="comment"># 用训练好的模型得到新的测试集特征返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_test, y_emb)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method: <span class="number">4</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>

<p>关于模型融合的理论和方法具体可以参考：<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/team-learning-data-mining/blob/master/FinancialRiskControl/Task5%20%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88.md">https://github.com/datawhalechina/team-learning-data-mining/blob/master/FinancialRiskControl/Task5%20%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88.md</a></p>
<p>这边文章由于时间关系，只提供了一个大致的思路，具体调参以及之后的融合可以参照task4和上文提供的链接尝试。</p>
<h1 id="导入三方模块"><a href="#导入三方模块" class="headerlink" title="导入三方模块"></a>导入三方模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, train_test_split, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns = <span class="literal">None</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, <span class="keyword">lambda</span> x: <span class="string">&#x27;%.2f&#x27;</span> % x)</span><br></pre></td></tr></table></figure>
<p><strong>做模型融合之前，我要做两件事情：特征筛选和模型选择</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">特征筛选：随机森林或SFS挑选最优特征</span></span><br><span class="line"><span class="string">模型选择：xgboost, lightgbm, logistic 加权融合。或者再加线性回归模型嵌套一个弱分类器学习预测</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="不同模型的数据准备注意点"><a href="#不同模型的数据准备注意点" class="headerlink" title="不同模型的数据准备注意点"></a>不同模型的数据准备注意点</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">使用xgboost要注意的点：</span></span><br><span class="line"><span class="string">    1.在Xgb中需要将离散特征one-hot编码再和连续特征一起输入训练，这样做是为了达到在cart树中处理离散特征的方式一致</span></span><br><span class="line"><span class="string">    2.无需处理缺失值，在Xgb中处理稀疏数据时，没有值的特征是走默认的分支，所以在Xgb中缺省值也是走默认分支</span></span><br><span class="line"><span class="string">lightgbm:</span></span><br><span class="line"><span class="string">    1.由于使用直方图算法，LightGBM直接支持类别特征，对类别特征不必进行独热编码处理（与xgboost不同）</span></span><br><span class="line"><span class="string">    2.也可以直接处理缺失值</span></span><br><span class="line"><span class="string">logistic:</span></span><br><span class="line"><span class="string">    1.数据要进行缺失值和异常值的处理</span></span><br><span class="line"><span class="string">    2.类别数据要做one-hot编码</span></span><br><span class="line"><span class="string">    3.对于某些方差大的特征，建议做归一化处理以增强模型稳定性</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="读取、压缩数据"><a href="#读取、压缩数据" class="headerlink" title="读取、压缩数据"></a>读取、压缩数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\train.csv&#x27;</span>)</span><br><span class="line">testA = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\testA.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>  <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:  <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:  <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>    <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))  <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = reduce_mem_usage(train)</span><br><span class="line">testA = reduce_mem_usage(testA)</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.2&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.3&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p><strong>为了方便起见，把训练集和测试集合并处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = pd.concat([train, testA], axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="lgb"><a href="#lgb" class="headerlink" title="lgb"></a>lgb</h1><h2 id="字段-employmentLength-10年以上算10年，1年一下算0年；然后转化成数值"><a href="#字段-employmentLength-10年以上算10年，1年一下算0年；然后转化成数值" class="headerlink" title="字段 employmentLength - 10年以上算10年，1年一下算0年；然后转化成数值"></a>字段 employmentLength - 10年以上算10年，1年一下算0年；然后转化成数值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;10+ years&#x27;</span>, value=<span class="string">&#x27;10 years&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;&lt; 1 year&#x27;</span>, value=<span class="string">&#x27;0 year&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">employmentLength_to_int</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> pd.isnull(s):</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.int8(s.split()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>] = data[<span class="string">&#x27;employmentLength&#x27;</span>].apply(employmentLength_to_int)</span><br></pre></td></tr></table></figure>
<h2 id="字段-earliesCreditLine-分别提取年份和月份做拼接"><a href="#字段-earliesCreditLine-分别提取年份和月份做拼接" class="headerlink" title="字段 earliesCreditLine - 分别提取年份和月份做拼接"></a>字段 earliesCreditLine - 分别提取年份和月份做拼接</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[-<span class="number">4</span>:])</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">month_re</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x == <span class="string">&#x27;Jan&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;01&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Feb&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;02&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Mar&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;03&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Apr&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;04&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;May&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;05&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jun&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;06&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jul&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;07&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Aug&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;08&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Sep&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;09&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Oct&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Nov&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;11&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;12&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>].apply(<span class="keyword">lambda</span> x: month_re(x))</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] + data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="字段-issueDate-从2017年6月1日开始；数据按照此节点统计天数"><a href="#字段-issueDate-从2017年6月1日开始；数据按照此节点统计天数" class="headerlink" title="字段 issueDate - 从2017年6月1日开始；数据按照此节点统计天数"></a>字段 issueDate - 从2017年6月1日开始；数据按照此节点统计天数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data[<span class="string">&#x27;issueDate&#x27;</span>], <span class="built_in">format</span>=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;issueDateDt&#x27;</span>] = data[<span class="string">&#x27;issueDate&#x27;</span>].apply(<span class="keyword">lambda</span> x: x - startdate).dt.days</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;issueDate&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="看一下特征的类别分布情况"><a href="#看一下特征的类别分布情况" class="headerlink" title="看一下特征的类别分布情况"></a>看一下特征的类别分布情况</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cate_features = [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> cate <span class="keyword">in</span> cate_features:</span><br><span class="line">    <span class="built_in">print</span>(cate, <span class="string">&#x27;类型数&#x27;</span>, data[cate].nunique())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">不适合做one-hot编码的是</span></span><br><span class="line"><span class="string">    employmentTitle 类型数 298101</span></span><br><span class="line"><span class="string">    postCode 类型数 935</span></span><br><span class="line"><span class="string">    title 类型数 6712</span></span><br><span class="line"><span class="string">    regionCode 类型数 51 - 大于50的先不处理了，维度还是比较高的</span></span><br><span class="line"><span class="string">    policyCode 类型数 1 - 无分析价值，可直接删除</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;policyCode&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="对于高维类别特征，进行转换，取他们同类型的数量值和排名值"><a href="#对于高维类别特征，进行转换，取他们同类型的数量值和排名值" class="headerlink" title="对于高维类别特征，进行转换，取他们同类型的数量值和排名值"></a>对于高维类别特征，进行转换，取他们同类型的数量值和排名值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;title&#x27;</span>]:</span><br><span class="line">    data[f + <span class="string">&#x27;_counts&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">    data[f + <span class="string">&#x27;_rank&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].rank(ascending=<span class="literal">False</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">del</span> data[f]</span><br><span class="line"></span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">train_lgb = data[data.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA_lgb = data[data.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="保存数据待用"><a href="#保存数据待用" class="headerlink" title="保存数据待用"></a>保存数据待用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_lgb.to_csv(<span class="string">&#x27;./data/train_data_for_lgb.csv&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line">testA_lgb.to_csv(<span class="string">&#x27;./data/testA_data_for_lgb.csv&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h1 id="xgb-主要one-hot类别特征"><a href="#xgb-主要one-hot类别特征" class="headerlink" title="xgb - 主要one-hot类别特征"></a>xgb - 主要one-hot类别特征</h1><h2 id="对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码"><a href="#对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码" class="headerlink" title="对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码"></a>对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_xgb = data</span><br><span class="line">data_xgb = pd.get_dummies(data_xgb, columns=[<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>,</span><br><span class="line">                                             <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>], drop_first=<span class="literal">True</span>)</span><br><span class="line">train_xgb = data_xgb[data_xgb.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA_xgb = data_xgb[data_xgb.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="保存数据待用-1"><a href="#保存数据待用-1" class="headerlink" title="保存数据待用"></a>保存数据待用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_xgb.to_csv(<span class="string">&#x27;./data/train_data_for_xgb.csv&#x27;</span>)</span><br><span class="line">testA_xgb.to_csv(<span class="string">&#x27;./data/testA_data_for_xgb.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="logistic-主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶"><a href="#logistic-主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶" class="headerlink" title="logistic - 主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶"></a>logistic - 主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_logistic = data_xgb</span><br></pre></td></tr></table></figure>
<h2 id="缺失值的处理"><a href="#缺失值的处理" class="headerlink" title="缺失值的处理"></a>缺失值的处理</h2><p>我没有选择删除数据（每条数据都是宝贵的），能补就补</p>
<h2 id="看看缺失数据"><a href="#看看缺失数据" class="headerlink" title="看看缺失数据"></a>看看缺失数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">missing = data_logistic.isnull().<span class="built_in">sum</span>()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure>
<p><strong>可以发现，有缺失值的特征不多，就22个；其中缺失率比较高的有16个，基本都是匿名特征且缺失率均在10%以下</strong></p>
<h2 id="看下匿名特征的分布情况，判断一下是离散特征还是连续特征"><a href="#看下匿名特征的分布情况，判断一下是离散特征还是连续特征" class="headerlink" title="看下匿名特征的分布情况，判断一下是离散特征还是连续特征"></a>看下匿名特征的分布情况，判断一下是离散特征还是连续特征</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_features = [<span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>, <span class="string">&#x27;n12&#x27;</span>, <span class="string">&#x27;n13&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_features:</span><br><span class="line">    <span class="built_in">print</span>(n, <span class="string">&#x27;类型数&#x27;</span>, data_logistic[n].nunique())</span><br><span class="line"></span><br><span class="line">data_logistic[[<span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>]].head(<span class="number">50</span>)</span><br><span class="line">data_logistic[[<span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>]].info()</span><br></pre></td></tr></table></figure>
<p><strong>去重以后值不多，看着像离散型特征</strong></p>
<h2 id="我先统一将这些匿名特征的缺失值单独分一类"><a href="#我先统一将这些匿名特征的缺失值单独分一类" class="headerlink" title="我先统一将这些匿名特征的缺失值单独分一类"></a>我先统一将这些匿名特征的缺失值单独分一类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">missing_features_part = [<span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>, <span class="string">&#x27;n12&#x27;</span>, <span class="string">&#x27;n13&#x27;</span>,</span><br><span class="line">                         <span class="string">&#x27;n14&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> missfea <span class="keyword">in</span> missing_features_part:</span><br><span class="line">    data_logistic.fillna(&#123;missfea: -<span class="number">99</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">    data_logistic[missfea] = data_logistic[missfea].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; 处理完成&#x27;</span>.<span class="built_in">format</span>(missfea))</span><br></pre></td></tr></table></figure>
<h2 id="剩下有缺失数据的特征如下"><a href="#剩下有缺失数据的特征如下" class="headerlink" title="剩下有缺失数据的特征如下"></a>剩下有缺失数据的特征如下</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">employmentLength           58541</span></span><br><span class="line"><span class="string">dti                          300</span></span><br><span class="line"><span class="string">pubRecBankruptcies           521</span></span><br><span class="line"><span class="string">revolUtil                    658</span></span><br><span class="line"><span class="string">employmentTitle_counts         1</span></span><br><span class="line"><span class="string">postCode_counts                1</span></span><br><span class="line"><span class="string">title_counts                   1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值"><a href="#employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值" class="headerlink" title="employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值"></a>employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fea_miss = [<span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;revolUtil&#x27;</span>, <span class="string">&#x27;pubRecBankruptcies&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;employmentTitle_counts&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;postCode_counts&#x27;</span>, <span class="string">&#x27;title_counts&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="employmentLength"><a href="#employmentLength" class="headerlink" title="employmentLength"></a>employmentLength</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;10&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;employmentLength&#x27;</span>: <span class="number">10</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="revolUtil"><a href="#revolUtil" class="headerlink" title="revolUtil"></a>revolUtil</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;revolUtil&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;0.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;revolUtil&#x27;</span>: <span class="number">0.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="pubRecBankruptcies"><a href="#pubRecBankruptcies" class="headerlink" title="pubRecBankruptcies"></a>pubRecBankruptcies</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;pubRecBankruptcies&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;0.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;pubRecBankruptcies&#x27;</span>: <span class="number">0.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="dti"><a href="#dti" class="headerlink" title="dti"></a>dti</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;dti&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;16.80&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;dti&#x27;</span>: <span class="number">16.80</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="employmentTitle-counts"><a href="#employmentTitle-counts" class="headerlink" title="employmentTitle_counts"></a>employmentTitle_counts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;employmentTitle_counts&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;1.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;employmentTitle_counts&#x27;</span>: <span class="number">1.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="postCode-counts"><a href="#postCode-counts" class="headerlink" title="postCode_counts"></a>postCode_counts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;postCode_counts&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;11092.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;postCode_counts&#x27;</span>: <span class="number">11092.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="title-counts"><a href="#title-counts" class="headerlink" title="title_counts"></a>title_counts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;title_counts&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;491400.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;title_counts&#x27;</span>: <span class="number">491400.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="随机森林筛选重要特征"><a href="#随机森林筛选重要特征" class="headerlink" title="随机森林筛选重要特征"></a>随机森林筛选重要特征</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_features = <span class="built_in">list</span>(data_logistic.select_dtypes(exclude=[<span class="string">&#x27;category&#x27;</span>]).columns)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;数值型特征&#x27;&#x27;&#x27;</span></span><br><span class="line">cate_features = <span class="built_in">list</span>(data_logistic.select_dtypes(include=[<span class="string">&#x27;category&#x27;</span>]).columns)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;类别型特征&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> cate_features:</span><br><span class="line">    data_logistic[fea] = data_logistic[fea].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 分离训练集和测试集</span></span><br><span class="line">train_logistic_forest = data_logistic[data_logistic.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> train_logistic_forest.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">label = [<span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line">X_train_logistic_forest = train_logistic_forest[features]</span><br><span class="line">y_train_logistic_forest = train_logistic_forest[label]</span><br><span class="line"><span class="comment"># ## 使用随机森林训练</span></span><br><span class="line">clf_forest = RandomForestClassifier()</span><br><span class="line">clf_forest.fit(X_train_logistic_forest, y_train_logistic_forest)</span><br></pre></td></tr></table></figure>
<h2 id="得到特征重要性"><a href="#得到特征重要性" class="headerlink" title="得到特征重要性"></a>得到特征重要性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">forest_importances = <span class="built_in">list</span>(clf_forest.feature_importances_)</span><br><span class="line">feature_importances = [(feature, <span class="built_in">round</span>(importance, <span class="number">2</span>)) <span class="keyword">for</span> feature, importance <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">    features, forest_importances)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 重要性排序</span></span><br><span class="line">feature_importances = <span class="built_in">sorted</span>(feature_importances, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(feature_importances)</span><br></pre></td></tr></table></figure>
<p><strong>重要性为0的特征就不保留了，不然内存占用太大了</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">important_features = [<span class="string">&#x27;interestRate&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revolBal&#x27;</span>, <span class="string">&#x27;revolUtil&#x27;</span>, <span class="string">&#x27;earliesCreditLine_date&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;title_rank&#x27;</span>, <span class="string">&#x27;loanAmnt&#x27;</span>, <span class="string">&#x27;installment&#x27;</span>, <span class="string">&#x27;annualIncome&#x27;</span>, <span class="string">&#x27;totalAcc&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;issueDateDt&#x27;</span>, <span class="string">&#x27;employmentTitle_rank&#x27;</span>, <span class="string">&#x27;postCode_counts&#x27;</span>, <span class="string">&#x27;postCode_rank&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;regionCode_rank&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;ficoRangeLow&#x27;</span>, <span class="string">&#x27;ficoRangeHigh&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;openAcc&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;employmentTitle_counts&#x27;</span>, <span class="string">&#x27;regionCode_counts&#x27;</span>, <span class="string">&#x27;delinquency_2years&#x27;</span>, <span class="string">&#x27;pubRec&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;title_counts&#x27;</span>, <span class="string">&#x27;grade_E&#x27;</span>, <span class="string">&#x27;homeOwnership_1&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;interestRate&#x27;, 0.05), (&#x27;dti&#x27;, 0.04), (&#x27;revolBal&#x27;, 0.04), (&#x27;revolUtil&#x27;, 0.04), </span></span><br><span class="line"><span class="string">(&#x27;earliesCreditLine_date&#x27;, 0.04), (&#x27;title_rank&#x27;, 0.04), (&#x27;loanAmnt&#x27;, 0.03), (&#x27;installment&#x27;, 0.03), </span></span><br><span class="line"><span class="string">(&#x27;annualIncome&#x27;, 0.03), (&#x27;totalAcc&#x27;, 0.03), (&#x27;issueDateDt&#x27;, 0.03), (&#x27;employmentTitle_rank&#x27;, 0.03), </span></span><br><span class="line"><span class="string">(&#x27;postCode_counts&#x27;, 0.03), (&#x27;postCode_rank&#x27;, 0.03), (&#x27;regionCode_rank&#x27;, 0.03), (&#x27;term&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;employmentLength&#x27;, 0.02), (&#x27;ficoRangeLow&#x27;, 0.02), (&#x27;ficoRangeHigh&#x27;, 0.02), (&#x27;openAcc&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;n1&#x27;, 0.02), (&#x27;n4&#x27;, 0.02), (&#x27;n5&#x27;, 0.02), (&#x27;n6&#x27;, 0.02), (&#x27;n7&#x27;, 0.02), (&#x27;n8&#x27;, 0.02), (&#x27;n10&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;n14&#x27;, 0.02), (&#x27;employmentTitle_counts&#x27;, 0.02), (&#x27;regionCode_counts&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;delinquency_2years&#x27;, 0.01), (&#x27;pubRec&#x27;, 0.01), (&#x27;n0&#x27;, 0.01), (&#x27;n2&#x27;, 0.01), (&#x27;n2.1&#x27;, 0.01), </span></span><br><span class="line"><span class="string">(&#x27;n9&#x27;, 0.01), (&#x27;title_counts&#x27;, 0.01), (&#x27;grade_E&#x27;, 0.01), (&#x27;homeOwnership_1&#x27;, 0.01), </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">columns = [<span class="string">&#x27;interestRate&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revolBal&#x27;</span>, <span class="string">&#x27;revolUtil&#x27;</span>, <span class="string">&#x27;earliesCreditLine_date&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;title_rank&#x27;</span>, <span class="string">&#x27;loanAmnt&#x27;</span>, <span class="string">&#x27;installment&#x27;</span>, <span class="string">&#x27;annualIncome&#x27;</span>, <span class="string">&#x27;totalAcc&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;issueDateDt&#x27;</span>, <span class="string">&#x27;employmentTitle_rank&#x27;</span>, <span class="string">&#x27;postCode_counts&#x27;</span>, <span class="string">&#x27;postCode_rank&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;regionCode_rank&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;ficoRangeLow&#x27;</span>, <span class="string">&#x27;ficoRangeHigh&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;openAcc&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>, <span class="string">&#x27;employmentTitle_counts&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;regionCode_counts&#x27;</span>, <span class="string">&#x27;delinquency_2years&#x27;</span>, <span class="string">&#x27;pubRec&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;title_counts&#x27;</span>, <span class="string">&#x27;grade_E&#x27;</span>, <span class="string">&#x27;homeOwnership_1&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 将重要性为零的特征都不保留了</span></span><br><span class="line">data_logistic = data_logistic[columns]</span><br></pre></td></tr></table></figure>
<h2 id="异常数据处理"><a href="#异常数据处理" class="headerlink" title="异常数据处理"></a>异常数据处理</h2><p><strong>对于异常值，这里不做处理，将数值型特征做归一化处理，降低异常值的干扰。归一化前先去对数</strong><br>为什么要取对数 - 数据集中有负数就不能取对数了 - 实践中,取对数的一般是水平量,而不是比例数据<br>1.缩小数据的绝对数值,方便计算<br>2.取对数后,可以将乘法计算转换称加法计算<br>3.对数值小的部分差异的敏感程度比数值大的部分的差异敏感程度更高<br>4.取对数之后不会改变数据的性质和相关关系,但压缩了变量的尺度<br>5.所得到的数据易消除异方差问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">min_max_scaler</span>(<span class="params">data, fea</span>):</span><br><span class="line">    data[fea] = np.log(data[fea] + <span class="number">2</span>)   <span class="comment"># 数据中有-1</span></span><br><span class="line">    data[fea] = ((data[fea] - np.<span class="built_in">min</span>(data[fea])) / (np.<span class="built_in">max</span>(data[fea]) - np.<span class="built_in">min</span>(data[fea]))) <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">min_max_columns = <span class="built_in">list</span>(data_logistic.select_dtypes(exclude=[<span class="string">&#x27;uint8&#x27;</span>]).columns)</span><br><span class="line">min_max_columns.remove(<span class="string">&#x27;isDefault&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> min_max_columns:</span><br><span class="line">    min_max_scaler(data_logistic, col)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_logistic.info()</span><br></pre></td></tr></table></figure>
<h2 id="两个数据集分开"><a href="#两个数据集分开" class="headerlink" title="两个数据集分开"></a>两个数据集分开</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_logistic = data_logistic[data_logistic.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA_logistic = data_logistic[data_logistic.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_logistic.to_csv(<span class="string">&#x27;./data/train_data_for_logistic.csv&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line">testA_logistic.to_csv(<span class="string">&#x27;./data/testA_data_for_logistic.csv&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="logistic"><a href="#logistic" class="headerlink" title="logistic"></a>logistic</h2><p><strong>数据准备</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_logistic = pd.read_csv(<span class="string">&#x27;./data/train_data_for_logistic.csv&#x27;</span>)</span><br><span class="line">testA_logistic = pd.read_csv(<span class="string">&#x27;./data/testA_data_for_logistic.csv&#x27;</span>)</span><br><span class="line">missing_fea = [<span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>]</span><br><span class="line">train_logistic.info()</span><br><span class="line">train_logistic[missing_fea] = train_logistic[missing_fea].fillna(train_logistic[missing_fea].median())</span><br><span class="line">testA_logistic[missing_fea] = testA_logistic[missing_fea].fillna(testA_logistic[missing_fea].median())</span><br></pre></td></tr></table></figure>
<h2 id="划分训练集的特征和标签"><a href="#划分训练集的特征和标签" class="headerlink" title="划分训练集的特征和标签"></a>划分训练集的特征和标签</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> train_logistic.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">label = [<span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line">X_train_logistic = train_logistic[features]</span><br><span class="line">y_train_logistic = train_logistic[label]</span><br></pre></td></tr></table></figure>
<h2 id="将训练集分为5份，4份作为训练集，1份作为验证集"><a href="#将训练集分为5份，4份作为训练集，1份作为验证集" class="headerlink" title="将训练集分为5份，4份作为训练集，1份作为验证集"></a>将训练集分为5份，4份作为训练集，1份作为验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X_train_logistic_split, X_val_logistic, y_train_logistic_split, y_val_logistic = train_test_split(</span><br><span class="line">    X_train_logistic, y_train_logistic, test_size=<span class="number">0.2</span>)</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr = lr.fit(X_train_logistic_split, y_train_logistic_split)</span><br><span class="line">y_val_logistic_pre = lr.predict(X_val_logistic)</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_val_logistic, y_val_logistic_pre)</span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;未调参前逻辑回归在验证集上的AUC： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(roc_auc))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">未调参前逻辑回归在验证集上的AUC： 0.531000169605175</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">欠拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>找出相关性高的特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_)</span><br><span class="line">m = &#123;&#125;</span><br><span class="line">col_name = <span class="built_in">list</span>(X_train_logistic_split.columns)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(col_name)):</span><br><span class="line">    <span class="comment"># 若没有key,加入key</span></span><br><span class="line">    m.setdefault(col_name[i], <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 这里取绝对值，主要看特征的相关性</span></span><br><span class="line">    m[col_name[i]] = <span class="built_in">abs</span>(lr.coef_[<span class="number">0</span>][i])</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(m.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><strong>大家这里可以尝试用相关性高的特征加以处理或特征构造，重新训练模型。我这里也没有调参，可以通过调参提高结果分数</strong></p>
<h2 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_xgb = pd.read_csv(<span class="string">&#x27;./data/train_data_for_xgb.csv&#x27;</span>)</span><br><span class="line">testA_xgb = pd.read_csv(<span class="string">&#x27;./data/testA_data_for_xgb.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">features_xgb = [f <span class="keyword">for</span> f <span class="keyword">in</span> train_xgb.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">X_train_xgb = train_xgb[features_xgb]</span><br><span class="line">y_train_xgb = train_xgb[<span class="string">&#x27;isDefault&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h1 id="xgb-梯度提升决策树"><a href="#xgb-梯度提升决策树" class="headerlink" title="## xgb - 梯度提升决策树"></a>## xgb - 梯度提升决策树</h1><p>‘’’<br>XGBRegressor - 梯度提升回归树,也叫梯度提升机</p>
<pre><code>            采用连续的方式构造树,每棵树都试图纠正前一棵树的错误
            与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝
            从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快

            gamma - 定了节点分裂所需的最小损失函数下降值,这个参数的值越大,算法越保守
            subsample - 这个参数控制对于每棵树随机采样的比例,减小这个参数的值,算法会更加保守,避免过拟合
            colsample_bytree - 用来控制每棵随机采样的列数的占比
            learning_rate - 学习速率,用于控制树的权重,xgb模型在进行完每一轮迭代之后,会将叶子节点的分数乘上该系数,
                            以便于削弱各棵树的影响,避免过拟合
</code></pre>
<p>‘’’</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_xgb</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    model = xgb.XGBRegressor(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.08</span>, gamma=<span class="number">0</span>,</span><br><span class="line">                             subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">    model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># xgb五折交叉验证</span></span><br><span class="line">xgr = xgb.XGBClassifier(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.1</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scores_train = []   <span class="comment"># 每次模型训练训练集中子训练集的得分</span></span><br><span class="line">scores = []         <span class="comment"># 每次模型训练训练集中验证集的得分</span></span><br><span class="line"></span><br><span class="line">sk = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">0</span>)  <span class="comment"># shuffle判断是否在每次抽样时对样本进行清洗</span></span><br><span class="line"><span class="keyword">for</span> train_ind, val_ind <span class="keyword">in</span> sk.split(X_train_xgb, y_train_xgb):</span><br><span class="line">    train_x = X_train_xgb.iloc[train_ind].values</span><br><span class="line">    train_y = y_train_xgb.iloc[train_ind]</span><br><span class="line">    val_x = X_train_xgb.iloc[val_ind].values</span><br><span class="line">    val_y = y_train_xgb.iloc[val_ind]</span><br><span class="line"></span><br><span class="line">    xgr.fit(train_x, train_y)</span><br><span class="line">    pred_train_xgb = xgr.predict(train_x)   <span class="comment"># 子训练集的预测值</span></span><br><span class="line">    pre_xgb = xgr.predict(val_x)            <span class="comment"># 验证集的预测值</span></span><br><span class="line"></span><br><span class="line">    scores_train.append(roc_auc_score(train_y, pred_train_xgb))</span><br><span class="line">    scores.append(roc_auc_score(val_y, pre_xgb))                    <span class="comment"># 统计验证集的mae</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train mae:&#x27;</span>, np.mean(scores_train))  <span class="comment"># 统计mae均值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val mae:&#x27;</span>, np.mean(scores))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Train mae: 0.5548858423493594</span></span><br><span class="line"><span class="string">Val mae: 0.5458927786959327</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>结果也不是很好，同样可以通过特征重新筛选和调参来提高</strong></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>至于lgb部分，可以参考Task4的代码，思路就是使用lgb,xgb,lr同时找到合适的特征并加以调参。训练完后三个模型可以使用文初提到的链接使用stacking、blending、加权融合或者投票（硬投票、软投票等方法）尝试模型融合。</p>
<p>在task5之后，我自己还会做一个task6，尝试完整的完成预测，再到线上提交加以迭代。</p>

    </div>

    
    
    
	
	  <div>
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	  </div>
	
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>小书包
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/" title="机器学习训练_金融风控_Task5_模型融合">http://example.com/2023/04/24/机器学习训练_金融风控_Task5_模型融合/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/" rel="tag"># 金融风控</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task4_%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/" rel="prev" title="机器学习训练_金融风控_Task4_建模调参">
      <i class="fa fa-chevron-left"></i> 机器学习训练_金融风控_Task4_建模调参
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/24/DataWhale_Matplotlib_Matplotlib%E5%88%9D%E7%9B%B8%E8%AF%86/" rel="next" title="Matplotlib初相识">
      Matplotlib初相识 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BA%8F"><span class="nav-number">1.</span> <span class="nav-text">序</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87-%E7%BB%93%E6%9E%9C%E7%9B%B4%E6%8E%A5%E8%9E%8D%E5%90%88"><span class="nav-number">1.1.</span> <span class="nav-text"># 简单加权平均-结果直接融合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%87%BD%E6%95%B0-%E6%A0%B9%E6%8D%AE%E5%8A%A0%E6%9D%83%E8%AE%A1%E7%AE%97"><span class="nav-number">1.2.</span> <span class="nav-text"># 定义结果的加权平均函数 - 根据加权计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%90%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.3.</span> <span class="nav-text"># 根据各模型的预测结果计算MAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%8A%A0%E6%9D%83%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.4.</span> <span class="nav-text"># 根据加权计算MAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E6%AF%94%E9%87%8D%E6%9D%83%E5%80%BC"><span class="nav-number">1.5.</span> <span class="nav-text">## 定义比重权值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%87%BD%E6%95%B0-mean%E5%B9%B3%E5%9D%87"><span class="nav-number">1.5.1.</span> <span class="nav-text"># 定义结果的加权平均函数 - mean平均</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%9D%87%E5%80%BC%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.6.</span> <span class="nav-text"># 根据均值计算MAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%87%BD%E6%95%B0-median%E5%B9%B3%E5%9D%87"><span class="nav-number">1.7.</span> <span class="nav-text"># 定义结果的加权平均函数 - median平均</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%AD%E4%BD%8D%E6%95%B0%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.8.</span> <span class="nav-text"># 根据中位数计算MAE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Stacking%E8%9E%8D%E5%90%88-%E5%9B%9E%E5%BD%92"><span class="nav-number">2.</span> <span class="nav-text"># Stacking融合(回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89Stacking%E8%9E%8D%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text"># 定义Stacking融合函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E7%9A%84%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE-test-prei%E4%BB%A3%E8%A1%A8%E7%AC%ACi%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E5%80%BC-y-test-true%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%80%BC"><span class="nav-number">2.2.</span> <span class="nav-text"># 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E4%B8%80%E4%B8%8BStacking%E8%9E%8D%E5%90%88%E7%9A%84%E6%95%88%E6%9E%9C"><span class="nav-number">2.3.</span> <span class="nav-text"># 看一下Stacking融合的效果</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88-Voting-Stacking%E2%80%A6"><span class="nav-number">3.</span> <span class="nav-text"># 分类模型融合 - Voting,Stacking…</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Voting%E6%8A%95%E7%A5%A8%E6%9C%BA%E5%88%B6"><span class="nav-number">3.1.</span> <span class="nav-text"># Voting投票机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8XGB%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">3.2.</span> <span class="nav-text"># 用XGB分类模型训练数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">3.3.</span> <span class="nav-text"># 用随机森林分类模型训练数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8SVC%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">3.4.</span> <span class="nav-text"># 用SVC训练数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A1%AC%E6%8A%95%E7%A5%A8"><span class="nav-number">3.5.</span> <span class="nav-text"># 硬投票</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E6%8A%95%E7%A5%A8"><span class="nav-number">3.6.</span> <span class="nav-text"># 软投票</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E7%9A%84Stacking-x2F-Blending%E8%9E%8D%E5%90%88"><span class="nav-number">4.</span> <span class="nav-text"># 分类的Stacking&#x2F;Blending融合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5%E6%8A%98Stacking-%E5%8D%B3%E6%AF%8F%E6%AC%A1Stacking%E8%AE%AD%E7%BB%83%E9%83%BD%E4%BC%9A%E5%9C%A8%E7%AC%AC%E4%B8%80%E5%B1%82%E5%9F%BA%E5%AD%A6%E4%B9%A0%E5%99%A8%E8%BF%9B%E8%A1%8C5%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-%E5%86%8D%E8%BF%9B%E5%85%A5%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%AD%A6%E4%B9%A0%E5%99%A8%E8%AE%AD%E7%BB%83"><span class="nav-number">4.1.</span> <span class="nav-text"># 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Blending-%E5%92%8CStacking%E7%B1%BB%E4%BC%BC-%E4%B8%8D%E5%90%8C%E7%82%B9%E5%9C%A8%E4%BA%8E"><span class="nav-number">4.2.</span> <span class="nav-text"># Blending - 和Stacking类似,不同点在于:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%A9%E7%94%A8mlxtend%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E7%9A%84Stacking%E8%9E%8D%E5%90%88"><span class="nav-number">4.3.</span> <span class="nav-text"># 利用mlxtend进行分类的Stacking融合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%85%B6%E5%AE%83%E6%96%B9%E6%B3%95"><span class="nav-number">4.4.</span> <span class="nav-text"># 一些其它方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E4%B8%89%E6%96%B9%E6%A8%A1%E5%9D%97"><span class="nav-number">5.</span> <span class="nav-text">导入三方模块</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%B3%A8%E6%84%8F%E7%82%B9"><span class="nav-number">6.</span> <span class="nav-text">不同模型的数据准备注意点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96%E3%80%81%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE"><span class="nav-number">7.</span> <span class="nav-text">读取、压缩数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lgb"><span class="nav-number">8.</span> <span class="nav-text">lgb</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%97%E6%AE%B5-employmentLength-10%E5%B9%B4%E4%BB%A5%E4%B8%8A%E7%AE%9710%E5%B9%B4%EF%BC%8C1%E5%B9%B4%E4%B8%80%E4%B8%8B%E7%AE%970%E5%B9%B4%EF%BC%9B%E7%84%B6%E5%90%8E%E8%BD%AC%E5%8C%96%E6%88%90%E6%95%B0%E5%80%BC"><span class="nav-number">8.1.</span> <span class="nav-text">字段 employmentLength - 10年以上算10年，1年一下算0年；然后转化成数值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%97%E6%AE%B5-earliesCreditLine-%E5%88%86%E5%88%AB%E6%8F%90%E5%8F%96%E5%B9%B4%E4%BB%BD%E5%92%8C%E6%9C%88%E4%BB%BD%E5%81%9A%E6%8B%BC%E6%8E%A5"><span class="nav-number">8.2.</span> <span class="nav-text">字段 earliesCreditLine - 分别提取年份和月份做拼接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%97%E6%AE%B5-issueDate-%E4%BB%8E2017%E5%B9%B46%E6%9C%881%E6%97%A5%E5%BC%80%E5%A7%8B%EF%BC%9B%E6%95%B0%E6%8D%AE%E6%8C%89%E7%85%A7%E6%AD%A4%E8%8A%82%E7%82%B9%E7%BB%9F%E8%AE%A1%E5%A4%A9%E6%95%B0"><span class="nav-number">8.3.</span> <span class="nav-text">字段 issueDate - 从2017年6月1日开始；数据按照此节点统计天数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E4%B8%80%E4%B8%8B%E7%89%B9%E5%BE%81%E7%9A%84%E7%B1%BB%E5%88%AB%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5"><span class="nav-number">8.4.</span> <span class="nav-text">看一下特征的类别分布情况</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E4%BA%8E%E9%AB%98%E7%BB%B4%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81%EF%BC%8C%E8%BF%9B%E8%A1%8C%E8%BD%AC%E6%8D%A2%EF%BC%8C%E5%8F%96%E4%BB%96%E4%BB%AC%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%95%B0%E9%87%8F%E5%80%BC%E5%92%8C%E6%8E%92%E5%90%8D%E5%80%BC"><span class="nav-number">8.5.</span> <span class="nav-text">对于高维类别特征，进行转换，取他们同类型的数量值和排名值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BE%85%E7%94%A8"><span class="nav-number">8.6.</span> <span class="nav-text">保存数据待用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#xgb-%E4%B8%BB%E8%A6%81one-hot%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81"><span class="nav-number">9.</span> <span class="nav-text">xgb - 主要one-hot类别特征</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E4%BA%8E%E7%BB%B4%E5%BA%A6%E5%A4%A7%E4%BA%8E1%E4%B8%94%E4%B8%8D%E4%BC%9A%E5%BD%A2%E6%88%90%E9%AB%98%E7%BB%B4%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E8%BF%9B%E8%A1%8Cone-hot%E7%BC%96%E7%A0%81"><span class="nav-number">9.1.</span> <span class="nav-text">对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BE%85%E7%94%A8-1"><span class="nav-number">9.2.</span> <span class="nav-text">保存数据待用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#logistic-%E4%B8%BB%E8%A6%81%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81%E7%9A%84%E7%BC%BA%E5%A4%B1%E5%80%BC%E3%80%81%E5%BC%82%E5%B8%B8%E5%80%BC%E3%80%81%E8%BF%9E%E7%BB%AD%E7%89%B9%E5%BE%81%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%A1%B6"><span class="nav-number">10.</span> <span class="nav-text">logistic - 主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86"><span class="nav-number">10.1.</span> <span class="nav-text">缺失值的处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E7%9C%8B%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE"><span class="nav-number">10.2.</span> <span class="nav-text">看看缺失数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E4%B8%8B%E5%8C%BF%E5%90%8D%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5%EF%BC%8C%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%8B%E6%98%AF%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%E8%BF%98%E6%98%AF%E8%BF%9E%E7%BB%AD%E7%89%B9%E5%BE%81"><span class="nav-number">10.3.</span> <span class="nav-text">看下匿名特征的分布情况，判断一下是离散特征还是连续特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E5%85%88%E7%BB%9F%E4%B8%80%E5%B0%86%E8%BF%99%E4%BA%9B%E5%8C%BF%E5%90%8D%E7%89%B9%E5%BE%81%E7%9A%84%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%8D%95%E7%8B%AC%E5%88%86%E4%B8%80%E7%B1%BB"><span class="nav-number">10.4.</span> <span class="nav-text">我先统一将这些匿名特征的缺失值单独分一类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%A9%E4%B8%8B%E6%9C%89%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81%E5%A6%82%E4%B8%8B"><span class="nav-number">10.5.</span> <span class="nav-text">剩下有缺失数据的特征如下</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#employmentLength%E6%98%AF%E5%B0%B1%E4%B8%9A%E5%B9%B4%E9%99%90%EF%BC%8C%E5%92%8C%E9%80%BE%E6%9C%9F%E5%BA%94%E8%AF%A5%E4%B8%8D%E6%98%AF%E5%BC%BA%E5%85%B3%E8%81%94%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%8F%A6%E5%A4%96%E5%87%A0%E4%B8%AA%E7%89%B9%E5%BE%81%E7%BC%BA%E5%A4%B1%E5%BE%88%E5%B0%91%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%87%BA%E7%8E%B0%E6%9C%80%E5%A4%9A%E7%9A%84%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%E6%9D%A5%E4%BB%A3%E6%9B%BF%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="nav-number">10.6.</span> <span class="nav-text">employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#employmentLength"><span class="nav-number">10.6.1.</span> <span class="nav-text">employmentLength</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#revolUtil"><span class="nav-number">10.6.2.</span> <span class="nav-text">revolUtil</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pubRecBankruptcies"><span class="nav-number">10.6.3.</span> <span class="nav-text">pubRecBankruptcies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dti"><span class="nav-number">10.6.4.</span> <span class="nav-text">dti</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#employmentTitle-counts"><span class="nav-number">10.6.5.</span> <span class="nav-text">employmentTitle_counts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#postCode-counts"><span class="nav-number">10.6.6.</span> <span class="nav-text">postCode_counts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#title-counts"><span class="nav-number">10.6.7.</span> <span class="nav-text">title_counts</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AD%9B%E9%80%89%E9%87%8D%E8%A6%81%E7%89%B9%E5%BE%81"><span class="nav-number">11.</span> <span class="nav-text">随机森林筛选重要特征</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%97%E5%88%B0%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">11.1.</span> <span class="nav-text">得到特征重要性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">11.2.</span> <span class="nav-text">异常数据处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E5%BC%80"><span class="nav-number">11.3.</span> <span class="nav-text">两个数据集分开</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">12.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic"><span class="nav-number">12.1.</span> <span class="nav-text">logistic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%92%E5%88%86%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9A%84%E7%89%B9%E5%BE%81%E5%92%8C%E6%A0%87%E7%AD%BE"><span class="nav-number">12.2.</span> <span class="nav-text">划分训练集的特征和标签</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%86%E8%AE%AD%E7%BB%83%E9%9B%86%E5%88%86%E4%B8%BA5%E4%BB%BD%EF%BC%8C4%E4%BB%BD%E4%BD%9C%E4%B8%BA%E8%AE%AD%E7%BB%83%E9%9B%86%EF%BC%8C1%E4%BB%BD%E4%BD%9C%E4%B8%BA%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-number">12.3.</span> <span class="nav-text">将训练集分为5份，4份作为训练集，1份作为验证集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost"><span class="nav-number">12.4.</span> <span class="nav-text">xgboost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">12.4.1.</span> <span class="nav-text">数据准备</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#xgb-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">13.</span> <span class="nav-text">## xgb - 梯度提升决策树</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">14.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">小书包</p>
  <div class="site-description" itemprop="description">种一棵树最好的时间是十年前，其次是现在</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/FelixBigTree" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;FelixBigTree" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/598376210@qq.com" title="E-Mail → 598376210@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-04 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小书包</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">198k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:01</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
