<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="模型融合_代码示例部分#导入工具包12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport pandas as pdfrom sklearn import metricsfrom sklearn import linear_modelfrom sklearn.datasets">
<meta property="og:type" content="article">
<meta property="og:title" content="二手车交易价格预测_Task5_模型融合">
<meta property="og:url" content="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/index.html">
<meta property="og:site_name" content="大宝的树屋">
<meta property="og:description" content="模型融合_代码示例部分#导入工具包12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport pandas as pdfrom sklearn import metricsfrom sklearn import linear_modelfrom sklearn.datasets">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2023-04-24T08:03:14.616Z">
<meta property="article:modified_time" content="2023-04-24T08:17:48.167Z">
<meta property="article:author" content="小书包">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="金融风控">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>二手车交易价格预测_Task5_模型融合 | 大宝的树屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

	<a target="_blank" rel="noopener" href="https://your-url" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">大宝的树屋</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="小书包">
      <meta itemprop="description" content="种一棵树最好的时间是十年前，其次是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大宝的树屋">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          二手车交易价格预测_Task5_模型融合
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-24 16:03:14 / 修改时间：16:17:48" itemprop="dateCreated datePublished" datetime="2023-04-24T16:03:14+08:00">2023-04-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">二手车交易价格预测</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>29k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>26 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="模型融合-代码示例部分"><a href="#模型融合-代码示例部分" class="headerlink" title="模型融合_代码示例部分"></a>模型融合_代码示例部分</h1><h2 id="导入工具包"><a href="#导入工具包" class="headerlink" title="#导入工具包"></a>#导入工具包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs     <span class="comment"># 这是打包好的波士顿房价数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier     <span class="comment"># 分类决策树模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier     <span class="comment"># 随机森林回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier       <span class="comment"># 分类投票模型</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier        <span class="comment"># xgboost用于解决f分类问题</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression     <span class="comment"># 逻辑回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC     <span class="comment"># 支持向量机模型 - 用于分类问题</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR     <span class="comment"># 支持向量机模型 - 用于回归问题</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split    <span class="comment"># 用于拆分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons     <span class="comment"># 创建月亮形的数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, roc_auc_score   <span class="comment"># ROC-Auc指标,评价模型得分用的</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score     <span class="comment"># 用于做交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV        <span class="comment"># 用于网格搜索</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold     <span class="comment"># 分层交叉验证,每一折中都保持着原始数据中各个类别的比例关系</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier, GradientBoostingClassifier, GradientBoostingRegressor</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> itertools    <span class="comment"># 用于创建自定义的迭代器</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec  <span class="comment"># 用于调整子图的位置大小</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment"># k近邻分类算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB  <span class="comment"># 先验为高斯分布的朴素贝叶斯</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier   <span class="comment"># 快速完成对sklearn模型的stacking</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_learning_curves   <span class="comment"># 绘制学习曲线</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_decision_regions  <span class="comment"># 绘制决策边界</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing       <span class="comment"># 数据归一化(标准化)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA, FastICA, FactorAnalysis, SparsePCA   <span class="comment"># 用于降维等特征处理</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)   <span class="comment"># 忽略警告</span></span><br></pre></td></tr></table></figure>
<h2 id="简单加权平均-结果直接融合"><a href="#简单加权平均-结果直接融合" class="headerlink" title="# 简单加权平均-结果直接融合"></a># 简单加权平均-结果直接融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">生成一些简单的样本数据,</span></span><br><span class="line"><span class="string">test_prei - 代表第i个模型的预测值</span></span><br><span class="line"><span class="string">y_test_true - 代表真实值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<h2 id="定义结果的加权平均函数-根据加权计算"><a href="#定义结果的加权平均函数-根据加权计算" class="headerlink" title="# 定义结果的加权平均函数 - 根据加权计算"></a># 定义结果的加权平均函数 - 根据加权计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_method</span>(<span class="params">test_pre1, test_pre2, test_pre3, w=[<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>]</span>):</span><br><span class="line">    weighted_result = w[<span class="number">0</span>] * pd.Series(test_pre1) + w[<span class="number">1</span>] * pd.Series(test_pre2) + w[<span class="number">2</span>] * pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> weighted_result</span><br></pre></td></tr></table></figure>
<h2 id="根据各模型的预测结果计算MAE"><a href="#根据各模型的预测结果计算MAE" class="headerlink" title="# 根据各模型的预测结果计算MAE"></a># 根据各模型的预测结果计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">metrics.mean_absolute_error - 多维数组MAE的计算方法</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred1 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred2 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred3 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre3))</span><br><span class="line"></span><br><span class="line">Pred1 MAE: <span class="number">0.1750000000000001</span></span><br><span class="line">Pred2 MAE: <span class="number">0.07499999999999993</span></span><br><span class="line">Pred3 MAE: <span class="number">0.10000000000000009</span></span><br></pre></td></tr></table></figure>
<h2 id="根据加权计算MAE"><a href="#根据加权计算MAE" class="headerlink" title="# 根据加权计算MAE"></a># 根据加权计算MAE</h2><h2 id="定义比重权值"><a href="#定义比重权值" class="headerlink" title="## 定义比重权值"></a>## 定义比重权值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]</span><br><span class="line">weighted_pre = weighted_method(test_pre1, test_pre2, test_pre3, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Weighted_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, weighted_pre))</span><br><span class="line"></span><br><span class="line">Weighted_pre MAE: <span class="number">0.05750000000000027</span></span><br></pre></td></tr></table></figure>
<h3 id="定义结果的加权平均函数-mean平均"><a href="#定义结果的加权平均函数-mean平均" class="headerlink" title="# 定义结果的加权平均函数 - mean平均"></a># 定义结果的加权平均函数 - mean平均</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    mean_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                             pd.Series(test_pre2),</span><br><span class="line">                             pd.Series(test_pre3)], axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mean_result</span><br></pre></td></tr></table></figure>
<h2 id="根据均值计算MAE"><a href="#根据均值计算MAE" class="headerlink" title="# 根据均值计算MAE"></a># 根据均值计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Mean_pre = mean_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Mean_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Mean_pre MAE: <span class="number">0.06666666666666693</span></span><br></pre></td></tr></table></figure>
<h2 id="定义结果的加权平均函数-median平均"><a href="#定义结果的加权平均函数-median平均" class="headerlink" title="# 定义结果的加权平均函数 - median平均"></a># 定义结果的加权平均函数 - median平均</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">median_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    median_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                               pd.Series(test_pre2),</span><br><span class="line">                               pd.Series(test_pre3)], axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> median_result</span><br></pre></td></tr></table></figure>
<h2 id="根据中位数计算MAE"><a href="#根据中位数计算MAE" class="headerlink" title="# 根据中位数计算MAE"></a># 根据中位数计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Median_pre = median_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Median_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Median_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Median_pre MAE: <span class="number">0.07500000000000007</span></span><br></pre></td></tr></table></figure>
<h1 id="Stacking融合-回归"><a href="#Stacking融合-回归" class="headerlink" title="# Stacking融合(回归)"></a># Stacking融合(回归)</h1><h2 id="定义Stacking融合函数"><a href="#定义Stacking融合函数" class="headerlink" title="# 定义Stacking融合函数"></a># 定义Stacking融合函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Stacking_method</span>(<span class="params">train_reg1, train_reg2, train_reg3,</span></span><br><span class="line"><span class="params">                    y_train_true,</span></span><br><span class="line"><span class="params">                    test_pre1, test_pre2, test_pre3,</span></span><br><span class="line"><span class="params">                    model_L2=linear_model.LinearRegression(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param train_reg1:  第一个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg2:  第二个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg3:  第三个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param y_train_true:    train真实的标签</span></span><br><span class="line"><span class="string">    :param test_pre1:   第一个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre2:   第二个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre3:   第三个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param model_L2:    次级模型:以真实训练集的标签为标签,以多个模型训练训练集后得到的标签合并后的数据集为特征进行训练</span></span><br><span class="line"><span class="string">                        注意:次级模型不宜选取的太复杂,这样会导致模型在训练集上过拟合,测试集泛化效果差</span></span><br><span class="line"><span class="string">    :return:            训练好的次机模型预测test数据集得到的预测值 - Stacking_result</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), pd.Series(train_reg3)], axis=<span class="number">1</span>).values,</span><br><span class="line">                 y_train_true)      <span class="comment"># 次级模型训练</span></span><br><span class="line">    stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),</span><br><span class="line">                                                  pd.Series(test_pre2), pd.Series(test_pre3)], axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> stacking_result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值"><a href="#生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值" class="headerlink" title="# 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值"></a># 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<h2 id="看一下Stacking融合的效果"><a href="#看一下Stacking融合的效果" class="headerlink" title="# 看一下Stacking融合的效果"></a># 看一下Stacking融合的效果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_L2 = linear_model.LinearRegression()      <span class="comment"># 不设定这个参数也可以,创建函数的时候默认了</span></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,</span><br><span class="line">                               test_pre1, test_pre2, test_pre3, model_L2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Stacking_pre MAE: &#x27;</span>, metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Stacking_pre MAE:  <span class="number">0.042134831460675204</span></span><br><span class="line"><span class="comment"># 发现模型效果相对于之前有了更近一步的提升</span></span><br></pre></td></tr></table></figure>
<h1 id="分类模型融合-Voting-Stacking…"><a href="#分类模型融合-Voting-Stacking…" class="headerlink" title="# 分类模型融合 - Voting,Stacking…"></a># 分类模型融合 - Voting,Stacking…</h1><h2 id="Voting投票机制"><a href="#Voting投票机制" class="headerlink" title="# Voting投票机制"></a># Voting投票机制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Voting - 投票机制</span></span><br><span class="line"><span class="string">        1.硬投票 - 对多个模型直接进行投票,不区分模型结果的相对重要度,最终投票数最多的类为最终被预测的类</span></span><br><span class="line"><span class="string">        2.软投票 - 和硬投票原理相同,增加了设置权重的功能,可以为不同模型设置不同权重,进而区别模型不同的重要度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 硬投票</span></span><br><span class="line">iris = datasets.load_iris()     <span class="comment"># 读取鸢尾花数据集 - 分类问题</span></span><br><span class="line"></span><br><span class="line">x = iris.data   <span class="comment"># 分离特征集和标签</span></span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)    <span class="comment"># 训练集和测试集按照7:3比例切分</span></span><br></pre></td></tr></table></figure>
<h2 id="用XGB分类模型训练数据"><a href="#用XGB分类模型训练数据" class="headerlink" title="# 用XGB分类模型训练数据"></a># 用XGB分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">colsample_bytree - 训练每棵树时，使用的特征占全部特征的比例</span></span><br><span class="line"><span class="string">objective - 目标函数</span></span><br><span class="line"><span class="string">            二分类问题 - binary:logistic - 返回概率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="用随机森林分类模型训练数据"><a href="#用随机森林分类模型训练数据" class="headerlink" title="# 用随机森林分类模型训练数据"></a># 用随机森林分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">n_estimators - 随机森林中决策树的个数</span></span><br><span class="line"><span class="string">max_depth - 决策树的最大深度</span></span><br><span class="line"><span class="string">            如果值为None,那么会扩展节点,直到所有的叶子是纯净的,或者直到所有叶子包含少于min_sample_split的样本</span></span><br><span class="line"><span class="string">min_samples_split - 分割内部节点所需要的最小样本数量</span></span><br><span class="line"><span class="string">min_samples_leaf - 需要在叶子结点上的最小样本数量</span></span><br><span class="line"><span class="string">oob_score - 是否使用袋外样本来估计泛化精度</span></span><br><span class="line"><span class="string">            树的生成过程并不会使用所有的样本,未使用的样本就叫(out_of_bag)oob袋外样本,通过袋外样本,可以评估这个树的准确度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="用SVC训练数据"><a href="#用SVC训练数据" class="headerlink" title="# 用SVC训练数据"></a># 用SVC训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">支持向量机 - 分类算法，但是也可以做回归,根据输入的数据不同可做不同的模型</span></span><br><span class="line"><span class="string">            1.若输入标签为连续值则做回归</span></span><br><span class="line"><span class="string">            2.若输入标签为分类值则用SVC()做分类</span></span><br><span class="line"><span class="string">            支持向量机的学习策略是间隔最大化，最终可转化为一个凸二次规划问题的求解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">C - 惩罚参数;   值越大,对误分类的惩罚大,不容犯错,于是训练集测试准确率高,但是泛化能力弱</span></span><br><span class="line"><span class="string">                值越小,对误分类的惩罚小,允许犯错,泛化能力较强</span></span><br><span class="line"><span class="string">probability - 是否采用概率估计,默认为False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="硬投票"><a href="#硬投票" class="headerlink" title="# 硬投票"></a># 硬投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">eclf - 其实就是三个模型的集成算法,硬投票决定最终被预测的类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)     <span class="comment"># 本质是Ensemble</span></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.05</span>) [Ensemble]</span><br></pre></td></tr></table></figure>
<h2 id="软投票"><a href="#软投票" class="headerlink" title="# 软投票"></a># 软投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.8</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.8</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)</span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;soft&#x27;</span>, weights=[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [Ensemble]</span><br></pre></td></tr></table></figure>
<h1 id="分类的Stacking-x2F-Blending融合"><a href="#分类的Stacking-x2F-Blending融合" class="headerlink" title="# 分类的Stacking&#x2F;Blending融合"></a># 分类的Stacking&#x2F;Blending融合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Stacking是一种分层模型集成框架,以两层为例</span></span><br><span class="line"><span class="string">        第一层由多个基学习器组成,其输入为原始训练集</span></span><br><span class="line"><span class="string">        第二层的模型则是以第一层学习器的输出作为训练集进行再训练,从而得到完整的stacking模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># ## 创建训练用的数据集</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]  <span class="comment"># 100个样本</span></span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 模型融合中使用到的各个单模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LogisticRegression()</span></span><br><span class="line"><span class="string">            solver - 用来优化权重     &#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;,默认adam,</span></span><br><span class="line"><span class="string">                                        lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好</span></span><br><span class="line"><span class="string">                                        sgd - 随机梯度下降 </span></span><br><span class="line"><span class="string">                                        adam - 机遇随机梯度的优化器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RandomForestClassifier()</span></span><br><span class="line"><span class="string">            n_estimators - 决策树个数</span></span><br><span class="line"><span class="string">            n_jobs - 用于拟合和预测的并行运行的工作数量,如果值为-1,那么工作数量被设置为核的数量</span></span><br><span class="line"><span class="string">            criterion - 衡量分裂质量的性能</span></span><br><span class="line"><span class="string">                        1.gini - Gini impurity衡量的是从一个集合中随机选择一个元素</span></span><br><span class="line"><span class="string">                                基于该集合中标签的概率分布为元素分配标签的错误率</span></span><br><span class="line"><span class="string">                                Gini impurity的计算就非常简单了,即1减去所有分类正确的概率,得到的就是分类不正确的概率</span></span><br><span class="line"><span class="string">                                若元素数量非常多,且所有元素单独属于一个分类时，Gini不纯度达到极小值0</span></span><br><span class="line"><span class="string">                        2.entropy - 信息增益熵</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">ExtraTreesClassifier() - 极端随机树</span></span><br><span class="line"><span class="string">    该算法与随机森林算法十分相似,都是由许多决策树构成,但该算法与随机森林有两点主要的区别:</span></span><br><span class="line"><span class="string">        1.随机森林应用的是Bagging模型,而ET是使用所有的训练样本得到每棵决策树,也就是每棵决策树应用的是相同的全部训练样本</span></span><br><span class="line"><span class="string">            关于Bagging和Boosting的差别,可以参考 https://www.cnblogs.com/earendil/p/8872001.html</span></span><br><span class="line"><span class="string">        2.随机森林是在一个随机子集内得到最佳分叉属性,而ET是完全随机的得到分叉值,从而实现对决策树进行分叉的</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"> Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                            1.lad - 绝对损失</span></span><br><span class="line"><span class="string">                                            2.huber - Huber损失</span></span><br><span class="line"><span class="string">                                            3.quantile - 分位数损失</span></span><br><span class="line"><span class="string">                                            4.ls - 均方差损失(默认)</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">                                    alpha - 当我们使用Huber损失和分位数损失&#x27;quantile&#x27;时,需要指定分位数的值,只有regressor有</span></span><br><span class="line"><span class="string">                                    </span></span><br><span class="line"><span class="string">                            GradientBoostingClassifier() - 参数绝大多数和Regressor相同,不同的是loss函数</span></span><br><span class="line"><span class="string">                                            1.deviance - 对数似然损失函数(默认)</span></span><br><span class="line"><span class="string">                                            2.exponential - 指数损失函数       </span></span><br><span class="line"><span class="string">参考网址: https://www.cnblogs.com/pinard/p/6143927.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((X.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))  <span class="comment"># 全零数组,行取训练集的个数,列取模型个数</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 全零数组,行取测试集的个数,列取模型个数</span></span><br></pre></td></tr></table></figure>
<h2 id="5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练"><a href="#5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练" class="headerlink" title="# 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练"></a># 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)     <span class="comment"># # 分层交叉验证,每一折中都保持着原始数据中各个类别的比例关系(测试集和训练集分离)</span></span><br><span class="line">skf = skf.split(X, y)     <span class="comment"># 把特征和标签分离</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">enumerate() - 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列,同时列出数据和数据下标,一般用在for循环当中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 30行5列的全0数组</span></span><br><span class="line">    <span class="comment"># 五折交叉训练,使用第i个部分作为预测集,剩余部分为验证集,获得的预测值成为第i部分的新特征</span></span><br><span class="line">    <span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf):</span><br><span class="line">        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        <span class="comment"># 将对测试集的概率预测第二列(也就是结果为1)的概率装进y_submission中</span></span><br><span class="line">        y_submission = clf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">        dataset_blend_train[test, j] = y_submission     <span class="comment"># 把预测验证集(比如第一折)的结果依次对应装进dataset_blend_train中</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        predict_proba() - 返回的是一个n行k列的数组</span></span><br><span class="line"><span class="string">                            第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率,并且每一行的概率和为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        因为我们采取到的数据集的标签只有0或1,所以predict_proba返回的概率只有两个</span></span><br><span class="line"><span class="string">                    如果左边的概率大于0.5,那么预测值为0</span></span><br><span class="line"><span class="string">                    如果右边的概率大于0.5,那么预测值为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># # 将对测试集的概率预测的第二列(也就是结果为1)的概率装进dataset_blend_test_j中</span></span><br><span class="line">        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对于测试集,直接用这5个模型的预测值均值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = dataset_blend_test_j.mean(<span class="number">1</span>)     <span class="comment"># mean(1) - 求每行数的平均值(五折预测测试集的平均值)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>)    <span class="comment"># 次级学习器再次训练</span></span><br><span class="line">clf.fit(dataset_blend_train, y)     <span class="comment"># 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_blend_test)[:, <span class="number">1</span>]  <span class="comment"># 把第一层预测测试集的结果作为新特征,预测测试集的标签</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ROC曲线和AUC - 用来评价一个二值分类器(binary classifier)的优劣,用于衡量&#x27;二分类问题&#x27;机器学习算法性能(泛化能力)</span></span><br><span class="line"><span class="string">AUC - ROC曲线下的面积</span></span><br><span class="line"><span class="string">      AUC的取值范围在0.5和1之间</span></span><br><span class="line"><span class="string">      使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好</span></span><br><span class="line"><span class="string">      而作为一个数值,对应AUC更大的分类器效果更好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h2 id="Blending-和Stacking类似-不同点在于"><a href="#Blending-和Stacking类似-不同点在于" class="headerlink" title="# Blending - 和Stacking类似,不同点在于:"></a># Blending - 和Stacking类似,不同点在于:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.Stacking - 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string">2.Blending - 把第一层得到训练集中的30%的验证集的结果作为新特征继续训练,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Blending优点 - 比stacking简单,因为不用进行k次的交叉验证来获得stacker feature</span></span><br><span class="line"><span class="string">                避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</span></span><br><span class="line"><span class="string">Blending缺点- 使用了很少的数据,可能会过拟合,没有stacking使用多次的交叉验证来的稳健</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把训练数据分成d1(子训练集),d2(验证集)两部分 - 对半分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))   <span class="comment"># 35行5列的全0数组</span></span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))      <span class="comment"># 30行5列的全0数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 用子训练集依次训练各个模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    <span class="comment"># 返回模型对验证集的预测值为1的概率</span></span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 结果装进dataset_d1中 - 表示用子训练集训练的模型预测验证集标签的结果 - 就是上文说的30%的数据</span></span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment"># 建立第二层模型的特征 - 用第一层模型预测测试集的结果作为新的特征</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 看一下预测的预测集标签和真实的预测集标签的roc_auc_score</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用第二层模型训练特征</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)   <span class="comment"># 用验证集的第一层模型预测结果作为特征,用验证集的真实标签作为标签,再次训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, <span class="number">1</span>]  <span class="comment"># 用第一层模型预测测试集的结果作为特征,用第二层模型预测训练集返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Blending: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">Val auc Score of Blending: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h2 id="利用mlxtend进行分类的Stacking融合"><a href="#利用mlxtend进行分类的Stacking融合" class="headerlink" title="# 利用mlxtend进行分类的Stacking融合"></a># 利用mlxtend进行分类的Stacking融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, <span class="number">1</span>:<span class="number">3</span>], iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=<span class="number">1</span>)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">StackingClassifier() - 快速Stacking融合的方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">classifiers - 一级分类器列表</span></span><br><span class="line"><span class="string">meta_classifier - 二级分类器(元分类器)</span></span><br><span class="line"><span class="string">use_probas - 如果为True,则基于预测的概率而不是类标签来训练元分类器,默认为False</span></span><br><span class="line"><span class="string">average_probas - 如果为真,将概率平均为元特征,默认为False</span></span><br><span class="line"><span class="string">verbose - 是否输出到日志</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                          meta_classifier=lr)</span><br><span class="line">label = [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>, <span class="string">&#x27;Stacking Classifier&#x27;</span>]</span><br><span class="line">clf_list = [clf1, clf2, clf3, sclf]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># 网格布局,每行2个,每列2个</span></span><br><span class="line">grid = itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)  <span class="comment"># 求多个可迭代对象的笛卡尔积,其实就是更加灵活调整网格的大小</span></span><br><span class="line"></span><br><span class="line">clf_cv_mean = []    <span class="comment"># 存放每个模型的准确率的均值</span></span><br><span class="line">clf_cv_std = []     <span class="comment"># 存放每个模型的准确率的标准差</span></span><br><span class="line"><span class="keyword">for</span> clf, label, grd <span class="keyword">in</span> <span class="built_in">zip</span>(clf_list, label, grid):</span><br><span class="line"></span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 3折交叉验证,评分标准为模型准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.2f (+/- %.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line">    clf_cv_mean.append(scores.mean())</span><br><span class="line">    clf_cv_std.append(scores.std())</span><br><span class="line"></span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(label)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.02</span>) [Stacking Classifier]</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出,融合后的曲线更加优秀</p>
<h2 id="一些其它方法"><a href="#一些其它方法" class="headerlink" title="# 一些其它方法"></a># 一些其它方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将特征放进模型中预测,并将预测结果变换并作为新的特征加入原有特征中,再经过模型预测结果(Stacking变化)</span></span><br><span class="line"><span class="string">可以反复预测多次将结果加入最后的特征中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ensemble_add_feature</span>(<span class="params">train, test, target, clfs</span>):</span><br><span class="line">    <span class="comment"># n_folds = 5</span></span><br><span class="line">    <span class="comment"># skf = list(StratifiedKFold(y, n_folds=n_folds))</span></span><br><span class="line">    train_ = np.zeros((train.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    test_ = np.zeros((test.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="comment"># 依次训练单个模型</span></span><br><span class="line">        <span class="built_in">print</span>(j, clf)</span><br><span class="line">        <span class="comment"># 使用第1部分作为预测,第2部分来训练模型(第1部分预测的输出作为第2部分的新特征)</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train]</span></span><br><span class="line">        clf.fit(train, target)  <span class="comment"># 训练模型</span></span><br><span class="line">        y_train = clf.predict(train)    <span class="comment"># 模型在训练集中的预测值</span></span><br><span class="line">        y_test = clf.predict(test)      <span class="comment"># 模型在测试集中的预测值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成新特征</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        j 从0开始递增,构建新的特征集,特征为训练集和测试集各自的预测值的平方</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_[:, j*<span class="number">2</span>] = y_train ** <span class="number">2</span></span><br><span class="line">        test_[:, j*<span class="number">2</span>] = y_test ** <span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)    <span class="comment"># np.exp(a) - 返回e的a次方</span></span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method:&#x27;</span>, j)</span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf = LogisticRegression()  <span class="comment"># 次级模型</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.3</span>)</span><br><span class="line">x_train = pd.DataFrame(x_train)     <span class="comment"># 转换成DataFrame格式,方便后续构造新特征</span></span><br><span class="line">x_test = pd.DataFrame(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给出模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 新特征的构造 - 用上面的各个单模型预测训练集和测试集的结果,作为新特征</span></span><br><span class="line">New_train, New_test = ensemble_add_feature(x_train, x_test, y_train, clfs)</span><br><span class="line">clf.fit(New_train, y_train)     <span class="comment"># 用训练集的新特征和训练集的真实标签训练数据</span></span><br><span class="line">y_emb = clf.predict_proba(New_test)[:, <span class="number">1</span>]   <span class="comment"># 用训练好的模型得到新的测试集特征返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_test, y_emb)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method: <span class="number">4</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h1 id="本赛题示例"><a href="#本赛题示例" class="headerlink" title="本赛题示例"></a>本赛题示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据读取</span></span><br><span class="line">Train_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\used_car_train_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">TestA_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\used_car_testA_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(TestA_data.shape)</span><br></pre></td></tr></table></figure>
<h2 id="回顾一下数据"><a href="#回顾一下数据" class="headerlink" title="# 回顾一下数据"></a># 回顾一下数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Train_data.head()</span><br><span class="line">Train_data.info()   <span class="comment"># 20个浮点类型字段,10个整数型字段,1个object字段</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过info(),训练集发现一共有4个字段有缺失值</span></span><br><span class="line"><span class="string">                          model字段有1个缺失值</span></span><br><span class="line"><span class="string">                          bodyType字段有4506个缺失值</span></span><br><span class="line"><span class="string">                          fuelType字段有8680个缺失值</span></span><br><span class="line"><span class="string">                          gearbox字段有5981个缺失值            </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们把数值型字段和非数值型字段做一个区分</span></span><br><span class="line">numerical_cols = Train_data.select_dtypes(exclude=<span class="string">&#x27;object&#x27;</span>).columns     <span class="comment"># 这是所有数值类型的字段</span></span><br><span class="line"><span class="built_in">print</span>(numerical_cols)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中SaleID(合同号),name(汽车交易名称),regDate(汽车注册时间)三个字段看似没有用处,暂时抛弃这些特征</span></span><br><span class="line"><span class="comment"># price是标签字段,不放在特征集中</span></span><br><span class="line">feature_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> numerical_cols <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;SaleID&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;regDate&#x27;</span>, <span class="string">&#x27;price&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 区分特征集和标签集</span></span><br><span class="line">X_data = Train_data[feature_cols]</span><br><span class="line">Y_data = Train_data[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">X_test = TestA_data[feature_cols]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X train shape:&#x27;</span>, X_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X test shape:&#x27;</span>, X_test.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X train shape: (<span class="number">150000</span>, <span class="number">26</span>)</span><br><span class="line">X test shape: (<span class="number">50000</span>, <span class="number">26</span>)</span><br></pre></td></tr></table></figure>
<h2 id="创建一个统计函数-方便后续信息统计"><a href="#创建一个统计函数-方便后续信息统计" class="headerlink" title="# 创建一个统计函数,方便后续信息统计"></a># 创建一个统计函数,方便后续信息统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sta_inf</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_min:&#x27;</span>, np.<span class="built_in">min</span>(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_max:&#x27;</span>, np.<span class="built_in">max</span>(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_mean:&#x27;</span>, np.mean(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_ptp:&#x27;</span>, np.ptp(data))    <span class="comment"># ptp - 最大值和最小值的差值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_std:&#x27;</span>, np.std(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_var:&#x27;</span>, np.var(data))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sta of label&#x27;</span>, sta_inf(Y_data))      <span class="comment"># 看一下训练集标签的统计分布</span></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: <span class="number">11</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">99999</span></span><br><span class="line">_mean: <span class="number">5923.327333333334</span></span><br><span class="line">_ptp: <span class="number">99988</span></span><br><span class="line">_std: <span class="number">7501.973469876438</span></span><br><span class="line">_var: <span class="number">56279605.94272992</span></span><br></pre></td></tr></table></figure>
<h2 id="简单填补训练集和测试集的缺失值-用-1填充"><a href="#简单填补训练集和测试集的缺失值-用-1填充" class="headerlink" title="# 简单填补训练集和测试集的缺失值 - 用-1填充"></a># 简单填补训练集和测试集的缺失值 - 用-1填充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_data = X_data.fillna(-<span class="number">1</span>)</span><br><span class="line">X_test = X_test.fillna(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="为了代码简洁-创建好模型训练函数"><a href="#为了代码简洁-创建好模型训练函数" class="headerlink" title="# 为了代码简洁,创建好模型训练函数"></a># 为了代码简洁,创建好模型训练函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 线性回归</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_lr</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    reg_model = linear_model.LinearRegression()</span><br><span class="line">    reg_model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> reg_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## Ridge 岭回归 - 加入了l2正则化的线性回归</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">L2正则化 - 岭回归 - 模型被限制在圆形区域(二维区域下),损失函数的最小值因为圆形约束没有角,所以不会使得权重为0,但是可以使得权重</span></span><br><span class="line"><span class="string">                    都尽可能的小,最后得到一个所有参数都比较小的模型,这样模型比较简单,能适应不同数据集,一定程度上避免了过拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_ridge</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    reg_model = linear_model.Ridge(alpha=<span class="number">0.8</span>)    <span class="comment"># alpha - 正则化系数</span></span><br><span class="line">    reg_model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> reg_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## Lasso回归 - 加入了l1正则化的线性回归</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">L1正则化 - Lasso回归 - 模型被限制在正方形区域(二维区域下),损失函数的最小值往往在正方形(约束)的角上,很多权值为0(多维),所以可以</span></span><br><span class="line"><span class="string">                        实现模型的稀疏性(生成稀疏权值矩阵,进而用于特征选择</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_lasso</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    reg_model = linear_model.LassoCV()</span><br><span class="line">    reg_model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> reg_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## gbdt -梯度下降树 - 传统机器学习算法里面是对真实分布拟合的最好的几种算法之一</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Boosting算法思想 -  一堆弱分类器的组合就可以成为一个强分类器;</span></span><br><span class="line"><span class="string">                    不断地在错误中学习，迭代来降低犯错概率</span></span><br><span class="line"><span class="string">                    通过一系列的迭代来优化分类结果,每迭代一次引入一个弱分类器,来克服现在已经存在的弱分类器组合的短板</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Adaboost      - 整个训练集上维护一个分布权值向量W</span></span><br><span class="line"><span class="string">                        用赋予权重的训练集通过弱分类算法产生分类假设（基学习器）y(x)</span></span><br><span class="line"><span class="string">                        然后计算错误率,用得到的错误率去更新分布权值向量w</span></span><br><span class="line"><span class="string">                        对错误分类的样本分配更大的权值,正确分类的样本赋予更小的权值</span></span><br><span class="line"><span class="string">                        每次更新后用相同的弱分类算法产生新的分类假设,这些分类假设的序列构成多分类器</span></span><br><span class="line"><span class="string">                        对这些多分类器用加权的方法进行联合,最后得到决策结果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">参考资料:https://www.cnblogs.com/zhubinwang/p/5170087.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_gbdt</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    estimator = GradientBoostingRegressor(loss=<span class="string">&#x27;ls&#x27;</span>, subsample=<span class="number">0.85</span>, max_depth=<span class="number">5</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line">    param_grid = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.08</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;</span><br><span class="line">    gbdt = GridSearchCV(estimator, param_grid, cv=<span class="number">3</span>)    <span class="comment"># 网格搜索最佳参数,3折交叉检验</span></span><br><span class="line">    gbdt.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(gbdt.best_params_)    <span class="comment"># 输出最佳参数</span></span><br><span class="line">    <span class="built_in">print</span>(gbdt.best_estimator_)</span><br><span class="line">    <span class="keyword">return</span> gbdt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## xgb - 梯度提升决策树</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">XGBRegressor - 梯度提升回归树,也叫梯度提升机</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                采用连续的方式构造树,每棵树都试图纠正前一棵树的错误</span></span><br><span class="line"><span class="string">                与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝</span></span><br><span class="line"><span class="string">                从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快</span></span><br><span class="line"><span class="string">                </span></span><br><span class="line"><span class="string">                gamma - 定了节点分裂所需的最小损失函数下降值,这个参数的值越大,算法越保守</span></span><br><span class="line"><span class="string">                subsample - 这个参数控制对于每棵树随机采样的比例,减小这个参数的值,算法会更加保守,避免过拟合</span></span><br><span class="line"><span class="string">                colsample_bytree - 用来控制每棵随机采样的列数的占比</span></span><br><span class="line"><span class="string">                learning_rate - 学习速率,用于控制树的权重,xgb模型在进行完每一轮迭代之后,会将叶子节点的分数乘上该系数,</span></span><br><span class="line"><span class="string">                                以便于削弱各棵树的影响,避免过拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_xgb</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    model = xgb.XGBRegressor(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.08</span>, gamma=<span class="number">0</span>,</span><br><span class="line">                             subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">    model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## lgb - xgb加强版</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LightGBM - 使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低</span></span><br><span class="line"><span class="string">            思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram</span></span><br><span class="line"><span class="string">            然后遍历训练数据，统计每个离散值在直方图中的累计统计量</span></span><br><span class="line"><span class="string">            在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            LightGBM采用leaf-wise生长策略:</span></span><br><span class="line"><span class="string">            每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。</span></span><br><span class="line"><span class="string">            因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度</span></span><br><span class="line"><span class="string">            Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合</span></span><br><span class="line"><span class="string">            因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合</span></span><br><span class="line"><span class="string">参数:</span></span><br><span class="line"><span class="string">num_leaves - 控制了叶节点的数目,它是控制树模型复杂度的主要参数,取值应 &lt;= 2 ^（max_depth）</span></span><br><span class="line"><span class="string">bagging_fraction - 每次迭代时用的数据比例,用于加快训练速度和减小过拟合</span></span><br><span class="line"><span class="string">feature_fraction - 每次迭代时用的特征比例,例如为0.8时,意味着在每次迭代中随机选择80％的参数来建树,</span></span><br><span class="line"><span class="string">                    boosting为random forest时用</span></span><br><span class="line"><span class="string">min_data_in_leaf - 每个叶节点的最少样本数量。</span></span><br><span class="line"><span class="string">                    它是处理leaf-wise树的过拟合的重要参数</span></span><br><span class="line"><span class="string">                    将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合</span></span><br><span class="line"><span class="string">max_depth - 控制了树的最大深度,该参数可以显式的限制树的深度</span></span><br><span class="line"><span class="string">n_estimators - 分多少颗决策树(总共迭代的次数)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">objective - 问题类型</span></span><br><span class="line"><span class="string">            regression - 回归任务,使用L2损失函数</span></span><br><span class="line"><span class="string">            regression_l1 - 回归任务,使用L1损失函数</span></span><br><span class="line"><span class="string">            huber - 回归任务,使用huber损失函数</span></span><br><span class="line"><span class="string">            fair - 回归任务,使用fair损失函数</span></span><br><span class="line"><span class="string">            mape (mean_absolute_precentage_error) - 回归任务,使用MAPE损失函数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_lgb</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    estimator = lgb.LGBMRegressor(num_leaves=<span class="number">63</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line">    param_grid = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]&#125;</span><br><span class="line">    gbm = GridSearchCV(estimator, param_grid)</span><br><span class="line">    gbm.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(gbm.best_params_)</span><br><span class="line">    <span class="keyword">return</span> gbm</span><br></pre></td></tr></table></figure>
<h2 id="XGB的五折交叉回归验证-这里只是举例-别的模型也可以这么验证"><a href="#XGB的五折交叉回归验证-这里只是举例-别的模型也可以这么验证" class="headerlink" title="# XGB的五折交叉回归验证 - 这里只是举例,别的模型也可以这么验证"></a># XGB的五折交叉回归验证 - 这里只是举例,别的模型也可以这么验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">xgr = xgb.XGBRegressor(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.1</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scores_train = []   <span class="comment"># 每次模型训练训练集中子训练集的得分</span></span><br><span class="line">scores = []         <span class="comment"># 每次模型训练训练集中验证集的得分</span></span><br><span class="line"></span><br><span class="line">sk = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">0</span>)  <span class="comment"># shuffle判断是否在每次抽样时对样本进行清洗</span></span><br><span class="line"><span class="keyword">for</span> train_ind, val_ind <span class="keyword">in</span> sk.split(X_data, Y_data):</span><br><span class="line">    train_x = X_data.iloc[train_ind].values</span><br><span class="line">    train_y = Y_data.iloc[train_ind]</span><br><span class="line">    val_x = X_data.iloc[val_ind].values</span><br><span class="line">    val_y = Y_data.iloc[val_ind]</span><br><span class="line"></span><br><span class="line">    xgr.fit(train_x, train_y)</span><br><span class="line">    pred_train_xgb = xgr.predict(train_x)   <span class="comment"># 子训练集的预测值</span></span><br><span class="line">    pre_xgb = xgr.predict(val_x)            <span class="comment"># 验证集的预测值</span></span><br><span class="line"></span><br><span class="line">    score_train = mean_absolute_error(train_y, pred_train_xgb)</span><br><span class="line">    scores_train.append(score_train)       <span class="comment"># 统计子训练集的mae</span></span><br><span class="line"></span><br><span class="line">    score = mean_absolute_error(val_y, pre_xgb)</span><br><span class="line">    scores.append(score)                    <span class="comment"># 统计验证集的mae</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train mae:&#x27;</span>, np.mean(scores_train))  <span class="comment"># 统计mae均值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val mae:&#x27;</span>, np.mean(scores))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>因为跑的很慢,所以记录一下这次的结果<br>Train mae: 596.3128886185606<br>Val mae: 693.382067947197</p>
<h2 id="划分数据集-并用多种方法训练和预测"><a href="#划分数据集-并用多种方法训练和预测" class="headerlink" title="# 划分数据集,并用多种方法训练和预测"></a># 划分数据集,并用多种方法训练和预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_val, y_train, y_val = train_test_split(X_data, Y_data, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 训练并预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict LR...&#x27;</span>)</span><br><span class="line">model_lr = build_model_lr(x_train, y_train)</span><br><span class="line">val_lr = model_lr.predict(x_val)    <span class="comment"># 得到验证集的预测值</span></span><br><span class="line">subA_lr = model_lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict Ridge...&#x27;</span>)</span><br><span class="line">model_ridge = build_model_ridge(x_train, y_train)</span><br><span class="line">val_ridge = model_ridge.predict(x_val)</span><br><span class="line">subA_ridge = model_ridge.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict Lasso...&#x27;</span>)</span><br><span class="line">model_lasso = build_model_lasso(x_train, y_train)</span><br><span class="line">val_lasso = model_lasso.predict(x_val)</span><br><span class="line">subA_lasso = model_lasso.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict GBDT...&#x27;</span>)</span><br><span class="line">model_gbdt = build_model_gbdt(x_train, y_train)</span><br><span class="line">val_gbdt = model_gbdt.predict(x_val)</span><br><span class="line">subA_gbdt = model_gbdt.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Predict LR...</span><br><span class="line">Predict Ridge...</span><br><span class="line">Predict Lasso...</span><br><span class="line">Predict GBDT...</span><br><span class="line">&#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.2</span>&#125;</span><br></pre></td></tr></table></figure>
<p>因为GBDT加了网格搜索调参,所以跑的特别慢…</p>
<h2 id="一般比赛中效果最为显著的两种方法-XGB-x2F-LGB"><a href="#一般比赛中效果最为显著的两种方法-XGB-x2F-LGB" class="headerlink" title="# 一般比赛中效果最为显著的两种方法 - XGB&#x2F;LGB"></a># 一般比赛中效果最为显著的两种方法 - XGB&#x2F;LGB</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict XGB...&#x27;</span>)</span><br><span class="line">model_xgb = build_model_xgb(x_train, y_train)</span><br><span class="line">val_xgb = model_xgb.predict(x_val)</span><br><span class="line">subA_xgb = model_xgb.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict LGB...&#x27;</span>)</span><br><span class="line">model_lgb = build_model_lgb(x_train, y_train)</span><br><span class="line">val_lgb = model_lgb.predict(x_val)</span><br><span class="line">subA_lgb = model_lgb.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 看一下lgb模型预测测试集的数据统计性分布</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sta inf of lgb:&#x27;</span>, sta_inf(subA_lgb))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: -<span class="number">76.40857341373585</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">88951.1857499891</span></span><br><span class="line">_mean: <span class="number">5926.870646241635</span></span><br><span class="line">_ptp: <span class="number">89027.59432340284</span></span><br><span class="line">_std: <span class="number">7379.389534056081</span></span><br><span class="line">_var: <span class="number">54455389.89533643</span></span><br></pre></td></tr></table></figure>
<h2 id="加权融合-简单加权平均"><a href="#加权融合-简单加权平均" class="headerlink" title="# 加权融合 - 简单加权平均"></a># 加权融合 - 简单加权平均</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 使用我们定义过的加权平均函数weighted_method</span></span><br><span class="line"><span class="comment"># ## 设置权重</span></span><br><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]</span><br><span class="line"><span class="comment"># ## 预测验证集的准确度 - 三个模型加权融合</span></span><br><span class="line">val_pre = weighted_method(val_lgb, val_xgb, val_gbdt, w)</span><br><span class="line">MAE_Weighted = mean_absolute_error(y_val, val_pre)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of Weighted of val:&#x27;</span>, MAE_Weighted)</span><br><span class="line"><span class="comment"># 预测测试集的加权融合结果</span></span><br><span class="line">subA = weighted_method(subA_lgb, subA_xgb, subA_gbdt, w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAE of Weighted of val: <span class="number">724.2198039000299</span></span><br><span class="line"><span class="comment"># ## 看一下预测测试集融合后的结果统计分布</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sta inf:&#x27;</span>, sta_inf(subA))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: -<span class="number">878.0418803904766</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">88761.70460429232</span></span><br><span class="line">_mean: <span class="number">5931.043037628969</span></span><br><span class="line">_ptp: <span class="number">89639.7464846828</span></span><br><span class="line">_std: <span class="number">7367.855733884833</span></span><br><span class="line">_var: <span class="number">54285298.11533961</span></span><br><span class="line"><span class="comment"># ## 生成提交文件</span></span><br><span class="line">sub = pd.DataFrame()</span><br><span class="line">sub[<span class="string">&#x27;SaleID&#x27;</span>] = X_test.index</span><br><span class="line">sub[<span class="string">&#x27;price&#x27;</span>] = subA</span><br><span class="line">sub.to_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\sub_Weighted.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ##与简单的lr(线性回归)进行对比</span></span><br><span class="line">val_lr_pred = model_lr.predict(x_val)</span><br><span class="line">MAE_lr = mean_absolute_error(y_val, val_lr_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of lr:&#x27;</span>, MAE_lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr_MAE:<span class="number">2588.26</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>weighted_MAE:724.84    —&gt; 能看出,对比lr,加权融合之后模型的精确度有了非常大的提高</p>
<h2 id="Stacking融合"><a href="#Stacking融合" class="headerlink" title="# Stacking融合"></a># Stacking融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 第一层,得到各个单模型预测子训练集的预测值</span></span><br><span class="line">train_lgb_pred = model_lgb.predict(x_train)</span><br><span class="line">train_xgb_pred = model_xgb.predict(x_train)</span><br><span class="line">train_gbdt_pred = model_gbdt.predict(x_train)</span><br><span class="line"><span class="comment"># ## 得到各个单模型预测验证集的预测值 --&gt; 前面预测过了,直接拿来用</span></span><br><span class="line"><span class="comment"># ## 得到各个单模型预测测试集的预测值 --&gt; 前面也预测过了,直接拿来用</span></span><br><span class="line"><span class="comment"># ## 创建新特征</span></span><br><span class="line"></span><br><span class="line">Stark_X_train = pd.DataFrame()  <span class="comment"># 由子训练集的预测数据得到的新特征</span></span><br><span class="line">Stark_X_train[<span class="string">&#x27;Method_1&#x27;</span>] = train_lgb_pred</span><br><span class="line">Stark_X_train[<span class="string">&#x27;Method_2&#x27;</span>] = train_xgb_pred</span><br><span class="line">Stark_X_train[<span class="string">&#x27;Method_3&#x27;</span>] = train_gbdt_pred</span><br><span class="line"></span><br><span class="line">Stark_X_val = pd.DataFrame()    <span class="comment"># 由验证集的预测数据得到的新特征</span></span><br><span class="line">Stark_X_val[<span class="string">&#x27;Method_1&#x27;</span>] = val_lgb</span><br><span class="line">Stark_X_val[<span class="string">&#x27;Method_2&#x27;</span>] = val_xgb</span><br><span class="line">Stark_X_val[<span class="string">&#x27;Method_3&#x27;</span>] = val_gbdt</span><br><span class="line"></span><br><span class="line">Stark_X_test = pd.DataFrame()   <span class="comment"># 由测试集的预测数据得到的新特征</span></span><br><span class="line">Stark_X_test[<span class="string">&#x27;Method_1&#x27;</span>] = subA_lgb</span><br><span class="line">Stark_X_test[<span class="string">&#x27;Method_2&#x27;</span>] = subA_xgb</span><br><span class="line">Stark_X_test[<span class="string">&#x27;Method_3&#x27;</span>] = subA_gbdt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 第二层模型训练 - 元模型得简单一点,这里用线性回归</span></span><br><span class="line">model_lr_Stacking = build_model_lr(Stark_X_train, y_train)      <span class="comment"># 用子训练集的新特征和标签训练</span></span><br><span class="line"><span class="comment"># ## 看模型在子训练集上的表现</span></span><br><span class="line">train_pre_Stacking = model_lr_Stacking.predict(Stark_X_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of Stacking-LR&#x27;</span>, mean_absolute_error(y_train, train_pre_Stacking))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAE of Stacking-LR <span class="number">630.5897643683384</span></span><br><span class="line"><span class="comment"># ## 看模型在验证集上的表现</span></span><br><span class="line">val_pre_Stacking = model_lr_Stacking.predict(Stark_X_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of Stacking-LR:&#x27;</span>, mean_absolute_error(y_val, val_pre_Stacking))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAE of Stacking-LR: <span class="number">722.2460136359339</span></span><br><span class="line"><span class="comment"># ## 预测测试集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict Stacking-LR...&#x27;</span>)</span><br><span class="line">subA_Stacking = model_lr_Stacking.predict(Stark_X_test)</span><br></pre></td></tr></table></figure>
<h2 id="我们看一下预测值的统计性分布"><a href="#我们看一下预测值的统计性分布" class="headerlink" title="# 我们看一下预测值的统计性分布"></a># 我们看一下预测值的统计性分布</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sta_inf(subA_Stacking)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: -<span class="number">3638.174875620137</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">90457.25117154507</span></span><br><span class="line">_mean: <span class="number">5926.428386169822</span></span><br><span class="line">_ptp: <span class="number">94095.42604716521</span></span><br><span class="line">_std: <span class="number">7417.358806265995</span></span><br><span class="line">_var: <span class="number">55017211.6608917</span></span><br><span class="line"><span class="comment"># 有负值,我们去掉过小的预测值 - 预测值小于10的替换成10</span></span><br><span class="line">subA_Stacking[subA_Stacking &lt; <span class="number">10</span>] = <span class="number">10</span></span><br><span class="line"><span class="comment"># 生成结果</span></span><br><span class="line">sub = pd.DataFrame()</span><br><span class="line">sub[<span class="string">&#x27;SaleID&#x27;</span>] = TestA_data.SaleID</span><br><span class="line">sub[<span class="string">&#x27;price&#x27;</span>] = subA_Stacking</span><br><span class="line">sub.to_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\sub_Stacking.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交两个结果之后,发现使用Stacking融合比使用加权融合效果好,但是还没有第一次baseline的分数理想</span></span><br></pre></td></tr></table></figure>
<p>最后完整的比赛打算尝试从这些方面着重入手:<br>0.调整数据类型,压缩数据大小<br>1.测试集和训练集都有缺失值<br>2.notRepairedDamage有异常值,处理完记得转换成数值型特征<br>3.训练集标签分布呈现长尾分布,截断或log变换<br>4.箱线图处理训练集特征的异常数据<br>5.构造特征:使用时间		creatDate-regDate<br>errors&#x3D;’coerce’将不能转换的值变成nan,然后尝试用XGBOOST预测填补缺失值<br>6.邮编提取城市信息(二手车价格和城市可能有关)<br>7.根据品牌的销售统计量构造统计量特征<br>8.数据分桶 - 看下哪些特征是连续型的,变成离散型<br>9.使用SFS挑选最优特征<br>10.调参 - 贝叶斯调参<br>11.xgb,lgb加权融合</p>

    </div>

    
    
    
	
	  <div>
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	  </div>
	
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>小书包
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/" title="二手车交易价格预测_Task5_模型融合">http://example.com/2023/04/24/二手车交易价格预测_Task5_模型融合/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/" rel="tag"># 金融风控</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/" rel="prev" title="二手车交易价格预测_Task4_建模与调参">
      <i class="fa fa-chevron-left"></i> 二手车交易价格预测_Task4_建模与调参
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task1_%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/" rel="next" title="机器学习训练_金融风控_Task1_赛题理解">
      机器学习训练_金融风控_Task1_赛题理解 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88-%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B%E9%83%A8%E5%88%86"><span class="nav-number">1.</span> <span class="nav-text">模型融合_代码示例部分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="nav-number">1.1.</span> <span class="nav-text">#导入工具包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87-%E7%BB%93%E6%9E%9C%E7%9B%B4%E6%8E%A5%E8%9E%8D%E5%90%88"><span class="nav-number">1.2.</span> <span class="nav-text"># 简单加权平均-结果直接融合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%87%BD%E6%95%B0-%E6%A0%B9%E6%8D%AE%E5%8A%A0%E6%9D%83%E8%AE%A1%E7%AE%97"><span class="nav-number">1.3.</span> <span class="nav-text"># 定义结果的加权平均函数 - 根据加权计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%90%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.4.</span> <span class="nav-text"># 根据各模型的预测结果计算MAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%8A%A0%E6%9D%83%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.5.</span> <span class="nav-text"># 根据加权计算MAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E6%AF%94%E9%87%8D%E6%9D%83%E5%80%BC"><span class="nav-number">1.6.</span> <span class="nav-text">## 定义比重权值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%87%BD%E6%95%B0-mean%E5%B9%B3%E5%9D%87"><span class="nav-number">1.6.1.</span> <span class="nav-text"># 定义结果的加权平均函数 - mean平均</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%9D%87%E5%80%BC%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.7.</span> <span class="nav-text"># 根据均值计算MAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%87%BD%E6%95%B0-median%E5%B9%B3%E5%9D%87"><span class="nav-number">1.8.</span> <span class="nav-text"># 定义结果的加权平均函数 - median平均</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%AD%E4%BD%8D%E6%95%B0%E8%AE%A1%E7%AE%97MAE"><span class="nav-number">1.9.</span> <span class="nav-text"># 根据中位数计算MAE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Stacking%E8%9E%8D%E5%90%88-%E5%9B%9E%E5%BD%92"><span class="nav-number">2.</span> <span class="nav-text"># Stacking融合(回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89Stacking%E8%9E%8D%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text"># 定义Stacking融合函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E4%B8%80%E4%BA%9B%E7%AE%80%E5%8D%95%E7%9A%84%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE-test-prei%E4%BB%A3%E8%A1%A8%E7%AC%ACi%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E5%80%BC-y-test-true%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%80%BC"><span class="nav-number">2.2.</span> <span class="nav-text"># 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E4%B8%80%E4%B8%8BStacking%E8%9E%8D%E5%90%88%E7%9A%84%E6%95%88%E6%9E%9C"><span class="nav-number">2.3.</span> <span class="nav-text"># 看一下Stacking融合的效果</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88-Voting-Stacking%E2%80%A6"><span class="nav-number">3.</span> <span class="nav-text"># 分类模型融合 - Voting,Stacking…</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Voting%E6%8A%95%E7%A5%A8%E6%9C%BA%E5%88%B6"><span class="nav-number">3.1.</span> <span class="nav-text"># Voting投票机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8XGB%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">3.2.</span> <span class="nav-text"># 用XGB分类模型训练数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">3.3.</span> <span class="nav-text"># 用随机森林分类模型训练数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8SVC%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">3.4.</span> <span class="nav-text"># 用SVC训练数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A1%AC%E6%8A%95%E7%A5%A8"><span class="nav-number">3.5.</span> <span class="nav-text"># 硬投票</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E6%8A%95%E7%A5%A8"><span class="nav-number">3.6.</span> <span class="nav-text"># 软投票</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E7%9A%84Stacking-x2F-Blending%E8%9E%8D%E5%90%88"><span class="nav-number">4.</span> <span class="nav-text"># 分类的Stacking&#x2F;Blending融合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5%E6%8A%98Stacking-%E5%8D%B3%E6%AF%8F%E6%AC%A1Stacking%E8%AE%AD%E7%BB%83%E9%83%BD%E4%BC%9A%E5%9C%A8%E7%AC%AC%E4%B8%80%E5%B1%82%E5%9F%BA%E5%AD%A6%E4%B9%A0%E5%99%A8%E8%BF%9B%E8%A1%8C5%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-%E5%86%8D%E8%BF%9B%E5%85%A5%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%AD%A6%E4%B9%A0%E5%99%A8%E8%AE%AD%E7%BB%83"><span class="nav-number">4.1.</span> <span class="nav-text"># 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Blending-%E5%92%8CStacking%E7%B1%BB%E4%BC%BC-%E4%B8%8D%E5%90%8C%E7%82%B9%E5%9C%A8%E4%BA%8E"><span class="nav-number">4.2.</span> <span class="nav-text"># Blending - 和Stacking类似,不同点在于:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%A9%E7%94%A8mlxtend%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E7%9A%84Stacking%E8%9E%8D%E5%90%88"><span class="nav-number">4.3.</span> <span class="nav-text"># 利用mlxtend进行分类的Stacking融合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%85%B6%E5%AE%83%E6%96%B9%E6%B3%95"><span class="nav-number">4.4.</span> <span class="nav-text"># 一些其它方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AC%E8%B5%9B%E9%A2%98%E7%A4%BA%E4%BE%8B"><span class="nav-number">5.</span> <span class="nav-text">本赛题示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BE%E4%B8%80%E4%B8%8B%E6%95%B0%E6%8D%AE"><span class="nav-number">5.1.</span> <span class="nav-text"># 回顾一下数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%BB%9F%E8%AE%A1%E5%87%BD%E6%95%B0-%E6%96%B9%E4%BE%BF%E5%90%8E%E7%BB%AD%E4%BF%A1%E6%81%AF%E7%BB%9F%E8%AE%A1"><span class="nav-number">5.2.</span> <span class="nav-text"># 创建一个统计函数,方便后续信息统计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%A1%AB%E8%A1%A5%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E7%BC%BA%E5%A4%B1%E5%80%BC-%E7%94%A8-1%E5%A1%AB%E5%85%85"><span class="nav-number">5.3.</span> <span class="nav-text"># 简单填补训练集和测试集的缺失值 - 用-1填充</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BA%86%E4%BB%A3%E7%A0%81%E7%AE%80%E6%B4%81-%E5%88%9B%E5%BB%BA%E5%A5%BD%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="nav-number">5.4.</span> <span class="nav-text"># 为了代码简洁,创建好模型训练函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGB%E7%9A%84%E4%BA%94%E6%8A%98%E4%BA%A4%E5%8F%89%E5%9B%9E%E5%BD%92%E9%AA%8C%E8%AF%81-%E8%BF%99%E9%87%8C%E5%8F%AA%E6%98%AF%E4%B8%BE%E4%BE%8B-%E5%88%AB%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%B9%9F%E5%8F%AF%E4%BB%A5%E8%BF%99%E4%B9%88%E9%AA%8C%E8%AF%81"><span class="nav-number">5.5.</span> <span class="nav-text"># XGB的五折交叉回归验证 - 这里只是举例,别的模型也可以这么验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%B9%B6%E7%94%A8%E5%A4%9A%E7%A7%8D%E6%96%B9%E6%B3%95%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B"><span class="nav-number">5.6.</span> <span class="nav-text"># 划分数据集,并用多种方法训练和预测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E6%AF%94%E8%B5%9B%E4%B8%AD%E6%95%88%E6%9E%9C%E6%9C%80%E4%B8%BA%E6%98%BE%E8%91%97%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95-XGB-x2F-LGB"><span class="nav-number">5.7.</span> <span class="nav-text"># 一般比赛中效果最为显著的两种方法 - XGB&#x2F;LGB</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E6%9D%83%E8%9E%8D%E5%90%88-%E7%AE%80%E5%8D%95%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="nav-number">5.8.</span> <span class="nav-text"># 加权融合 - 简单加权平均</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stacking%E8%9E%8D%E5%90%88"><span class="nav-number">5.9.</span> <span class="nav-text"># Stacking融合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E7%9C%8B%E4%B8%80%E4%B8%8B%E9%A2%84%E6%B5%8B%E5%80%BC%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%80%A7%E5%88%86%E5%B8%83"><span class="nav-number">5.10.</span> <span class="nav-text"># 我们看一下预测值的统计性分布</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">小书包</p>
  <div class="site-description" itemprop="description">种一棵树最好的时间是十年前，其次是现在</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/FelixBigTree" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;FelixBigTree" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/598376210@qq.com" title="E-Mail → 598376210@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-04 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小书包</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">198k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:01</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
