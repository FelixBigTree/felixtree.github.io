<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="建模与调参_代码示例部分# 导入工具包12345import pandas as pdimport numpy as npimport warningswarnings.filterwarnings(&amp;#x27;ignore&amp;#x27;)   # 代码可以正常运行但是会提示警告,很烦人,有了这行代码就能忽略警告了pd.set_option(&amp;#x27;display.max_columns&amp;#x2">
<meta property="og:type" content="article">
<meta property="og:title" content="二手车交易价格预测_Task4_建模与调参">
<meta property="og:url" content="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/index.html">
<meta property="og:site_name" content="大宝的树屋">
<meta property="og:description" content="建模与调参_代码示例部分# 导入工具包12345import pandas as pdimport numpy as npimport warningswarnings.filterwarnings(&amp;#x27;ignore&amp;#x27;)   # 代码可以正常运行但是会提示警告,很烦人,有了这行代码就能忽略警告了pd.set_option(&amp;#x27;display.max_columns&amp;#x2">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020033109212229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331092245311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020033109232833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331092504165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331092718897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331093755291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331094145139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331094216502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331094310769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331101034898.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331101048101.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200331101813468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2023-04-24T08:03:04.807Z">
<meta property="article:modified_time" content="2023-04-24T08:17:38.985Z">
<meta property="article:author" content="小书包">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="金融风控">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2020033109212229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>二手车交易价格预测_Task4_建模与调参 | 大宝的树屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

	<a target="_blank" rel="noopener" href="https://github.com/FelixBigTree" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">大宝的树屋</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="小书包">
      <meta itemprop="description" content="种一棵树最好的时间是十年前，其次是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大宝的树屋">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          二手车交易价格预测_Task4_建模与调参
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-24 16:03:04 / 修改时间：16:17:38" itemprop="dateCreated datePublished" datetime="2023-04-24T16:03:04+08:00">2023-04-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">二手车交易价格预测</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="建模与调参-代码示例部分"><a href="#建模与调参-代码示例部分" class="headerlink" title="建模与调参_代码示例部分"></a>建模与调参_代码示例部分</h1><h2 id="导入工具包"><a href="#导入工具包" class="headerlink" title="# 导入工具包"></a># 导入工具包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)   <span class="comment"># 代码可以正常运行但是会提示警告,很烦人,有了这行代码就能忽略警告了</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)      <span class="comment"># 显示所有列</span></span><br></pre></td></tr></table></figure>
<h2 id="创建一个reduce-mem-usage函数-通过调整数据类型-减少数据在内存中占用的空间"><a href="#创建一个reduce-mem-usage函数-通过调整数据类型-减少数据在内存中占用的空间" class="headerlink" title="# 创建一个reduce_mem_usage函数,通过调整数据类型,减少数据在内存中占用的空间"></a># 创建一个reduce_mem_usage函数,通过调整数据类型,减少数据在内存中占用的空间</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>()     <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is&#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:      <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:      <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:                               <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>()   <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))   <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<h2 id="读取我们为树模型准备的特征数据-并且将数据压缩"><a href="#读取我们为树模型准备的特征数据-并且将数据压缩" class="headerlink" title="# 读取我们为树模型准备的特征数据,并且将数据压缩"></a># 读取我们为树模型准备的特征数据,并且将数据压缩</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = reduce_mem_usage(pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\data_for_tree.csv&#x27;</span>))</span><br><span class="line"></span><br><span class="line">Memory usage of dataframe is60507376<span class="number">.00</span> MB</span><br><span class="line">Memory usage after optimization <span class="keyword">is</span> <span class="number">15724155.00</span> MB</span><br><span class="line">Decreased by <span class="number">74.0</span>%</span><br></pre></td></tr></table></figure>
<p>数据内存大小压缩了74%,看来效果拔群</p>
<h2 id="把我们需要用的特征挑出-仅仅是列名-用来分离特征集和标签集"><a href="#把我们需要用的特征挑出-仅仅是列名-用来分离特征集和标签集" class="headerlink" title="# 把我们需要用的特征挑出(仅仅是列名,用来分离特征集和标签集)"></a># 把我们需要用的特征挑出(仅仅是列名,用来分离特征集和标签集)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">continuous_feature_names = [x <span class="keyword">for</span> x <span class="keyword">in</span> sample_feature.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;brand&#x27;</span>, <span class="string">&#x27;model&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="处理一下sample-feature数据集的缺失值"><a href="#处理一下sample-feature数据集的缺失值" class="headerlink" title="# 处理一下sample_feature数据集的缺失值"></a># 处理一下sample_feature数据集的缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = sample_feature.dropna().replace(<span class="string">&#x27;-&#x27;</span>, <span class="number">0</span>).reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里我比较疑惑为什么replace了’-‘?<br>然后我看了看数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature[sample_feature.isin([<span class="string">&#x27;-&#x27;</span>])] </span><br></pre></td></tr></table></figure>
<p>发现每非常多的样本都存在’-‘,说明有一个特征里面有’-‘</p>
<p>我第一时间考虑到了notRepairedDamage这个特征,因为它是唯一一个object类型的特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature[<span class="string">&#x27;notRepairedDamage&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>果不其然,它是个类别特征,Categories (3, object): [-, 0.0, 1.0],有三个类别:’-‘, ‘0’, ‘1’</p>
<h2 id="既然想起来了notRepairedDamage不是数值型特征-那么我们把它转化一下"><a href="#既然想起来了notRepairedDamage不是数值型特征-那么我们把它转化一下" class="headerlink" title="# 既然想起来了notRepairedDamage不是数值型特征,那么我们把它转化一下"></a># 既然想起来了notRepairedDamage不是数值型特征,那么我们把它转化一下</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature[<span class="string">&#x27;notRepairedDamage&#x27;</span>] = sample_feature[<span class="string">&#x27;notRepairedDamage&#x27;</span>].astype(np.float32)</span><br></pre></td></tr></table></figure>
<h2 id="处理完了之后-我们拿出我们挑选的特征和它的价格构造训练集"><a href="#处理完了之后-我们拿出我们挑选的特征和它的价格构造训练集" class="headerlink" title="# 处理完了之后,我们拿出我们挑选的特征和它的价格构造训练集"></a># 处理完了之后,我们拿出我们挑选的特征和它的价格构造训练集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train = sample_feature[continuous_feature_names + [<span class="string">&#x27;price&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y = train[<span class="string">&#x27;price&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="简单建模"><a href="#简单建模" class="headerlink" title="# 简单建模"></a># 简单建模</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression       <span class="comment"># 线性回归模型 y = wx + b</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">这里比较疑惑,不是说用树模型嘛?挑的也是树模型的数据,为啥这里建了LR,看看再说</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">model = LinearRegression(normalize=<span class="literal">True</span>)    <span class="comment"># normalize参数决定是否将数据归一化</span></span><br><span class="line">model = model.fit(train_X, train_y)</span><br></pre></td></tr></table></figure>
<h2 id="查看训练的lR模型的截距-intercept-与权重-coef"><a href="#查看训练的lR模型的截距-intercept-与权重-coef" class="headerlink" title="# 查看训练的lR模型的截距(intercept)与权重(coef)"></a># 查看训练的lR模型的截距(intercept)与权重(coef)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model.intercept_)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">zip() - 可以将两个可迭代的对象,组合返回成一个元组数据</span></span><br><span class="line"><span class="string">dict() - 使用元组数据构建字典</span></span><br><span class="line"><span class="string">items方法 - items() 函数以列表返回可遍历的(键, 值) 元组数组 </span></span><br><span class="line"><span class="string">sort(iterable, cmp, key, reverse) - 排序函数</span></span><br><span class="line"><span class="string">    iterable - 指定要排序的list或者iterable</span></span><br><span class="line"><span class="string">    key - 指定取待排序元素的哪一项进行排序 - 这里x[1]表示按照列表中第二个元素排序</span></span><br><span class="line"><span class="string">    reverse - 是一个bool变量，表示升序还是降序排列，默认为False(升序)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">sorted</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(continuous_feature_names, model.coef_)).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 这行代码返回了每个特征的权重,按照权重降序排列</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.random.randint() - 产生离散均匀分布的整数</span></span><br><span class="line"><span class="string">            取数范围:若high不为None时,取[low,high)之间随机整数,否则取值[0,low)之间随机整数</span></span><br><span class="line"><span class="string">            size - 54输出的大小，可以是整数也可以是元组</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">subsample_index = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(train_y), size=<span class="number">50</span>)  <span class="comment"># 随机生成0-50000之间的50个整数</span></span><br></pre></td></tr></table></figure>
<h2 id="绘制v-9的值与标签的散点图"><a href="#绘制v-9的值与标签的散点图" class="headerlink" title="# 绘制v_9的值与标签的散点图"></a># 绘制v_9的值与标签的散点图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], train_y[subsample_index], color=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], model.predict(train_X.loc[subsample_index]), color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;v_9&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;True Price&#x27;</span>, <span class="string">&#x27;Predicted&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The predicted price is obvious different from true price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020033109212229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们发现预测点和真实值差别较大,且有预测值出现了结果为负的情况,不符合实际情况,模型有问题</p>
<h2 id="我们看一下价格的分布图"><a href="#我们看一下价格的分布图" class="headerlink" title="# 我们看一下价格的分布图"></a># 我们看一下价格的分布图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;It is clear to see the price shows a typical exponential distribution&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331092245311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>价格呈长尾分布,不利于建模预测,因为很多模型都假设数据误差项符合正态分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.quantile(train_y, 0.9) - 求train_y 的90%的分位数</span></span><br><span class="line"><span class="string">下面这个代码是把价格大于90%分位数的部分截断了,就是长尾分布截断</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.distplot(train_y[train_y &lt; np.quantile(train_y, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/2020033109232833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>看上去好多了</p>
<h2 id="为了更加贴近正态分布-对price进行log-x-1-变换"><a href="#为了更加贴近正态分布-对price进行log-x-1-变换" class="headerlink" title="# 为了更加贴近正态分布,对price进行log(x+1)变换"></a># 为了更加贴近正态分布,对price进行log(x+1)变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_y_In = np.log(train_y + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The transformed price seems like normal distribution&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y_In)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">sns.distplot(train_y_In[train_y_In &lt; np.quantile(train_y_In, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200331092504165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这回看上去像正态分布了,右边的图是做了长尾截断处理</p>
<h2 id="我们再训练一次"><a href="#我们再训练一次" class="headerlink" title="# 我们再训练一次"></a># 我们再训练一次</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = model.fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model.intercept_))</span><br><span class="line"><span class="built_in">sorted</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(continuous_feature_names, model.coef_)).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="可视化一波-还是看v-9"><a href="#可视化一波-还是看v-9" class="headerlink" title="# 可视化一波,还是看v_9"></a># 可视化一波,还是看v_9</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], train_y[subsample_index], color=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.exp() - 求e的幂次方,因为训练模型的时候log变换了,所以预测完了对比结果的时候得变回来</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;v_9&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;True Price&#x27;</span>, <span class="string">&#x27;Predicted&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331092718897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>看上去准多了,预测值和真实值更加贴合</p>
<h2 id="五折交叉验证"><a href="#五折交叉验证" class="headerlink" title="# 五折交叉验证"></a># 五折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error, make_scorer</span><br></pre></td></tr></table></figure>
<p>定义一个函数,用来处理预测值和真实值的log变换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">numpy.nan_to_num(x) - 使用0代替数组x中的nan元素，使用有限的数字代替inf元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_transfer</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">warpper</span>(<span class="params">y, yhat</span>):</span><br><span class="line">        result = func(np.log(y), np.nan_to_num(np.log(yhat)))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> warpper</span><br></pre></td></tr></table></figure>

<h2 id="我们使用线性回归模型-对未处理过标签的特征数据做5折交叉验证"><a href="#我们使用线性回归模型-对未处理过标签的特征数据做5折交叉验证" class="headerlink" title="# 我们使用线性回归模型,对未处理过标签的特征数据做5折交叉验证"></a># 我们使用线性回归模型,对未处理过标签的特征数据做5折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scores = cross_val_score(model, X=train_X, y=train_y, verbose=<span class="number">1</span>, cv=<span class="number">5</span>,</span><br><span class="line">                         scoring=make_scorer(log_transfer(mean_absolute_error)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">verbose - 日志显示</span></span><br><span class="line"><span class="string">            verbose = 0 为不在标准输出流输出日志信息</span></span><br><span class="line"><span class="string">            verbose = 1 为输出进度条记录</span></span><br><span class="line"><span class="string">            verbose = 2 为每个epoch输出一行记录</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">make_scorer() - 工厂函数,自己定评分标准</span></span><br><span class="line"><span class="string">这里的log_transfer()是返回log化的标签预测值和真实值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AVG:&#x27;</span>, np.mean(scores))</span><br></pre></td></tr></table></figure>
<p>— AVG: 1.3654295934396061  —&gt; 5次的MAE平均值</p>
<h2 id="我们再使用线性回归模型-对处理过标签的特征数据做5折交叉验证"><a href="#我们再使用线性回归模型-对处理过标签的特征数据做5折交叉验证" class="headerlink" title="# 我们再使用线性回归模型,对处理过标签的特征数据做5折交叉验证"></a># 我们再使用线性回归模型,对处理过标签的特征数据做5折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores_In = cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">1</span>, cv=<span class="number">5</span>,</span><br><span class="line">                         scoring=make_scorer(mean_absolute_error))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AVG:&#x27;</span>, np.mean(scores_In))</span><br></pre></td></tr></table></figure>
<p>— AVG: 0.1932330179438017<br>MAE从1.365降低到0.193,误差缩小了很多</p>
<h2 id="事实上-五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况"><a href="#事实上-五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况" class="headerlink" title="# 事实上,五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况"></a># 事实上,五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况</h2><p>用2018年的二手车价格预测2017年是不合理的,所以我们可以用时间靠前的4&#x2F;5样本当作训练集,靠后的1&#x2F;5当验证集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime     <span class="comment"># 这里我没看到datetime的作用,只能认为数据集是按照时间排列的</span></span><br><span class="line">sample_feature = sample_feature.reset_index(drop=<span class="literal">True</span>)      <span class="comment"># 重置索引</span></span><br><span class="line">split_point = <span class="built_in">len</span>(sample_feature) // <span class="number">5</span> * <span class="number">4</span>      <span class="comment"># 设置分割点</span></span><br><span class="line"></span><br><span class="line">train = sample_feature.loc[:split_point].dropna()</span><br><span class="line">val = sample_feature.loc[split_point:].dropna()</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y_In = np.log(train[<span class="string">&#x27;price&#x27;</span>] + <span class="number">1</span>)</span><br><span class="line">val_X = val[continuous_feature_names]</span><br><span class="line">val_y_In = np.log(val[<span class="string">&#x27;price&#x27;</span>] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model = model.fit(train_X, train_y_In)</span><br><span class="line">mean_absolute_error(val_y_In, model.predict(val_X))</span><br></pre></td></tr></table></figure>
<p>— MAE为0.196,和五折交叉验证差别不大</p>
<h2 id="绘制学习率曲线与验证曲线"><a href="#绘制学习率曲线与验证曲线" class="headerlink" title="# 绘制学习率曲线与验证曲线"></a># 绘制学习率曲线与验证曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve, validation_curve</span><br></pre></td></tr></table></figure>
<p>创建绘制学习率曲线的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_learning_curve</span>(<span class="params">estimator, title, X, y, ylim=<span class="literal">None</span>, cv=<span class="literal">None</span>, n_jobs=<span class="number">1</span>, train_sizes=np.linspace(<span class="params"><span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span></span>)</span>):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.ylim(*ylim)     <span class="comment"># 如果规定了ylim的值,那么ylim就用规定的值</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Training example&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;score&#x27;</span>)</span><br><span class="line">    train_size, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,</span><br><span class="line">                                                           train_sizes=train_sizes,</span><br><span class="line">                                                           scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    fill_between()</span></span><br><span class="line"><span class="string">            train_sizes - 第一个参数表示覆盖的区域</span></span><br><span class="line"><span class="string">            train_scores_mean - train_scores_std - 第二个参数表示覆盖的下限</span></span><br><span class="line"><span class="string">            train_scores_mean + train_scores_std - 第三个参数表示覆盖的上限</span></span><br><span class="line"><span class="string">            color - 表示覆盖区域的颜色</span></span><br><span class="line"><span class="string">            alpha - 覆盖区域的透明度,越大越不透明 [0,1]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">&#x27;o-&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Training score&#x27;</span>)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">&#x27;o-&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;Cross-validation score&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br><span class="line"></span><br><span class="line">plot_learning_curve(LinearRegression(), <span class="string">&#x27;Liner_model&#x27;</span>, train_X[:<span class="number">1000</span>], train_y_In[:<span class="number">1000</span>],</span><br><span class="line">                    ylim=(<span class="number">0.0</span>, <span class="number">0.5</span>), cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331093755291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>模型在训练集拟合的不错,在验证集中表现一般</p>
<h2 id="嵌入式特征选择-大部分情况下都是用嵌入式做特征选择"><a href="#嵌入式特征选择-大部分情况下都是用嵌入式做特征选择" class="headerlink" title="# 嵌入式特征选择 - 大部分情况下都是用嵌入式做特征选择"></a># 嵌入式特征选择 - 大部分情况下都是用嵌入式做特征选择</h2><p>1.L1正则化 - Lasso回归 -<br>        模型被限制在正方形区域(二维区域下),损失函数的最小值往往在正方形(约束)的角上,很多权值为0(多维),所以可以实现模型的稀疏性(生成稀疏权值矩阵,进而用于特征选择</p>
<p>2.L2正则化 - 岭回归 -<br>        模型被限制在圆形区域(二维区域下),损失函数的最小值因为圆形约束没有角,所以不会使得权重为0,但是可以使得权重都尽可能的小,最后得到一个所有参数都比较小的模型,这样模型比较简单,能适应不同数据集,一定程度上避免了过拟合</p>
<h2 id="我们看下三种模型的效果对比-线性回归-加入了L1的Lasso回归-加入了L2的岭回归"><a href="#我们看下三种模型的效果对比-线性回归-加入了L1的Lasso回归-加入了L2的岭回归" class="headerlink" title="# 我们看下三种模型的效果对比:线性回归; 加入了L1的Lasso回归; 加入了L2的岭回归"></a># 我们看下三种模型的效果对比:线性回归; 加入了L1的Lasso回归; 加入了L2的岭回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">models = [LinearRegression(), Ridge(), Lasso()]</span><br><span class="line">result = <span class="built_in">dict</span>()     <span class="comment"># 创建一个用来装结果的字典</span></span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = <span class="built_in">str</span>(model).split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">0</span>]   <span class="comment"># 把括号去掉,只保留名字</span></span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>, cv=<span class="number">5</span>,       <span class="comment"># 五折交叉验证</span></span><br><span class="line">                             scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    <span class="built_in">print</span>(model_name + <span class="string">&#x27; is finished&#x27;</span>)</span><br><span class="line"></span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">&#x27;cv&#x27;</span> + <span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br><span class="line"></span><br><span class="line">model_Lr = LinearRegression().fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model_Lr.intercept_))</span><br><span class="line">sns.barplot(<span class="built_in">abs</span>(model_Lr.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331094145139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>— 线性回归模型:发现v_6, v_8, v_9权重大</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_Ridge = Ridge().fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model_Ridge.intercept_))</span><br><span class="line">sns.barplot(<span class="built_in">abs</span>(model_Ridge.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331094216502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>— 岭回归:发现有更多的参数对模型起到影响,而且参数都比较小,一定程度上避免了过拟合现象,抗扰动能力强</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_Lasso = Lasso().fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model_Lasso.intercept_))</span><br><span class="line">sns.barplot(<span class="built_in">abs</span>(model_Lasso.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331094310769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>— lasso回归:发现power和used_time这两个特征很重要,L1正则化有助于生成一个稀疏权值矩阵,进而用于特征选择</p>
<h2 id="看看常用的非线性模型-与线性模型的效果进行一个比对"><a href="#看看常用的非线性模型-与线性模型的效果进行一个比对" class="headerlink" title="# 看看常用的非线性模型,与线性模型的效果进行一个比对"></a># 看看常用的非线性模型,与线性模型的效果进行一个比对</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">SVM - 支持向量机 - 通过寻求结构化风险最小来提高学习机泛化能力,基本模型定义为特征空间上的间隔最大的线性分类器</span></span><br><span class="line"><span class="string">                    支持向量机的学习策略便是间隔最大化</span></span><br><span class="line"><span class="string">    SVR - 用于标签连续值的回归问题</span></span><br><span class="line"><span class="string">    SVC - 用于分类标签的分类问题</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC     <span class="comment"># 这里用SVR是不是好得多?</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor      <span class="comment"># 决策树回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor  <span class="comment"># 随机森林回归</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Boosting算法思想 -  一堆弱分类器的组合就可以成为一个强分类器;</span></span><br><span class="line"><span class="string">                    不断地在错误中学习，迭代来降低犯错概率</span></span><br><span class="line"><span class="string">                    通过一系列的迭代来优化分类结果,每迭代一次引入一个弱分类器,来克服现在已经存在的弱分类器组合的短板</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Adaboost      - 整个训练集上维护一个分布权值向量W</span></span><br><span class="line"><span class="string">                        用赋予权重的训练集通过弱分类算法产生分类假设（基学习器）y(x)</span></span><br><span class="line"><span class="string">                        然后计算错误率,用得到的错误率去更新分布权值向量w</span></span><br><span class="line"><span class="string">                        对错误分类的样本分配更大的权值,正确分类的样本赋予更小的权值</span></span><br><span class="line"><span class="string">                        每次更新后用相同的弱分类算法产生新的分类假设,这些分类假设的序列构成多分类器</span></span><br><span class="line"><span class="string">                        对这些多分类器用加权的方法进行联合,最后得到决策结果</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">        Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">参考资料:https://www.cnblogs.com/zhubinwang/p/5170087.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">MLPRegressor  - 人工神经网络,了解的不多</span></span><br><span class="line"><span class="string">参数详解</span></span><br><span class="line"><span class="string">    hidden_layer_sizes - hidden_layer_sizes=(50, 50),表示有两层隐藏层,第一层隐藏层有50个神经元,第二层也有50个神经元</span></span><br><span class="line"><span class="string">    activation - 激活函数   &#123;‘identity’, ‘logistic’, ‘tanh’, ‘relu’&#125;,默认relu</span></span><br><span class="line"><span class="string">                identity - f(x) = x</span></span><br><span class="line"><span class="string">                logistic - 其实就是sigmod函数,f(x) = 1 / (1 + exp(-x))</span></span><br><span class="line"><span class="string">                tanh - f(x) = tanh(x)</span></span><br><span class="line"><span class="string">                relu - f(x) = max(0, x) </span></span><br><span class="line"><span class="string">    solver - 用来优化权重     &#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;,默认adam,</span></span><br><span class="line"><span class="string">                lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好</span></span><br><span class="line"><span class="string">                sgd - 随机梯度下降 </span></span><br><span class="line"><span class="string">                adam - 机遇随机梯度的优化器</span></span><br><span class="line"><span class="string">    alpha - 正则化项参数,可选的，默认0.0001</span></span><br><span class="line"><span class="string">    learning_rate - 学习率,用于权重更新,只有当solver为’sgd’时使用</span></span><br><span class="line"><span class="string">    max_iter - 最大迭代次数,默认200</span></span><br><span class="line"><span class="string">    shuffle - 判断是否在每次迭代时对样本进行清洗,默认True,只有当solver=’sgd’或者‘adam’时使用</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPRegressor</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">XGBRegressor - 梯度提升回归树,也叫梯度提升机</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">                采用连续的方式构造树,每棵树都试图纠正前一棵树的错误</span></span><br><span class="line"><span class="string">                与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝</span></span><br><span class="line"><span class="string">                从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> lightgbm.sklearn <span class="keyword">import</span> LGBMRegressor</span><br><span class="line"></span><br><span class="line">models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(),</span><br><span class="line">          GradientBoostingRegressor(), MLPRegressor(solver=<span class="string">&#x27;lbfgs&#x27;</span>, max_iter=<span class="number">100</span>),</span><br><span class="line">          XGBRegressor(n_estimators=<span class="number">100</span>, objective=<span class="string">&#x27;reg:squarederror&#x27;</span>),</span><br><span class="line">          LGBMRegressor(n_estimators=<span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line">result = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = <span class="built_in">str</span>(model).split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_In,</span><br><span class="line">                             verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    <span class="built_in">print</span>(model_name + <span class="string">&#x27; is finished&#x27;</span>)</span><br><span class="line"></span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">&#x27;cv&#x27;</span> + <span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331101034898.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200331101048101.png" alt="在这里插入图片描述"><br>整体看来,随机森林在每一折的表现最好,XGB也不错,LGB最小值在0.143左右</p>
<h2 id="模型调参-以LGB-LGBMRegressor为例"><a href="#模型调参-以LGB-LGBMRegressor为例" class="headerlink" title="# 模型调参-以LGB - LGBMRegressor为例"></a># 模型调参-以LGB - LGBMRegressor为例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LightGBM - 使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低</span></span><br><span class="line"><span class="string">            思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram</span></span><br><span class="line"><span class="string">            然后遍历训练数据，统计每个离散值在直方图中的累计统计量</span></span><br><span class="line"><span class="string">            在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            LightGBM采用leaf-wise生长策略:</span></span><br><span class="line"><span class="string">            每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。</span></span><br><span class="line"><span class="string">            因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度</span></span><br><span class="line"><span class="string">            Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合</span></span><br><span class="line"><span class="string">            因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">参数:</span></span><br><span class="line"><span class="string">num_leaves - 控制了叶节点的数目,它是控制树模型复杂度的主要参数,取值应 &lt;= 2 ^（max_depth）</span></span><br><span class="line"><span class="string">bagging_fraction - 每次迭代时用的数据比例,用于加快训练速度和减小过拟合</span></span><br><span class="line"><span class="string">feature_fraction - 每次迭代时用的特征比例,例如为0.8时,意味着在每次迭代中随机选择80％的参数来建树,</span></span><br><span class="line"><span class="string">                    boosting为random forest时用</span></span><br><span class="line"><span class="string">min_data_in_leaf - 每个叶节点的最少样本数量。</span></span><br><span class="line"><span class="string">                    它是处理leaf-wise树的过拟合的重要参数</span></span><br><span class="line"><span class="string">                    将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合</span></span><br><span class="line"><span class="string">max_depth - 控制了树的最大深度,该参数可以显式的限制树的深度</span></span><br><span class="line"><span class="string">n_estimators - 分多少颗决策树(总共迭代的次数)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">objective - 问题类型</span></span><br><span class="line"><span class="string">            regression - 回归任务,使用L2损失函数</span></span><br><span class="line"><span class="string">            regression_l1 - 回归任务,使用L1损失函数</span></span><br><span class="line"><span class="string">            huber - 回归任务,使用huber损失函数</span></span><br><span class="line"><span class="string">            fair - 回归任务,使用fair损失函数</span></span><br><span class="line"><span class="string">            mape (mean_absolute_precentage_error) - 回归任务,使用MAPE损失函数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="贪心算法"><a href="#贪心算法" class="headerlink" title="## 贪心算法"></a>## 贪心算法</h3><h3 id="–-LGB的参数集合"><a href="#–-LGB的参数集合" class="headerlink" title="– LGB的参数集合"></a>– LGB的参数集合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">objective = [<span class="string">&#x27;regression&#x27;</span>, <span class="string">&#x27;regression_l1&#x27;</span>, <span class="string">&#x27;mape&#x27;</span>, <span class="string">&#x27;huber&#x27;</span>, <span class="string">&#x27;fair&#x27;</span>]</span><br><span class="line"></span><br><span class="line">num_leaves = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">max_depth = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">bagging_fraction = []</span><br><span class="line">feature_fraction = []</span><br><span class="line">drop_rate = []</span><br></pre></td></tr></table></figure>
<p>贪心调参</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">建立数学模型来描述问题</span></span><br><span class="line"><span class="string">把求解的问题分成若干个子问题</span></span><br><span class="line"><span class="string">对每个子问题求解，得到子问题的局部最优解</span></span><br><span class="line"><span class="string">把子问题的解局部最优解合成原来问题的一个解</span></span><br><span class="line"><span class="string">总是做出在当前看来是最好的选择,也就是说,不从整体最优上加以考虑,它所做出的仅仅是在某种意义上的局部最优解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对于一个具体问题,要确定它是否具有贪心选择性质,必须证明每一步所作的贪心选择最终导致问题的整体最优解</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 局部最优解 - objective</span></span><br><span class="line">best_obj = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>,</span><br><span class="line">                                    cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 局部最优解 - num_leaves    --- 此时得限制objective参数,得是mae值最小,即误差最小的那个问题类型</span></span><br><span class="line">best_leaves = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    best_obj.items() - 把best_obj字典中的元组装进列表</span></span><br><span class="line"><span class="string">    key=lambda x: x[1] - 取前一个对象的第二维的数据,即原字典的value值</span></span><br><span class="line"><span class="string">    min(best_obj.items(), key=lambda x: x[1]) - 选择best_obj中value值最小的那组元组</span></span><br><span class="line"><span class="string">    min(best_obj.items(), key=lambda x: x[1])[0] - 返回value值最小的key,即问题类型objective</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>,</span><br><span class="line">                                    cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 局部最优解 - max_depth    --- 此时得限制objective参数和num_leaves参数,都得是mae值的问题类型和叶节点数</span></span><br><span class="line">best_depth = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=<span class="built_in">min</span>(best_leaves.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>,</span><br><span class="line">                                    cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure>
<p>经过漫长的等待….</p>
<p>以上就是贪心算法的思想:局部求得最优解之后,固定条件再求别的局部最优解,最后得出所有局部最优的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(x=[<span class="string">&#x27;0_initial&#x27;</span>, <span class="string">&#x27;1_turning_obj&#x27;</span>, <span class="string">&#x27;2_turning_leaves&#x27;</span>, <span class="string">&#x27;3_turning_depth&#x27;</span>],</span><br><span class="line">             y=[<span class="number">0.143</span>, <span class="built_in">min</span>(best_obj.values()), <span class="built_in">min</span>(best_leaves.values()), <span class="built_in">min</span>(best_depth.values())])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200331101813468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>能看出,XGB模型在没有调参的情况下,MAE约0.143<br>局部选择最优objective参数后,MAE下降约为0.142<br>再选择了最优的best_leaves参数后,MAE为0.1354左右(优化了很多)<br>最后选择了best_depth参数后,MAE为0.1353左右,差别不大</p>
<h2 id="Grid-Search-网格搜索调参"><a href="#Grid-Search-网格搜索调参" class="headerlink" title="## Grid Search  - 网格搜索调参"></a>## Grid Search  - 网格搜索调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过循环遍历，尝试每一种参数组合，返回最好的得分值的参数组合</span></span><br><span class="line"><span class="string">GridSearchCV能够使我们找到范围内最优的参数，param_grid参数越多，组合越多，计算的时间也需要越多</span></span><br><span class="line"><span class="string">GridSearchCV适用于小数据集</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">parameters = &#123;<span class="string">&#x27;objective&#x27;</span>: objective, <span class="string">&#x27;num_leaves&#x27;</span>: num_leaves, <span class="string">&#x27;max_depth&#x27;</span>: max_depth&#125;     <span class="comment"># 给定参数取值范围</span></span><br><span class="line">model = LGBMRegressor()     <span class="comment"># 创建实例</span></span><br><span class="line">clf = GridSearchCV(model, parameters, cv=<span class="number">5</span>)     <span class="comment"># 网格搜索,遍历各种特征组合,五折交叉验证</span></span><br><span class="line">clf = clf.fit(train_X, train_y_In)      <span class="comment"># 训练的过程的确很缓慢,超级慢</span></span><br><span class="line">clf.best_params_</span><br><span class="line"><span class="comment"># 结果Out[10]: &#123;&#x27;max_depth&#x27;: 10, &#x27;num_leaves&#x27;: 55, &#x27;objective&#x27;: &#x27;huber&#x27;&#125;,和教材不一样</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LGBMRegressor(objective=<span class="string">&#x27;huber&#x27;</span>, num_leaves=<span class="number">55</span>, max_depth=<span class="number">15</span>)</span><br><span class="line">np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Out[<span class="number">36</span>]: <span class="number">0.1370086509753757</span></span><br></pre></td></tr></table></figure>
<p>MAE为0.137<br>结果Out[10]: {‘max_depth’: 10, ‘num_leaves’: 55, ‘objective’: ‘huber’},和教材不一样</p>
<h2 id="贝叶斯调参"><a href="#贝叶斯调参" class="headerlink" title="## 贝叶斯调参"></a>## 贝叶斯调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">贝叶斯优化是一种用模型找到函数最小值方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">贝叶斯方法与随机或网格搜索的不同之处在于:它在尝试下一组超参数时,会参考之前的评估结果,因此可以省去很多无用功</span></span><br><span class="line"><span class="string">贝叶斯调参法使用不断更新的概率模型,通过推断过去的结果来&#x27;集中&#x27;有希望的超参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">贝叶斯优化问题的四个部分</span></span><br><span class="line"><span class="string">            1.目标函数 - 机器学习模型使用该组超参数在验证集上的损失</span></span><br><span class="line"><span class="string">                        它的输入为一组超参数,输出需要最小化的值(交叉验证损失)</span></span><br><span class="line"><span class="string">            2.域空间 - 要搜索的超参数的取值范围</span></span><br><span class="line"><span class="string">                        在搜索的每次迭代中,贝叶斯优化算法将从域空间为每个超参数选择一个值</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">                        当我们进行随机或网格搜索时,域空间是一个网格</span></span><br><span class="line"><span class="string">                        而在贝叶斯优化中,不是按照顺序()网格)或者随机选择一个超参数,而是按照每个超参数的概率分布选择</span></span><br><span class="line"><span class="string">            3.优化算法 - 构造替代函数并选择下一个超参数值进行评估的方法</span></span><br><span class="line"><span class="string">            4.来自目标函数评估的存储结果,包括超参数和验证集上的损失</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数,我们要这个目标函数输出的值最小</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rf_cv</span>(<span class="params">num_leaves, max_depth, subsample, min_child_samples</span>):</span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        LGBMRegressor(</span><br><span class="line">            objective=<span class="string">&#x27;regression_l1&#x27;</span>, num_leaves=<span class="built_in">int</span>(num_leaves), max_depth=<span class="built_in">int</span>(max_depth),</span><br><span class="line">            subsample=subsample, min_child_samples=<span class="built_in">int</span>(min_child_samples)),</span><br><span class="line">        X=train_X, y=train_y_In, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)</span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - val</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化参数,即域空间</span></span><br><span class="line">rf_bo = BayesianOptimization(rf_cv, &#123;<span class="string">&#x27;num_leaves&#x27;</span>: (<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">                                     <span class="string">&#x27;max_depth&#x27;</span>: (<span class="number">2</span>, <span class="number">100</span>), <span class="string">&#x27;subsample&#x27;</span>: (<span class="number">0.1</span>, <span class="number">1</span>),</span><br><span class="line">                                     <span class="string">&#x27;min_child_samples&#x27;</span>: (<span class="number">2</span>, <span class="number">100</span>)&#125;</span><br><span class="line">                             )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始优化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">rf_bo.maximize() - 最大化分数   这里的目标函数是1-MAE,应该是越大越好,所以用最大化分数</span></span><br><span class="line"><span class="string">rf_bo.minimize() - 最小化分数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rf_bo.maximize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最优目标函数对应的MAE值</span></span><br><span class="line"><span class="number">1</span> - rf_bo.<span class="built_in">max</span>[<span class="string">&#x27;target&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>Target值一旦出现新高,就会标记紫色<br>观察最高的值为0.8694<br>MAE值为0.1306;<br>目前是三种调参方法中得到最高精度的一种</p>
<p>不难发现,随着对参数的调整,模型的精度在一点点提高</p>
<hr>
<p>后续Task5 模型融合,最后的时刻到了~</p>

    </div>

    
    
    
	
	  <div>
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	  </div>
	
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>小书包
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/" title="二手车交易价格预测_Task4_建模与调参">http://example.com/2023/04/24/二手车交易价格预测_Task4_建模与调参/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/" rel="tag"># 金融风控</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task3_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="prev" title="二手车交易价格预测_Task3_特征工程">
      <i class="fa fa-chevron-left"></i> 二手车交易价格预测_Task3_特征工程
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/" rel="next" title="二手车交易价格预测_Task5_模型融合">
      二手车交易价格预测_Task5_模型融合 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82-%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B%E9%83%A8%E5%88%86"><span class="nav-number">1.</span> <span class="nav-text">建模与调参_代码示例部分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="nav-number">1.1.</span> <span class="nav-text"># 导入工具包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAreduce-mem-usage%E5%87%BD%E6%95%B0-%E9%80%9A%E8%BF%87%E8%B0%83%E6%95%B4%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-%E5%87%8F%E5%B0%91%E6%95%B0%E6%8D%AE%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E5%8D%A0%E7%94%A8%E7%9A%84%E7%A9%BA%E9%97%B4"><span class="nav-number">1.2.</span> <span class="nav-text"># 创建一个reduce_mem_usage函数,通过调整数据类型,减少数据在内存中占用的空间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96%E6%88%91%E4%BB%AC%E4%B8%BA%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87%E7%9A%84%E7%89%B9%E5%BE%81%E6%95%B0%E6%8D%AE-%E5%B9%B6%E4%B8%94%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="nav-number">1.3.</span> <span class="nav-text"># 读取我们为树模型准备的特征数据,并且将数据压缩</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%8A%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81%E6%8C%91%E5%87%BA-%E4%BB%85%E4%BB%85%E6%98%AF%E5%88%97%E5%90%8D-%E7%94%A8%E6%9D%A5%E5%88%86%E7%A6%BB%E7%89%B9%E5%BE%81%E9%9B%86%E5%92%8C%E6%A0%87%E7%AD%BE%E9%9B%86"><span class="nav-number">1.4.</span> <span class="nav-text"># 把我们需要用的特征挑出(仅仅是列名,用来分离特征集和标签集)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E4%B8%80%E4%B8%8Bsample-feature%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="nav-number">1.5.</span> <span class="nav-text"># 处理一下sample_feature数据集的缺失值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A2%E7%84%B6%E6%83%B3%E8%B5%B7%E6%9D%A5%E4%BA%86notRepairedDamage%E4%B8%8D%E6%98%AF%E6%95%B0%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%81-%E9%82%A3%E4%B9%88%E6%88%91%E4%BB%AC%E6%8A%8A%E5%AE%83%E8%BD%AC%E5%8C%96%E4%B8%80%E4%B8%8B"><span class="nav-number">1.6.</span> <span class="nav-text"># 既然想起来了notRepairedDamage不是数值型特征,那么我们把它转化一下</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%AE%8C%E4%BA%86%E4%B9%8B%E5%90%8E-%E6%88%91%E4%BB%AC%E6%8B%BF%E5%87%BA%E6%88%91%E4%BB%AC%E6%8C%91%E9%80%89%E7%9A%84%E7%89%B9%E5%BE%81%E5%92%8C%E5%AE%83%E7%9A%84%E4%BB%B7%E6%A0%BC%E6%9E%84%E9%80%A0%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="nav-number">1.7.</span> <span class="nav-text"># 处理完了之后,我们拿出我们挑选的特征和它的价格构造训练集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%BB%BA%E6%A8%A1"><span class="nav-number">1.8.</span> <span class="nav-text"># 简单建模</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E8%AE%AD%E7%BB%83%E7%9A%84lR%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%88%AA%E8%B7%9D-intercept-%E4%B8%8E%E6%9D%83%E9%87%8D-coef"><span class="nav-number">1.9.</span> <span class="nav-text"># 查看训练的lR模型的截距(intercept)与权重(coef)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%98%E5%88%B6v-9%E7%9A%84%E5%80%BC%E4%B8%8E%E6%A0%87%E7%AD%BE%E7%9A%84%E6%95%A3%E7%82%B9%E5%9B%BE"><span class="nav-number">1.10.</span> <span class="nav-text"># 绘制v_9的值与标签的散点图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E7%9C%8B%E4%B8%80%E4%B8%8B%E4%BB%B7%E6%A0%BC%E7%9A%84%E5%88%86%E5%B8%83%E5%9B%BE"><span class="nav-number">1.11.</span> <span class="nav-text"># 我们看一下价格的分布图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BA%86%E6%9B%B4%E5%8A%A0%E8%B4%B4%E8%BF%91%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83-%E5%AF%B9price%E8%BF%9B%E8%A1%8Clog-x-1-%E5%8F%98%E6%8D%A2"><span class="nav-number">1.12.</span> <span class="nav-text"># 为了更加贴近正态分布,对price进行log(x+1)变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E5%86%8D%E8%AE%AD%E7%BB%83%E4%B8%80%E6%AC%A1"><span class="nav-number">1.13.</span> <span class="nav-text"># 我们再训练一次</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%80%E6%B3%A2-%E8%BF%98%E6%98%AF%E7%9C%8Bv-9"><span class="nav-number">1.14.</span> <span class="nav-text"># 可视化一波,还是看v_9</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">1.15.</span> <span class="nav-text"># 五折交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B-%E5%AF%B9%E6%9C%AA%E5%A4%84%E7%90%86%E8%BF%87%E6%A0%87%E7%AD%BE%E7%9A%84%E7%89%B9%E5%BE%81%E6%95%B0%E6%8D%AE%E5%81%9A5%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">1.16.</span> <span class="nav-text"># 我们使用线性回归模型,对未处理过标签的特征数据做5折交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E5%86%8D%E4%BD%BF%E7%94%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B-%E5%AF%B9%E5%A4%84%E7%90%86%E8%BF%87%E6%A0%87%E7%AD%BE%E7%9A%84%E7%89%B9%E5%BE%81%E6%95%B0%E6%8D%AE%E5%81%9A5%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">1.17.</span> <span class="nav-text"># 我们再使用线性回归模型,对处理过标签的特征数据做5折交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8B%E5%AE%9E%E4%B8%8A-%E4%BA%94%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%9C%A8%E6%9F%90%E4%BA%9B%E4%B8%8E%E6%97%B6%E9%97%B4%E7%9B%B8%E5%85%B3%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E5%8F%8D%E8%80%8C%E5%8F%8D%E6%98%A0%E4%BA%86%E4%B8%8D%E7%9C%9F%E5%AE%9E%E7%9A%84%E6%83%85%E5%86%B5"><span class="nav-number">1.18.</span> <span class="nav-text"># 事实上,五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%98%E5%88%B6%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%9B%B2%E7%BA%BF%E4%B8%8E%E9%AA%8C%E8%AF%81%E6%9B%B2%E7%BA%BF"><span class="nav-number">1.19.</span> <span class="nav-text"># 绘制学习率曲线与验证曲线</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%A4%A7%E9%83%A8%E5%88%86%E6%83%85%E5%86%B5%E4%B8%8B%E9%83%BD%E6%98%AF%E7%94%A8%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%81%9A%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">1.20.</span> <span class="nav-text"># 嵌入式特征选择 - 大部分情况下都是用嵌入式做特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E7%9C%8B%E4%B8%8B%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%8A%A0%E5%85%A5%E4%BA%86L1%E7%9A%84Lasso%E5%9B%9E%E5%BD%92-%E5%8A%A0%E5%85%A5%E4%BA%86L2%E7%9A%84%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="nav-number">1.21.</span> <span class="nav-text"># 我们看下三种模型的效果对比:线性回归; 加入了L1的Lasso回归; 加入了L2的岭回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E7%9C%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%88%E6%9E%9C%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E6%AF%94%E5%AF%B9"><span class="nav-number">1.22.</span> <span class="nav-text"># 看看常用的非线性模型,与线性模型的效果进行一个比对</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82-%E4%BB%A5LGB-LGBMRegressor%E4%B8%BA%E4%BE%8B"><span class="nav-number">1.23.</span> <span class="nav-text"># 模型调参-以LGB - LGBMRegressor为例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95"><span class="nav-number">1.23.1.</span> <span class="nav-text">## 贪心算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%93-LGB%E7%9A%84%E5%8F%82%E6%95%B0%E9%9B%86%E5%90%88"><span class="nav-number">1.23.2.</span> <span class="nav-text">– LGB的参数集合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Grid-Search-%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E8%B0%83%E5%8F%82"><span class="nav-number">1.24.</span> <span class="nav-text">## Grid Search  - 网格搜索调参</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%B0%83%E5%8F%82"><span class="nav-number">1.25.</span> <span class="nav-text">## 贝叶斯调参</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">小书包</p>
  <div class="site-description" itemprop="description">种一棵树最好的时间是十年前，其次是现在</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/FelixBigTree" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;FelixBigTree" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/598376210@qq.com" title="E-Mail → 598376210@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-04 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小书包</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">198k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:01</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
