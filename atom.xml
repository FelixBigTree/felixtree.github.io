<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大宝的树屋</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-04-24T08:16:07.396Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>小书包</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Matplotlib_艺术画笔见乾坤</title>
    <link href="http://example.com/2023/04/24/DataWhale_Matplotlib_%E8%89%BA%E6%9C%AF%E7%94%BB%E7%AC%94%E8%A7%81%E4%B9%BE%E5%9D%A4/"/>
    <id>http://example.com/2023/04/24/DataWhale_Matplotlib_%E8%89%BA%E6%9C%AF%E7%94%BB%E7%AC%94%E8%A7%81%E4%B9%BE%E5%9D%A4/</id>
    <published>2023-04-24T08:05:02.545Z</published>
    <updated>2023-04-24T08:16:07.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-序"><a href="#0-序" class="headerlink" title="0 序"></a>0 序</h1><p>这一章节对matplotlib的框架做了详细的介绍，包括matplotlib的三个API分别是什么，有什么作用？matplotlib都有哪些对象容器，分别有什么联系和关系，又包含了哪些功能等等。接下来，我们就一起来看看，俗话说得好，磨刀不误砍柴工，打基础至关重要呀～</p><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h1><h2 id="1-1-matplotlib的三层api"><a href="#1-1-matplotlib的三层api" class="headerlink" title="1.1 matplotlib的三层api"></a>1.1 matplotlib的三层api</h2><p>matplotlib有三个层次的API：</p><pre><code>matplotlib.backend_bases.FigureCanvas - 绘图区matplotlib.backend_bases.Renderer - 渲染器，控制如何在 FigureCanvas 上画图。matplotlib.artist.Artist - 图表组件，即调用了Renderer的接口在Canvas上作图。</code></pre><h2 id="1-2-Artist的分类"><a href="#1-2-Artist的分类" class="headerlink" title="1.2 Artist的分类"></a>1.2 Artist的分类</h2><p>Artist有两种类型：primitives 和containers。<br>    primitive - 基本要素，它包含一些我们要在绘图区作图用到的标准图形对象，如曲线Line2D，文字text，矩形Rectangle，图像image等<br>    container - 容器，即用来装基本要素的地方，包括图形figure、坐标系Axes和坐标轴Axis。他们之间的关系如下图所示<br>    <img src="https://img-blog.csdnimg.cn/20201218235244996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="1-3-matplotlib标准用法"><a href="#1-3-matplotlib标准用法" class="headerlink" title="1.3 matplotlib标准用法"></a>1.3 matplotlib标准用法</h2><p>matplotlib的标准使用流程为：</p><pre><code>创建一个Figure实例使用Figure实例创建一个或者多个Axes或Subplot实例使用Axes实例的辅助方法来创建primitive</code></pre><p>Axes是一种容器，它可能是matplotlib API中最重要的类，并且我们大多数时间都花在和它打交道上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入三方模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Circle, Wedge</span><br><span class="line"><span class="keyword">from</span> matplotlib.collections <span class="keyword">import</span> PatchCollection</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个Figure实例</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在画布上建立一个2*1的子画布绘图区</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里代表在2*1的第一个画布&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用axes实例的方法画一条曲线</span></span><br><span class="line">num = np.arange(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.01</span>)</span><br><span class="line"><span class="built_in">print</span>(num)</span><br><span class="line">sin = np.sin(<span class="number">2</span> * np.pi * num)</span><br><span class="line"></span><br><span class="line">line, = ax.plot(num, sin, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ax.plot()返回带有一个元素的元组。通过在分配目标列表中添加逗号，Python解开返回值并将其分配给依次命名为左侧的每个变量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">大多数情况下，这适用于具有多个返回值的函数;</span></span><br><span class="line"><span class="string">左侧可以包含任意数量的元素，并且如果它是元组或变量列表，则用逗号将进行拆包</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201218235411232.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="2-自定义你的Artist对象"><a href="#2-自定义你的Artist对象" class="headerlink" title="2 自定义你的Artist对象"></a>2 自定义你的Artist对象</h1><h2 id="2-1-Artist属性"><a href="#2-1-Artist属性" class="headerlink" title="2.1 Artist属性"></a>2.1 Artist属性</h2><p>Figure本身包含一个Rectangle，Rectangle的大小就是Figure的大小，可以用来设置Figure的背景色和透明度。 每个Axes边界框(默认白底黑边)，也有一个Rectangle，通过它可以设置Axes的颜色、透明度等。 这些实例都存储在成员变量(member variables) Figure.patch 和 Axes.patch中 Patch是一个来源于MATLAB的名词，它是图形上颜色的一个2D补丁，包含rectangels-矩形，circles-圆 和 plygons-多边形）</p><p>Figure.patch属性：是一个Rectangle，代表了图表的矩形框，它的大小就是图表的大小， 并且可以通过它设置figure的背景色和透明度。 Axes.patch属性：也是一个Rectangle，代表了绘图坐标轴内部的矩形框（白底黑边）， 通过它可以设置Axes的颜色、透明度等。</p><p>每个matplotlib Artist都有以下常见属性：</p><pre><code>.alpha属性：透明度。值为0—1之间的浮点数.axes属性：返回这个Artist所属的axes，可能为None.figure属性：该Artist所属的Figure，可能为None.label：一个text label.visible：布尔值，控制Artist是否绘制</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .patch</span></span><br><span class="line">plt.figure().patch</span><br><span class="line">plt.axes().patch</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201218235544827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-2-属性调用方式"><a href="#2-2-属性调用方式" class="headerlink" title="2.2 属性调用方式"></a>2.2 属性调用方式</h2><p>Artist对象的所有属性都通过相应的 get_* 和 set_* 函数进行读写。<br>例如下面的语句将alpha属性设置为当前值的一半</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig_1 = plt.figure()</span><br><span class="line">ax1 = fig_1.add_subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">num = np.arange(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.01</span>)</span><br><span class="line"><span class="built_in">print</span>(num)</span><br><span class="line">sin = np.sin(<span class="number">2</span> * np.pi * num)</span><br><span class="line"></span><br><span class="line">line, = ax1.plot(num, sin, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line">ax1.set_alpha(<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201218235632562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取ax1的透明度</span></span><br><span class="line">ax1_alpha = ax1.get_alpha()</span><br><span class="line"><span class="built_in">print</span>(ax1_alpha)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201218235700983.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重设ax1的透明度</span></span><br><span class="line">ax1.set_alpha(<span class="number">0.5</span> * ax1_alpha)</span><br><span class="line"><span class="built_in">print</span>(ax1.get_alpha())</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201218235733479.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想一次设置多个属性</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;zorder参数 - 控制绘图顺序&#x27;&#x27;&#x27;</span></span><br><span class="line">ax1.<span class="built_in">set</span>(alpha=<span class="number">0.5</span>, zorder=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>可以使用 matplotlib.artist.getp(o,”alpha”) 来获取属性 如果指定属性名，则返回对象的该属性值； 如果不指定属性名，则返回对象的所有的属性和值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure rectangle的属性</span></span><br><span class="line">matplotlib.artist.getp(fig.patch)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020121823583634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="3-基本元素-primitives"><a href="#3-基本元素-primitives" class="headerlink" title="3 基本元素 - primitives"></a>3 基本元素 - primitives</h1><h2 id="3-1-2DLines"><a href="#3-1-2DLines" class="headerlink" title="3.1 2DLines"></a>3.1 2DLines</h2><p>在matplotlib中曲线的绘制，主要是通过类 matplotlib.lines.Line2D 来完成的。<br>Line2D的基类 - matplotlib.artist.Artist</p><p>matplotlib中线-line的含义：它表示的可以是连接所有顶点的实线样式，也可以是每个顶点的标记。</p><p>Line2D的构造函数如下：<br>class matplotlib.lines.Line2D(<br>    xdata, ydata, linewidth&#x3D;None, linestyle&#x3D;None, color&#x3D;None, marker&#x3D;None, markersize&#x3D;None,<br>    markeredgewidth&#x3D;None, markeredgecolor&#x3D;None, markerfacecolor&#x3D;None, markerfacecoloralt&#x3D;‘none’,<br>    fillstyle&#x3D;None, antialiased&#x3D;None, dash_capstyle&#x3D;None, solid_capstyle&#x3D;None,<br>    dash_joinstyle&#x3D;None, solid_joinstyle&#x3D;None, pickradius&#x3D;5, drawstyle&#x3D;None, markevery&#x3D;None, **kwargs<br>    )<br>        其中，常用参数如下：<br>            xdata - 需要绘制的line中点的在x轴上的取值，若忽略，则默认为range(1,len(ydata)+1)<br>            ydata - 需要绘制的line中点的在y轴上的取值<br>            linewidth - 线条的宽度<br>            linestyle - 线型<br>            color - 线条的颜色<br>            marker - 点的标记<br>            markersize - 标记的size</p><h3 id="3-1-1-Lines2D的属性设置"><a href="#3-1-1-Lines2D的属性设置" class="headerlink" title="3.1.1 Lines2D的属性设置"></a>3.1.1 Lines2D的属性设置</h3><p>有三种方法可以用设置线的属性</p><h4 id="直接在plot-函数中设置"><a href="#直接在plot-函数中设置" class="headerlink" title="直接在plot()函数中设置"></a>直接在plot()函数中设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line">plt.plot(x, y, linewidth=<span class="number">10</span>)<span class="comment">#linewidth - 控制线的粗细程度</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000139380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="通过获得线对象，对线对象进行设置"><a href="#通过获得线对象，对线对象进行设置" class="headerlink" title="通过获得线对象，对线对象进行设置"></a>通过获得线对象，对线对象进行设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">line, = plt.plot(x, y, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">line.set_antialiased(<span class="literal">False</span>) <span class="comment"># 关闭抗锯齿功能</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000217562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开抗锯齿试试</span></span><br><span class="line">line, = plt.plot(x, y, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">line.set_antialiased(<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;线条更加细腻平滑&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000350145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="获得线属性，使用setp-函数设置"><a href="#获得线属性，使用setp-函数设置" class="headerlink" title="获得线属性，使用setp()函数设置"></a>获得线属性，使用setp()函数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines = plt.plot(x, y)</span><br><span class="line">plt.setp(lines, color=<span class="string">&#x27;r&#x27;</span>, linewidth=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000418172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="3-1-2-如何绘制lines"><a href="#3-1-2-如何绘制lines" class="headerlink" title="3.1.2 如何绘制lines"></a>3.1.2 如何绘制lines</h3><h4 id="pyplot方法绘制"><a href="#pyplot方法绘制" class="headerlink" title="pyplot方法绘制"></a>pyplot方法绘制</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000505601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="Line2D对象绘制"><a href="#Line2D对象绘制" class="headerlink" title="Line2D对象绘制"></a>Line2D对象绘制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建实例</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;和1，1，1效果是一样的&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用Line2D对象绘制线条</span></span><br><span class="line">line = Line2D(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将线条加入子画布</span></span><br><span class="line">ax.add_line(line)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置子画布x轴和y轴的范围</span></span><br><span class="line">ax.set_xlim(<span class="built_in">min</span>(x), <span class="built_in">max</span>(x))</span><br><span class="line">ax.set_ylim(<span class="built_in">min</span>(y), <span class="built_in">max</span>(y))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000543609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="errorbar绘制误差折线图"><a href="#errorbar绘制误差折线图" class="headerlink" title="errorbar绘制误差折线图"></a>errorbar绘制误差折线图</h4><p>pyplot里有个专门绘制误差线的功能，通过errorbar类实现</p><p>它的构造函数：<br>matplotlib.pyplot.errorbar(<br>        x, y, yerr&#x3D;None, xerr&#x3D;None, fmt&#x3D;’’, ecolor&#x3D;None, elinewidth&#x3D;None,<br>        capsize&#x3D;None, barsabove&#x3D;False, lolims&#x3D;False, uplims&#x3D;False, xlolims&#x3D;False,<br>        xuplims&#x3D;False, errorevery&#x3D;1, capthick&#x3D;None, *, data&#x3D;None, **kwargs<br>        )<br>                其中主要的参数是如下：<br>                    x - 需要绘制的line中点的在x轴上的取值<br>                    y - 需要绘制的line中点的在y轴上的取值<br>                    yerr - 指定y轴水平的误差<br>                    xerr - 指定x轴水平的误差<br>                    fmt - 指定折线图中某个点的颜色，形状，线条风格，例如‘co–’<br>                    ecolor - 指定error bar的颜色<br>                    elinewidth - 指定error bar的线条宽度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制一个errorbar试试，创建一个画布</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">x = np.arange(<span class="number">10</span>)</span><br><span class="line">y = <span class="number">2.5</span> * np.sin(x / <span class="number">20</span> * np.pi)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成0.05-0.2中10个元素的等差数列，这里的个数和x轴个数（若连续则为观测个数）保持一致</span></span><br><span class="line">yerr = np.linspace(<span class="number">0.05</span>, <span class="number">0.2</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(yerr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建误差折线图</span></span><br><span class="line">plt.errorbar(x, y + <span class="number">3</span>, yerr=yerr, label=<span class="string">&#x27;both limits (default)&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020121900071786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="3-1-3-patches"><a href="#3-1-3-patches" class="headerlink" title="3.1.3 patches"></a>3.1.3 patches</h3><p>matplotlib.patches.Patch类是二维图形类。它的基类是matplotlib.artist.Artist，它的构造函数如下</p><p>Patch( edgecolor&#x3D;None, facecolor&#x3D;None, color&#x3D;None,linewidth&#x3D;None, linestyle&#x3D;None, antialiased&#x3D;None,hatch&#x3D;None, fill&#x3D;True, capstyle&#x3D;None, joinstyle&#x3D;None,**kwargs )</p><h4 id="Rectangle-矩形"><a href="#Rectangle-矩形" class="headerlink" title="Rectangle-矩形"></a>Rectangle-矩形</h4><p>Rectangle矩形类在官网中的定义是： 通过锚点xy及其宽度和高度生成。<br>Rectangle本身的主要比较简单，即xy控制锚点，width和height分别控制宽和高。它的构造函数如下</p><p>class matplotlib.patches.Rectangle(xy, width, height, angle&#x3D;0.0, **kwargs)<br>实际中最常见的矩形图是hist直方图和bar条形图</p><h5 id="hist-直方图"><a href="#hist-直方图" class="headerlink" title="hist-直方图"></a>hist-直方图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">构造函数如下</span><br><span class="line"></span><br><span class="line">matplotlib.pyplot.hist(</span><br><span class="line">        x, bins=<span class="literal">None</span>, <span class="built_in">range</span>=<span class="literal">None</span>, density=<span class="literal">None</span>, bottom=<span class="literal">None</span>, histtype=‘bar’, align=‘mid’, </span><br><span class="line">        log=<span class="literal">False</span>, color=<span class="literal">None</span>, label=<span class="literal">None</span>, stacked=<span class="literal">False</span>, normed=<span class="literal">None</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">            常用的参数如下：</span><br><span class="line"></span><br><span class="line">                x - 数据集，最终的直方图将对数据集进行统计</span><br><span class="line">                bins - 统计的区间分布</span><br><span class="line">                <span class="built_in">range</span> - <span class="built_in">tuple</span>, 显示的区间，<span class="built_in">range</span>在没有给出bins时生效</span><br><span class="line">                density - <span class="built_in">bool</span>，默认为false，显示的是频数统计结果，为<span class="literal">True</span>则显示频率统计结果，这里需要注意，</span><br><span class="line">                            频率统计结果=区间数目/(总数*区间宽度)，和normed效果一致，官方推荐使用density</span><br><span class="line">                histtype - 可选&#123;‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’&#125;之一，默认为bar，推荐使用默认配置，</span><br><span class="line">                            step使用的是梯状，stepfilled则会对梯状内部进行填充，效果与bar类似</span><br><span class="line">                align - 可选&#123;‘left’, ‘mid’, ‘right’&#125;之一，默认为’mid’，控制柱状图的水平分布，left或者right，</span><br><span class="line">                            会有部分空白区域，推荐使用默认</span><br><span class="line">                log - <span class="built_in">bool</span>，默认<span class="literal">False</span>,即y坐标轴是否选择指数刻度</span><br><span class="line">                stacked - <span class="built_in">bool</span>，默认为<span class="literal">False</span>，是否为堆积状图</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000918655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hist绘制直方图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成100个0-99间的随机整数作为数据集</span></span><br><span class="line">x = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 划分分桶区间：[0,10), [10, 20)...[90, 100]</span></span><br><span class="line">bins = np.arange(<span class="number">0</span>, <span class="number">101</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 绘制柱状图</span></span><br><span class="line">plt.hist(x, bins, color=<span class="string">&#x27;fuchsia&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;scores&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219000937295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="Rectangle矩形类绘制直方图"><a href="#Rectangle矩形类绘制直方图" class="headerlink" title="Rectangle矩形类绘制直方图"></a>Rectangle矩形类绘制直方图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个DataFrame数据框</span></span><br><span class="line">df = pd.DataFrame(columns = [<span class="string">&#x27;data&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># ‘data’列用x填充</span></span><br><span class="line">df.loc[:, <span class="string">&#x27;data&#x27;</span>] = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分桶列</span></span><br><span class="line">df[<span class="string">&#x27;fenzu&#x27;</span>] = pd.cut(df[<span class="string">&#x27;data&#x27;</span>], bins=bins, right=<span class="literal">False</span>, include_lowest=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">参数解释：</span></span><br><span class="line"><span class="string">    right - 是否包含右端点</span></span><br><span class="line"><span class="string">    include_lowest - 是否包含左端点</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分桶列排列重组</span></span><br><span class="line">df_cnt = df[<span class="string">&#x27;fenzu&#x27;</span>].value_counts().reset_index()</span><br><span class="line"><span class="built_in">print</span>(df_cnt)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001037450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用正则将第一个数字取出的记为最小值</span></span><br><span class="line">df_cnt.loc[:,<span class="string">&#x27;mini&#x27;</span>] = df_cnt[<span class="string">&#x27;index&#x27;</span>].astype(<span class="built_in">str</span>).<span class="built_in">map</span>(<span class="keyword">lambda</span> x:re.findall(<span class="string">&#x27;\[(.*)\,&#x27;</span>,x)[<span class="number">0</span>]).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将右边的取出记为最大值</span></span><br><span class="line">df_cnt.loc[:,<span class="string">&#x27;maxi&#x27;</span>] = df_cnt[<span class="string">&#x27;index&#x27;</span>].astype(<span class="built_in">str</span>).<span class="built_in">map</span>(<span class="keyword">lambda</span> x:re.findall(<span class="string">&#x27;\,(.*)\)&#x27;</span>,x)[<span class="number">0</span>]).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值和最大值的差值是这个分桶区间的宽度</span></span><br><span class="line">df_cnt.loc[:,<span class="string">&#x27;width&#x27;</span>] = df_cnt[<span class="string">&#x27;maxi&#x27;</span>]- df_cnt[<span class="string">&#x27;mini&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据最小值升序</span></span><br><span class="line">df_cnt.sort_values(<span class="string">&#x27;mini&#x27;</span>, ascending=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重塑索引</span></span><br><span class="line">df_cnt.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df_cnt.head(<span class="number">20</span>))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001109230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用Rectangle绘制hist</span></span><br><span class="line"><span class="comment"># 创建实例</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建子画布</span></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"><span class="comment"># rect2 = plt.Rectangle((0, 0), 10, 10)</span></span><br><span class="line"><span class="comment"># ax2.add_patch(rect2)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_cnt.index:</span><br><span class="line">    <span class="comment"># 每个矩形的起始是它的min，终止是min+width，高度是分组数（y）</span></span><br><span class="line">    rect = plt.Rectangle((df_cnt.loc[i, <span class="string">&#x27;mini&#x27;</span>], <span class="number">0</span>), df_cnt.loc[i, <span class="string">&#x27;width&#x27;</span>], df_cnt.loc[i, <span class="string">&#x27;fenzu&#x27;</span>])</span><br><span class="line">    ax2.add_patch(rect)</span><br><span class="line">ax2.set_xlim(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">ax2.set_ylim(<span class="number">0</span>, <span class="number">16</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001159110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="bar-柱状图"><a href="#bar-柱状图" class="headerlink" title="bar-柱状图"></a>bar-柱状图</h5><p>bar()的构造函数如下：</p><p>matplotlib.pyplot.bar(left, height, alpha&#x3D;1, width&#x3D;0.8, color&#x3D;, edgecolor&#x3D;, label&#x3D;, lw&#x3D;3)</p><pre><code>    常用的参数：            left - x轴的位置序列，一般采用range函数产生一个序列，但是有时候可以是字符串            height - y轴的数值序列，也就是柱形图的高度，一般就是我们需要展示的数据；            alpha - 透明度，值越小越透明            width - 为柱形图的宽度，一般这是为0.8即可；            color或facecolor - 柱形图填充的颜色；            edgecolor - 图形边缘颜色            label - 解释每个图像代表的含义，这个参数是为legend()函数做铺垫的，表示该次bar的标签            lw - linewidths,边缘or线的宽度</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bar绘制柱状图</span></span><br><span class="line">y = <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">17</span>)</span><br><span class="line">plt.bar(np.arange(<span class="number">16</span>), y, alpha=<span class="number">0.5</span>, width=<span class="number">0.5</span>, color=<span class="string">&#x27;yellow&#x27;</span>, edgecolor=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;the first bar&#x27;</span>, lw=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001242416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h6 id="Rectangle矩形类绘制柱状图"><a href="#Rectangle矩形类绘制柱状图" class="headerlink" title="Rectangle矩形类绘制柱状图"></a>Rectangle矩形类绘制柱状图</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">17</span>):</span><br><span class="line">    <span class="comment"># 生成一个个的矩形框</span></span><br><span class="line">    rect = plt.Rectangle((i+<span class="number">0.25</span>, <span class="number">0</span>), <span class="number">0.5</span>, i)</span><br><span class="line">    <span class="comment"># 将生成的矩形框加入子画布</span></span><br><span class="line">    ax1.add_patch(rect)</span><br><span class="line">ax1.set_xlim(<span class="number">0</span>, <span class="number">16</span>)</span><br><span class="line">ax1.set_ylim(<span class="number">0</span>, <span class="number">16</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001332604.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="Polygon-多边形"><a href="#Polygon-多边形" class="headerlink" title="Polygon-多边形"></a>Polygon-多边形</h4><p>matplotlib.patches.Polygon类是多边形类。其基类是matplotlib.patches.Patch，它的构造函数如下：</p><p>class matplotlib.patches.Polygon(xy, closed&#x3D;True, **kwargs)<br>其中：<br>    xy是一个N×2的numpy array，为多边形的顶点。<br>    closed为True则指定多边形将起点和终点重合从而显式关闭多边形</p><p>matplotlib.patches.Polygon类中常用的是fill类，它是基于xy绘制一个填充的多边形，它的定义如下：</p><p>matplotlib.pyplot.fill(*args, data&#x3D;None, **kwargs)</p><p>关于x、y和color的序列，其中color是可选的参数，每个多边形都是由其节点的x和y位置列表定义的，后面可以选择一个颜色说明符。您可以通过提供多个x、y、[颜色]组来绘制多个多边形</p><h5 id="fill绘制图形"><a href="#fill绘制图形" class="headerlink" title="fill绘制图形"></a>fill绘制图形</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">5</span> * np.pi, <span class="number">1000</span>)</span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.sin(<span class="number">2</span> * x)</span><br><span class="line">plt.fill(x, y1, color=<span class="string">&#x27;g&#x27;</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">plt.fill(x, y2, color=<span class="string">&#x27;r&#x27;</span>, alpha=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001507702.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="Wedge-契形"><a href="#Wedge-契形" class="headerlink" title="Wedge-契形"></a>Wedge-契形</h4><p>matplotlib.patches.Polygon类是多边形类。其基类是matplotlib.patches.Patch，它的构造函数如下：</p><p>class matplotlib.patches.Wedge(center, r, theta1, theta2, width&#x3D;None, **kwargs)<br>    一个Wedge是以坐标x,y为中心，半径为r，从θ1扫到θ2(单位是度)；<br>    如果宽度给定，则从内半径r到外半径r画出部分楔形；<br>    wedge中比较常见的是绘制饼状图</p><p>matplotlib.pyplot.pie的构造函数如下</p><p>matplotlib.pyplot.pie(<br>        x, explode&#x3D;None, labels&#x3D;None, colors&#x3D;None, autopct&#x3D;None, pctdistance&#x3D;0.6,<br>        shadow&#x3D;False, labeldistance&#x3D;1.1, startangle&#x3D;0, radius&#x3D;1,<br>        counterclock&#x3D;True, wedgeprops&#x3D;None, textprops&#x3D;None, center&#x3D;0, 0,<br>        frame&#x3D;False, rotatelabels&#x3D;False, *, normalize&#x3D;None, data&#x3D;None<br>        )<br>            制作数据x的饼图，每个楔子的面积用x&#x2F;sum(x)表示<br>        其中主要的参数如下：<br>            x - 契型的形状，一维数组。<br>            explode - 如果不是等于None，则是一个len(x)数组，它指定用于偏移每个楔形块的半径的分数。<br>            labels - 用于指定每个契型块的标记，取值是列表或为None。<br>            colors - 饼图循环使用的颜色序列。如果取值为None，将使用当前活动循环中的颜色。<br>            startangle - 饼状图开始的绘制的角度。<br>            autopct - 控制饼图内百分比设置,可以使用format字符串或者format function<br>                        ‘%1.1f’指小数点前后位数(如下例，百分值显示1位小数)；<br>            shadow - 在饼图下面画一个阴影。默认值：False，即不画阴影</p><h5 id="pie绘制饼状图"><a href="#pie绘制饼状图" class="headerlink" title="pie绘制饼状图"></a>pie绘制饼状图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成饼图的标签</span></span><br><span class="line">labels = <span class="string">&#x27;Frogs&#x27;</span>, <span class="string">&#x27;Hogs&#x27;</span>, <span class="string">&#x27;Dogs&#x27;</span>, <span class="string">&#x27;Logs&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给定每个标签的数值</span></span><br><span class="line">sizes = [<span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类似与excel将饼图拉出来</span></span><br><span class="line">explode = (<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">fig1, ax1 = plt.subplots()</span><br><span class="line">ax1.pie(sizes, explode=explode, labels=labels, autopct=<span class="string">&#x27;%1.1f%%&#x27;</span>, shadow=<span class="literal">True</span>, startangle=<span class="number">90</span>)</span><br><span class="line">ax1.axis(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;该行代码使饼图长宽相等&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001619437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="wedge绘制饼图"><a href="#wedge绘制饼图" class="headerlink" title="wedge绘制饼图"></a>wedge绘制饼图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot()</span><br><span class="line"></span><br><span class="line">patches = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以0.3，0.3为中心，0.2为半径，按照角度扫描顺序，分辨绘制4个子扇图</span></span><br><span class="line">patches += [</span><br><span class="line">    Wedge((<span class="number">0.3</span>, <span class="number">0.3</span>), <span class="number">.2</span>, <span class="number">0</span>, <span class="number">54</span>),</span><br><span class="line">    Wedge((<span class="number">0.3</span>, <span class="number">0.3</span>), <span class="number">.2</span>, <span class="number">54</span>, <span class="number">162</span>),</span><br><span class="line">    Wedge((<span class="number">0.3</span>, <span class="number">0.3</span>), <span class="number">.2</span>, <span class="number">162</span>, <span class="number">324</span>),</span><br><span class="line">    Wedge((<span class="number">0.3</span>, <span class="number">0.3</span>), <span class="number">.2</span>, <span class="number">324</span>, <span class="number">360</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">colors = <span class="number">100</span> * np.random.rand(<span class="built_in">len</span>(patches))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;通过random.rand函数可以返回一个或一组服从“0~1”均匀分布的随机样本值。随机样本取值范围是[0,1)，不包括1&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造一个patch的集合</span></span><br><span class="line">p = PatchCollection(patches, alpha=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将color分别赋予每个patch</span></span><br><span class="line">p.set_array(colors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将集合添加至子画布</span></span><br><span class="line">ax.add_collection(p)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001652428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="3-1-4-collections"><a href="#3-1-4-collections" class="headerlink" title="3.1.4 collections"></a>3.1.4 collections</h3><p>collections类是用来绘制一组对象的集合<br>collections有许多不同的子类，如RegularPolyCollection, CircleCollection, Pathcollection, 分别对应不同的集合子类型。<br>其中比较常用的就是散点图，它是属于PathCollection子类，scatter方法提供了该类的封装，根据x与y绘制不同大小或颜色标记的散点图。 它的构造函数如下：</p><p>Axes.scatter(<br>        self, x, y, s&#x3D;None, c&#x3D;None, marker&#x3D;None, cmap&#x3D;None, norm&#x3D;None,<br>        vmin&#x3D;None, vmax&#x3D;None, alpha&#x3D;None, linewidths&#x3D;None, verts&#x3D;,<br>        edgecolors&#x3D;None, *, plotnonfinite&#x3D;False, data&#x3D;None, **kwargs<br>        )<br>            其中主要的参数如下：</p><pre><code>        x - 数据点x轴的位置        y - 数据点y轴的位置        s - 尺寸大小        c - 可以是单个颜色格式的字符串，也可以是一系列颜色        marker - 标记的类型</code></pre><h4 id="scatter绘制散点图"><a href="#scatter绘制散点图" class="headerlink" title="scatter绘制散点图"></a>scatter绘制散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="built_in">list</span>(np.arange(<span class="number">0</span>, <span class="number">11</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的10一定得用列表框起来，不然没法得到6个10的列表，就变成60了。这边是生成y的数组，每个元素都是10</span></span><br><span class="line">y = [<span class="number">10</span>] * <span class="built_in">len</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尺寸 = 20 * 2 ** n</span></span><br><span class="line">s = [<span class="number">20</span> * <span class="number">2</span> ** n <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x))]</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, s)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001750409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="3-1-5-images"><a href="#3-1-5-images" class="headerlink" title="3.1.5 images"></a>3.1.5 images</h3><p>images是matplotlib中绘制image图像的类，其中最常用的imshow可以根据数组绘制成图像，它的构造函数如下：</p><p>class matplotlib.image.AxesImage(<br>        ax, cmap&#x3D;None, norm&#x3D;None, interpolation&#x3D;None, origin&#x3D;None,<br>        extent&#x3D;None, filternorm&#x3D;True, filterrad&#x3D;4.0, resample&#x3D;False, **kwargs<br>        )</p><p>imshow根据数组绘制图像,构造函数如下：</p><p>matplotlib.pyplot.imshow(<br>        X, cmap&#x3D;None, norm&#x3D;None, aspect&#x3D;None, interpolation&#x3D;None,<br>        alpha&#x3D;None, vmin&#x3D;None, vmax&#x3D;None, origin&#x3D;None, extent&#x3D;None,<br>        shape&#x3D;, filternorm&#x3D;1, filterrad&#x3D;4.0, imlim&#x3D;, resample&#x3D;None,<br>        url&#x3D;None, *, data&#x3D;None, **kwargs<br>        ）<br>            主要参数解释如下：<br>                    orign - 指定绘制热图时的方向，默认值为upper,此时热图的右上角为(0, 0)<br>                            当设置为lower时，热图的左下角为(0,0)<br>                    cmap - colormap的简称，用于指定渐变色，默认的值为viridis<br>                    vmin和vmax - 用于限定数值的范围，只将vmin和vmax之间的值进行映射<br>                    interprolation - 控制热图的显示形式，是一个较难理解的参数<br>                    extent - 指定热图x轴和y轴的极值，取值为一个长度为4的元组或列表<br>                                其中，前两个数值对应x轴的最小值和最大值，后两个参数对应y轴的最小值和最大值</p><p>使用imshow画图时首先需要传入一个数组，数组对应的是空间内的像素位置和像素点的值<br>interpolation参数可以设置不同的差值方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 举个例子</span></span><br><span class="line">methods = [</span><br><span class="line">    <span class="literal">None</span>, <span class="string">&#x27;none&#x27;</span>, <span class="string">&#x27;nearest&#x27;</span>, <span class="string">&#x27;bilinear&#x27;</span>, <span class="string">&#x27;bicubic&#x27;</span>, <span class="string">&#x27;spline16&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;spline36&#x27;</span>, <span class="string">&#x27;hanning&#x27;</span>, <span class="string">&#x27;hamming&#x27;</span>, <span class="string">&#x27;hermite&#x27;</span>, <span class="string">&#x27;kaiser&#x27;</span>, <span class="string">&#x27;quadric&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;catrom&#x27;</span>, <span class="string">&#x27;gaussian&#x27;</span>, <span class="string">&#x27;bessel&#x27;</span>, <span class="string">&#x27;mitchell&#x27;</span>, <span class="string">&#x27;sinc&#x27;</span>, <span class="string">&#x27;lanczos&#x27;</span>]</span><br><span class="line"></span><br><span class="line">grid = np.random.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># subplot_kw - 字典类型，可选参数。把字典的关键字传递给add_subplot()来创建每个子图</span></span><br><span class="line">fig, axes = plt.subplots(nrows=<span class="number">3</span>, ncols=<span class="number">6</span>, figsize=(<span class="number">9</span>, <span class="number">6</span>), </span><br><span class="line">                         subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: [], <span class="string">&#x27;yticks&#x27;</span>: []&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax, interp_method <span class="keyword">in</span> <span class="built_in">zip</span>(axes.flat, methods):</span><br><span class="line">    ax.imshow(grid, interpolation=interp_method, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">    ax.set_title(<span class="built_in">str</span>(interp_method))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># tight_layout会自动调整子图参数，使之填充整个图像区域。这是个实验特性，可能在一些情况下不工作。</span></span><br><span class="line"><span class="comment"># 它仅仅检查坐标轴标签、刻度标签以及标题的部分</span></span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><h1 id="4-对象容器-Object-container"><a href="#4-对象容器-Object-container" class="headerlink" title="4 对象容器 - Object container"></a>4 对象容器 - Object container</h1><p>容器会包含一些primitives，并且容器还有它自身的属性。<br>比如Axes Artist，它是一种容器，它包含了很多primitives，比如Line2D，Text；同时，它也有自身的属性，比如xscal，用来控制X轴是linear还是log的</p><h2 id="4-1-Figure容器"><a href="#4-1-Figure容器" class="headerlink" title="4.1 Figure容器"></a>4.1 Figure容器</h2><p>matplotlib.figure.Figure是Artist最顶层的container-对象容器它包含了图表中的所有元素。<br>一张图表的背景就是Figure.patch的一个矩形Rectangle</p><p>当我们向图表添加Figure.add_subplot()或者Figure.add_axes()元素时，这些都会被添加到Figure.axes列表中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个Figure</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">ax2 = fig.add_axes([<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.3</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;位置参数，四个数分别代表了(left,bottom,width,height)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219001951174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>由于Figure维持了current axes，因此你应该手动的从Figure.axes列表中添加删除元素<br>而是要通过Figure.add_subplot()、Figure.add_axes()来添加元素<br>通过Figure.delaxes()来删除元素。<br>也可以迭代或者访问Figure.axes中的Axes，然后修改这个Axes的属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遍历axes里的内容，并且添加网格线</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">212</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> fig.axes:</span><br><span class="line">    ax.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002017950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Figure也有它自己的text、line、patch、image<br>可以直接通过add primitive语句直接添加<br>但是注意Figure默认的坐标系是以像素为单位，需要转换成figure坐标系：(0,0)表示左下点，(1,1)表示右上点</p><pre><code>Figure容器的常见属性：    Figure.patch属性 - Figure的背景矩形    Figure.axes属性 - 一个Axes实例的列表（包括Subplot)    Figure.images属性 - 一个FigureImages patch列表    Figure.lines属性 - 一个Line2D实例的列表（很少使用）    Figure.legends属性 - 一个Figure Legend实例列表（不同于Axes.legends)    Figure.texts属性 - 一个Figure Text实例列表</code></pre><h2 id="4-2-Axes容器"><a href="#4-2-Axes容器" class="headerlink" title="4.2 Axes容器"></a>4.2 Axes容器</h2><p>matplotlib.axes.Axes是matplotlib的核心。<br>大量的用于绘图的Artist存放在它内部，并且它有许多辅助方法来创建和添加Artist给它自己，而且它也有许多赋值方法来访问和修改这些Artist。</p><p>和Figure容器类似，Axes包含了一个patch属性<br>对于笛卡尔坐标系而言，它是一个Rectangle；<br>对于极坐标而言，它是一个Circle;<br>这个patch属性决定了绘图区域的形状、背景和边框</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># axes的path是一个rectangle实例</span></span><br><span class="line">rect = ax.patch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置绿色背景</span></span><br><span class="line">rect.set_facecolor(<span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002120479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>不应该直接通过Axes.lines和Axes.patches列表来添加图表。<br>因为当创建或添加一个对象到图表中时，Axes会做许多自动化的工作:<br>它会设置Artist中figure和axes的属性，同时默认Axes的转换；<br>它也会检视Artist中的数据，来更新数据结构，这样数据范围和呈现方式可以根据作图范围自动调整<br>也可以使用Axes的辅助方法.add_line()和.add_patch()方法来直接添加</p><p>另外Axes还包含两个最重要的Artist container：</p><p>ax.xaxis：XAxis对象的实例，用于处理x轴tick以及label的绘制<br>ax.yaxis：YAxis对象的实例，用于处理y轴tick以及label的绘制</p><p>Axes容器的常见属性有：<br>    artists - Artist实例列表<br>    patch - Axes所在的矩形实例<br>    collections - Collection实例<br>    images - Axes图像<br>    legends - Legend 实例<br>    lines - Line2D 实例<br>    patches - Patch 实例<br>    texts - Text 实例<br>    xaxis - matplotlib.axis.XAxis 实例<br>    yaxis - matplotlib.axis.YAxis 实例</p><h2 id="4-3-Axis容器"><a href="#4-3-Axis容器" class="headerlink" title="4.3 Axis容器"></a>4.3 Axis容器</h2><p>matplotlib.axis.Axis实例处理tick line、grid line、tick label以及axis label的绘制，<br>它包括坐标轴上的刻度线、刻度label、坐标网格、坐标轴标题。<br>通常你可以独立的配置y轴的左边刻度以及右边的刻度，也可以独立地配置x轴的上边刻度以及下边的刻度<br>刻度包括主刻度和次刻度，它们都是Tick刻度对象</p><p>每个Axis都有一个label属性，也有主刻度列表和次刻度列表。这些ticks是axis.XTick和axis.YTick实例，它们包含着line primitive以及text primitive用来渲染刻度线以及刻度文本</p><p>刻度是动态创建的，只有在需要创建的时候才创建（比如缩放的时候）。Axis也提供了一些辅助方法来获取刻度文本、刻度线位置等等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建画布</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">x = <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">y = [<span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;-&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002219974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取x轴对象</span></span><br><span class="line">axis = ax.xaxis</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取刻度线位置</span></span><br><span class="line">axis.get_ticklocs()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002243509.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取刻度label列表（一个text实例的列表）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;可以通过minor=True/False控制输出minor/major&#x27;&#x27;&#x27;</span></span><br><span class="line">axis.get_ticklabels()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020121900230111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取刻度线列表（一个line2D实例的列表）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;可以通过minor=True/False控制输出minor/major&#x27;&#x27;&#x27;</span></span><br><span class="line">axis.get_ticklines()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002319606.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取轴刻度间隔</span></span><br><span class="line">axis.get_data_interval()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002334454.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取轴视角间隔</span></span><br><span class="line">axis.get_view_interval()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002350743.png" alt="在这里插入图片描述"><br>展示如何调整一些轴和刻度的属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建figure矩形实例</span></span><br><span class="line">rect = fig.patch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置该矩形为黄色背景</span></span><br><span class="line">rect.set_facecolor(<span class="string">&#x27;lightgoldenrodyellow&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个axes对象，从（0.1，0.3）开始，宽和高均为0.4</span></span><br><span class="line">ax = fig.add_axes([<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建axse矩形实例，将ax的矩形设置为灰色</span></span><br><span class="line">rect = ax.patch</span><br><span class="line">rect.set_facecolor(<span class="string">&#x27;lightslategray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> ax.xaxis.get_ticklabels():</span><br><span class="line">    label.set_color(<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;调整x轴刻度标签实例（text实例）&#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    label.set_rotation(<span class="number">45</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;旋转角度为45度&#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    label.set_fontsize(<span class="number">16</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;设置字体大小&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ax.yaxis.get_ticklines():</span><br><span class="line">    line.set_color(<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;调用y轴刻度线条实例（line2D实例）&#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    line.set_markersize(<span class="number">25</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;设置marker（标记）大小&#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    line.set_markeredgewidth(<span class="number">2</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;设置marker(标记)粗细&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002419108.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="4-4-Tick容器"><a href="#4-4-Tick容器" class="headerlink" title="4.4 Tick容器"></a>4.4 Tick容器</h2><p>matplotlib.axis.Tick是从Figure到Axes到Axis到Tick中最末端的容器对象。<br>Tick包含了tick、grid line实例以及对应的label</p><p>所有的这些都可以通过Tick的属性获取，常见的tick属性有<br>        Tick.tick1line - Line2D实例<br>        Tick.tick2line - Line2D实例<br>        Tick.gridline - Line2D实例<br>        Tick.label1 - Text实例<br>        Tick.label2 - Text实例</p><p>y轴分为左右两个，因此tick1对应左侧的轴；tick2对应右侧的轴。<br>x轴分为上下两个，因此tick1对应下侧的轴；tick2对应上侧的轴。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将y轴右边轴设置成主轴，并将标签设置为美元符号且为绿色</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(<span class="number">100</span> * np.random.rand(<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置ticker的显示格式:美元符号，两位小数</span></span><br><span class="line">formatter = matplotlib.ticker.FormatStrFormatter(<span class="string">&#x27;$%1.2f&#x27;</span>)</span><br><span class="line">ax.yaxis.set_major_formatter(formatter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置ticker的参数，右侧为主轴，颜色为绿色</span></span><br><span class="line">ax.yaxis.set_tick_params(which=<span class="string">&#x27;major&#x27;</span>, labelcolor=<span class="string">&#x27;green&#x27;</span>, labelleft=<span class="literal">False</span>, labelright=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020121900245854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="5-思考题"><a href="#5-思考题" class="headerlink" title="5 思考题"></a>5 思考题</h1><h2 id="5-1"><a href="#5-1" class="headerlink" title="5.1"></a>5.1</h2><p>questions：primitives 和 container的区别和联系是什么？</p><p>answer：primitives是图形中的基本要素，包含一些我们在绘图区域作图时用到的标准图形对象，如Line2D, Text, Rectangle, image等；</p><h2 id="5-2"><a href="#5-2" class="headerlink" title="5.2"></a>5.2</h2><p>questions：四个容器的联系和区别是么？他们分别控制一张图表的哪些要素？</p><p>answer：四个容器关系为Figure包含Axes包含Axis包含Tick；<br>        Figure控制图表中的所有元素；<br>        Axes控制大量的用于绘图的Artist；<br>        Axis控制坐标轴如坐标轴上的刻度线、刻度label、坐标网格、坐标轴标题；<br>        Tick控制图表的tickTick，包含了tick、grid line实例以及对应的label</p><h1 id="6-绘图题"><a href="#6-绘图题" class="headerlink" title="6 绘图题"></a>6 绘图题</h1><h2 id="6-1"><a href="#6-1" class="headerlink" title="6.1"></a>6.1</h2><p>教程中展示的案例都是单一图，请自行创建数据，画出包含6个子图的线图，要求：<br>子图排布是 2 * 3 （2行 3列）；<br>线图可用教程中line2D方法绘制；<br>需要设置每个子图的横坐标和纵坐标刻度；<br>并设置整个图的标题，横坐标名称，以及纵坐标名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个2*3的画布,分别放这6组数据</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">    x = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">    y = [<span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line">    ax = fig.add_subplot(<span class="number">2</span>, <span class="number">3</span>, i)</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">&#x27;total_title&#x27;</span>, fontsize=<span class="number">20</span>, color=<span class="string">&#x27;red&#x27;</span>, backgroundcolor=<span class="string">&#x27;yellow&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;设置全局标题&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;x_label&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y_label&#x27;</span>)</span><br><span class="line">plt.subplots_adjust(wspace=<span class="number">0.1</span>, hspace=<span class="number">0.1</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002608134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="6-2"><a href="#6-2" class="headerlink" title="6.2"></a>6.2</h2><p>分别用一组长方形柱和填充面积的方式模仿画出下图，函数 y &#x3D; -1 * (x - 2) * (x - 8) +10 在区间[2,9]的积分面积<br><img src="https://img-blog.csdnimg.cn/2020121900271615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">x = np.linspace(<span class="number">2</span>, <span class="number">9</span>, <span class="number">20000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_y</span>(<span class="params">x</span>):</span><br><span class="line">    y = -<span class="number">1</span> * (x - <span class="number">2</span>) * (x - <span class="number">8</span>) + <span class="number">10</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y = cal_y(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图像</span></span><br><span class="line">ax.plot(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为要用长方形柱子并填充，所以先给到分桶</span></span><br><span class="line">cut = (<span class="number">9</span> - <span class="number">2</span>) / <span class="number">20000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x, y):</span><br><span class="line">    rect = plt.Rectangle((x, <span class="number">0</span>), cut, y)</span><br><span class="line">    ax.add_patch(rect)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201219002745287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="7-结语"><a href="#7-结语" class="headerlink" title="7 结语"></a>7 结语</h1><p>这期的内容比较多，但是基础而又重要，这对理解matplotlib里面的运作原理非常有帮助。通过这一课，我对matplotlib不再停留在浑浑噩噩的阶段，至少心中有了一个框架，问题分解后，解决问题就容易多了。希望这一次学习能对大家有帮助，下节课继续～</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-序&quot;&gt;&lt;a href=&quot;#0-序&quot; class=&quot;headerlink&quot; title=&quot;0 序&quot;&gt;&lt;/a&gt;0 序&lt;/h1&gt;&lt;p&gt;这一章节对matplotlib的框架做了详细的介绍，包括matplotlib的三个API分别是什么，有什么作用？matplotlib</summary>
      
    
    
    
    <category term="Matplotlib入门" scheme="http://example.com/categories/Matplotlib%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib_布局格式定方圆</title>
    <link href="http://example.com/2023/04/24/DataWhale_Matplotlib_%E5%B8%83%E5%B1%80%E6%A0%BC%E5%BC%8F%E5%AE%9A%E6%96%B9%E5%9C%86/"/>
    <id>http://example.com/2023/04/24/DataWhale_Matplotlib_%E5%B8%83%E5%B1%80%E6%A0%BC%E5%BC%8F%E5%AE%9A%E6%96%B9%E5%9C%86/</id>
    <published>2023-04-24T08:04:53.355Z</published>
    <updated>2023-04-24T08:16:01.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-序"><a href="#0-序" class="headerlink" title="0 序"></a>0 序</h1><p>这一章主要讲解了子图种类和子图上的方法。对于数据分析来说，针对某一类数据往往需要给出不同维度的图表分析，以形成可视化看板，所以子图的排版以及操作方法的熟练掌握就显的尤为重要了。<br>章节内容不多，但是同样重要，上代码～</p><h1 id="1-导入三方模块"><a href="#1-导入三方模块" class="headerlink" title="1 导入三方模块"></a>1 导入三方模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决中文乱码的问题</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决负号异常显示问题</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>这里的plt.rcParams可以解决大多数中文乱码的问题了。因为时间问题，我还没有细细研究mac上的字体解决方案（尝试了几个都还是乱码，所以暂时先用英文啦～）</p><h1 id="2-子图"><a href="#2-子图" class="headerlink" title="2 子图"></a>2 子图</h1><h2 id="2-1-使用plt-subplots绘制均匀状态下的子图"><a href="#2-1-使用plt-subplots绘制均匀状态下的子图" class="headerlink" title="2.1 使用plt.subplots绘制均匀状态下的子图"></a>2.1 使用plt.subplots绘制均匀状态下的子图</h2><p>返回元素分别是画布和子图构成的列表，第一个数字为行，第二个为列<br>    figsize参数可以指定整个画布的大小<br>    sharex和sharey分别表示是否共享横轴和纵轴刻度<br>    tight_layout函数可以调整子图的相对大小使字符不会重叠</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">10</span>, <span class="number">4</span>), sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br><span class="line">fig.suptitle(<span class="string">&#x27;sample1&#x27;</span>,size=<span class="number">20</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;整个画布的标题&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        axs[i][j].scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line">        axs[i][j].set_title(<span class="string">&#x27;row%d, column%d&#x27;</span>%(i+<span class="number">1</span>, j+<span class="number">1</span>))</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;设置每个子图的子标题&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        axs[i][j].set_xlim(-<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">        axs[i][j].set_ylim(-<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">if</span> i==<span class="number">1</span>: axs[i][j].set_xlabel(<span class="string">&#x27;xlabel&#x27;</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;xlabel只在最底部显示&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> j==<span class="number">0</span>: axs[i][j].set_ylabel(<span class="string">&#x27;ylabel&#x27;</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;ylabel只在最左侧显示&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">fig.tight_layout()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;调整子图的相对大小使字符不会重叠&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220204052268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-2-使用GridSpec绘制非均匀子图"><a href="#2-2-使用GridSpec绘制非均匀子图" class="headerlink" title="2.2 使用GridSpec绘制非均匀子图"></a>2.2 使用GridSpec绘制非均匀子图</h2><p>所谓非均匀包含两层含义:<br>    第一是指图的比例大小不同但没有跨行或跨列<br>    第二是指图为跨列或跨行状态</p><p>利用 add_gridspec 可以指定相对宽度比例 width_ratios 和相对高度比例参数 height_ratios</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">spec = fig.add_gridspec(nrows=<span class="number">2</span>, ncols=<span class="number">5</span>, width_ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], height_ratios=[<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;width_ratios和height_ratios都是比例&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;下面的代码和上一段代码意思相近&#x27;&#x27;&#x27;</span></span><br><span class="line">fig.suptitle(<span class="string">&#x27;sample2&#x27;</span>, size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        ax = fig.add_subplot(spec[i, j])</span><br><span class="line">        ax.scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line">        ax.set_title(<span class="string">&#x27;row%d, column%d&#x27;</span>%(i+<span class="number">1</span>, j+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> i==<span class="number">1</span>: ax.set_xlabel(<span class="string">&#x27;xlabel&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> j==<span class="number">0</span>: ax.set_ylabel(<span class="string">&#x27;ylabel&#x27;</span>)</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020122020425560.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在上面的例子中出现了 spec[i, j] 的用法，事实上通过切片就可以实现子图的合并而达到跨图的功能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">spec = fig.add_gridspec(nrows=<span class="number">2</span>, ncols=<span class="number">6</span>, </span><br><span class="line">                        width_ratios=[<span class="number">2</span>, <span class="number">2.5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1.5</span>, <span class="number">2</span>], height_ratios=[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub1</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">0</span>, :<span class="number">3</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;sub1占用的大小为第一行，宽度为第零块到第二块&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub2</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">0</span>, <span class="number">3</span>: <span class="number">5</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;sub占用的大小为第一行，宽度为第三块到第四块&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub3</span></span><br><span class="line">ax = fig.add_subplot(spec[:, <span class="number">5</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;整列（1行+2行），第五块&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;下同，类似切片，画画图就能理解了&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># sub4</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">ax.scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub5</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">1</span>, <span class="number">1</span>:<span class="number">5</span>])</span><br><span class="line">ax.scatter(np.random.randn(<span class="number">10</span>), np.random.randn(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220204902360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="3-子图上的方法"><a href="#3-子图上的方法" class="headerlink" title="3 子图上的方法"></a>3 子图上的方法</h1><p>在 ax 对象上定义了和 plt 类似的图形绘制函数，常用的有： plot, hist, scatter, bar, barh, pie</p><h2 id="绘制直线"><a href="#绘制直线" class="headerlink" title="绘制直线"></a>绘制直线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">ax.plot([<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220204939784.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="绘制直方图"><a href="#绘制直方图" class="headerlink" title="绘制直方图"></a>绘制直方图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">ax.hist(np.random.randn(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205037155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>常用直线的画法为： axhline, axvline, axline （水平、垂直、任意方向）</p><h3 id="添加水平-x2F-垂直辅助线"><a href="#添加水平-x2F-垂直辅助线" class="headerlink" title="添加水平&#x2F;垂直辅助线"></a>添加水平&#x2F;垂直辅助线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的0.2和0.8不知道设置什么</span></span><br><span class="line">ax.axhline(<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.8</span>)</span><br><span class="line">ax.axvline(<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.8</span>)</span><br><span class="line">ax.axline([<span class="number">0.3</span>, <span class="number">0.3</span>], [<span class="number">0.7</span>, <span class="number">0.7</span>])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205103725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="使用grid加灰色网格"><a href="#使用grid加灰色网格" class="headerlink" title="使用grid加灰色网格"></a>使用grid加灰色网格</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">ax.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205152202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>使用 set_xscale, set_title, set_xlabel 分别可以设置坐标轴的规度（指对数坐标等）、标题、轴名</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;sample3&#x27;</span>, size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    axs[j].plot(<span class="built_in">list</span>(<span class="string">&#x27;abcd&#x27;</span>), [<span class="number">10</span> ** i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">    <span class="keyword">if</span> j==<span class="number">0</span>:</span><br><span class="line">        axs[j].set_yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;y轴刻度为log坐标&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        axs[j].set_title(<span class="string">&#x27;sub_title1&#x27;</span>)</span><br><span class="line">        axs[j].set_ylabel(<span class="string">&#x27;log_label&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        axs[j].set_title(<span class="string">&#x27;sub_title1&#x27;</span>)</span><br><span class="line">        axs[j].set_ylabel(<span class="string">&#x27;normal_label&#x27;</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;y轴刻度为普通坐标&#x27;&#x27;&#x27;</span></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205339510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>与一般的 plt 方法类似， legend, annotate, arrow, text 对象也可以进行相应的绘制</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">ax.arrow(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, head_width=<span class="number">0.03</span>, head_length=<span class="number">0.05</span>, facecolor=<span class="string">&#x27;red&#x27;</span>, edgecolor=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">绘制箭头</span></span><br><span class="line"><span class="string">0，0 - 箭头起点坐标</span></span><br><span class="line"><span class="string">1，1 - 箭头终点坐标</span></span><br><span class="line"><span class="string">head_width - 箭头头部的宽</span></span><br><span class="line"><span class="string">head_length - 箭头头部的高</span></span><br><span class="line"><span class="string">facecolor - 箭头填充色</span></span><br><span class="line"><span class="string">edgecolor - 箭头的轮廓色</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.text(x=<span class="number">0</span>, y=<span class="number">0</span>, s=<span class="string">&#x27;this is a graph&#x27;</span>, fontsize=<span class="number">16</span>, rotation=<span class="number">70</span>, </span><br><span class="line">        rotation_mode=<span class="string">&#x27;anchor&#x27;</span>, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x,y - 文字的坐标</span></span><br><span class="line"><span class="string">s - 文字内容</span></span><br><span class="line"><span class="string">fontsize - 字体大小</span></span><br><span class="line"><span class="string">rotation - 旋转的角度</span></span><br><span class="line"><span class="string">rotation_mode - 对齐方式</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.annotate(<span class="string">&#x27;this is mid point&#x27;</span>, xy=(<span class="number">0.5</span>, <span class="number">0.5</span>), </span><br><span class="line">            xytext=(<span class="number">0.8</span>, <span class="number">0.2</span>), arrowprops=<span class="built_in">dict</span>(facecolor=<span class="string">&#x27;yellow&#x27;</span>, edgecolor=<span class="string">&#x27;black&#x27;</span>), </span><br><span class="line">            fontsize=<span class="number">16</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">xy - 箭头坐标</span></span><br><span class="line"><span class="string">xytext - 文字起始坐标</span></span><br><span class="line"><span class="string">arrowprops - 箭头的参数字典：填充颜色和边缘色</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205407667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这里有点丑，可以把画布调整大点或把箭头的注释字体弄小点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot([<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>], label=<span class="string">&#x27;line1&#x27;</span>)</span><br><span class="line">ax.plot([<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], label=<span class="string">&#x27;line1&#x27;</span>)</span><br><span class="line">ax.legend(loc=<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;loc=1 - 图例在右上方&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205501832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为了方便，我网上找了一个图例参数-代码的对照表，po给大家：<br><img src="https://img-blog.csdnimg.cn/20201220205536203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="4-作业"><a href="#4-作业" class="headerlink" title="4 作业"></a>4 作业</h1><h2 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h2><p>数据为墨尔本1981年至1990年的每月温度情况，请利用数据，画出如下的图：<br><img src="https://img-blog.csdnimg.cn/20201220205632434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>先看一下数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&#x27;./data/layout_ex1.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205734104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我看每个子图的x轴是月份刻度，但是数据里面是yyyy-mm，所以我选择做一下提取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;Time_month&#x27;</span>] = data[<span class="string">&#x27;Time&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(<span class="built_in">str</span>(x)[-<span class="number">2</span>:]))</span><br><span class="line">data.head(<span class="number">15</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220205858302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>然后准备每个子图的x轴和y轴的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x1 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">0</span>: <span class="number">12</span>])</span><br><span class="line">x2 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">12</span>: <span class="number">24</span>])</span><br><span class="line">x3 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">24</span>: <span class="number">36</span>])</span><br><span class="line">x4 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">36</span>: <span class="number">48</span>])</span><br><span class="line">x5 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">48</span>: <span class="number">60</span>])</span><br><span class="line">x6 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">60</span>: <span class="number">72</span>])</span><br><span class="line">x7 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">72</span>: <span class="number">84</span>])</span><br><span class="line">x8 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">84</span>: <span class="number">96</span>])</span><br><span class="line">x9 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">96</span>: <span class="number">108</span>])</span><br><span class="line">x10 = <span class="built_in">list</span>(data[<span class="string">&#x27;Time_month&#x27;</span>][<span class="number">108</span>: <span class="number">120</span>])</span><br><span class="line"></span><br><span class="line">x_list = [x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">0</span>: <span class="number">12</span>])</span><br><span class="line">y2 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">12</span>: <span class="number">24</span>])</span><br><span class="line">y3 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">24</span>: <span class="number">36</span>])</span><br><span class="line">y4 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">36</span>: <span class="number">48</span>])</span><br><span class="line">y5 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">48</span>: <span class="number">60</span>])</span><br><span class="line">y6 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">60</span>: <span class="number">72</span>])</span><br><span class="line">y7 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">72</span>: <span class="number">84</span>])</span><br><span class="line">y8 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">84</span>: <span class="number">96</span>])</span><br><span class="line">y9 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">96</span>: <span class="number">108</span>])</span><br><span class="line">y10 = <span class="built_in">list</span>(data[<span class="string">&#x27;Temperature&#x27;</span>][<span class="number">108</span>: <span class="number">120</span>])</span><br><span class="line"></span><br><span class="line">y_list = [y1, y2, y3, y4, y5, y6, y7, y8, y9, y10]</span><br></pre></td></tr></table></figure><p>开始画图！！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">20</span>, <span class="number">5</span>), sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Temperature curve of Melbourne from 1981 to 1990&#x27;</span>, size=<span class="number">25</span>)</span><br><span class="line">subtitle = [<span class="string">&#x27;1981&#x27;</span>, <span class="string">&#x27;1982&#x27;</span>, <span class="string">&#x27;1983&#x27;</span>, <span class="string">&#x27;1984&#x27;</span>, <span class="string">&#x27;1985&#x27;</span>, <span class="string">&#x27;1986&#x27;</span>, <span class="string">&#x27;1987&#x27;</span>, </span><br><span class="line">            <span class="string">&#x27;1988&#x27;</span>, <span class="string">&#x27;1989&#x27;</span>, <span class="string">&#x27;1990&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;搞了一个子标题的列表，方便循环的时候直接用&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k = <span class="number">0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这个k用于数子标题以及数据的选择&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        axs[i][j].plot(x_list[k], y_list[k], marker=<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;marker=&#x27;p&#x27; - 标记点为五角星～～～&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        axs[i][j].set_title(subtitle[k])</span><br><span class="line">        axs[i][j].set_yticks([<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>,<span class="number">20</span>])</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">1</span>: axs[i][j].set_xticks([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;只在第二行的子图显示x轴刻度&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>: axs[i][j].set_ylabel(<span class="string">&#x27;Temperature&#x27;</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;在第一列的子图显示y轴刻度&#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">fig.tight_layout（）</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220210334976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h2><p>画出数据的散点图和边际分布</p><p>用 np.random.randn(2, 150) 生成一组二维数据，使用两种非均匀子图的分割方法，做出该数据对应的散点图和边际分布图，如下图：<br><img src="https://img-blog.csdnimg.cn/20201220210436172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;正方形&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">spec = fig.add_gridspec(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>, width_ratios=[<span class="number">5</span>, <span class="number">1</span>], height_ratios=[<span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;划分块块，方便分区&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sub1</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;这里还是比较抽象，画个图就能理解&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.scatter(np.random.randn(<span class="number">2</span>, <span class="number">150</span>)[<span class="number">0</span>], np.random.randn(<span class="number">2</span>, <span class="number">150</span>)[<span class="number">1</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;二维数据，我第一行用来当x，第一行用来当y&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;my_data_x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;my_data_y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub2</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">ax.hist(np.random.randn(<span class="number">2</span>, <span class="number">150</span>)[<span class="number">0</span>], bins=<span class="number">10</span>)</span><br><span class="line">ax.set_xticks([])</span><br><span class="line">ax.set_yticks([])</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub3</span></span><br><span class="line">ax = fig.add_subplot(spec[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">ax.hist(np.random.randn(<span class="number">2</span>, <span class="number">150</span>)[<span class="number">1</span>], bins=<span class="number">10</span>, orientation=<span class="string">&#x27;horizontal&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;orientation=&#x27;horizontal&#x27;表示条形为水平方向&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">ax.set_xticks([])</span><br><span class="line">ax.set_yticks([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.subplots_adjust(wspace=<span class="number">0</span>, hspace=<span class="number">0</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;冬天了，让图靠得紧一点&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201220210756237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>画出来丑丑的，边界的色块也不知道该怎么调，不过肯定有方法，后面学会了再完善呀～</p><h1 id="5-结"><a href="#5-结" class="headerlink" title="5 结"></a>5 结</h1><p>这一期内容并不多，但是axes在matplotlib中的地位十分重要，希望通过这一篇文章可以提高大家的认识和基础功，下篇我们再会～</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-序&quot;&gt;&lt;a href=&quot;#0-序&quot; class=&quot;headerlink&quot; title=&quot;0 序&quot;&gt;&lt;/a&gt;0 序&lt;/h1&gt;&lt;p&gt;这一章主要讲解了子图种类和子图上的方法。对于数据分析来说，针对某一类数据往往需要给出不同维度的图表分析，以形成可视化看板，所以子图的</summary>
      
    
    
    
    <category term="Matplotlib入门" scheme="http://example.com/categories/Matplotlib%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib初相识</title>
    <link href="http://example.com/2023/04/24/DataWhale_Matplotlib_Matplotlib%E5%88%9D%E7%9B%B8%E8%AF%86/"/>
    <id>http://example.com/2023/04/24/DataWhale_Matplotlib_Matplotlib%E5%88%9D%E7%9B%B8%E8%AF%86/</id>
    <published>2023-04-24T08:04:38.519Z</published>
    <updated>2023-04-24T08:15:56.685Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-序"><a href="#0-序" class="headerlink" title="0.序"></a>0.序</h1><p>（当是完成本节课的小作业啦～）<br>answer：你在工作或学习中通常何时会用到数据可视化，希望通过可视化达到什么目的？</p><p>answer：本人从事交通大数据的数据清洗及分析工作，工作中会需要实时观查数据同步的异常情况，并且需要分析数据质量。故通常将各个监测指标可视化，分区并实时观察数据同步及分布情况。所以我想做一个可视化的面板，实现自动可视化报表监测。将统计结果直观的展现出来，并根据预设的阈值触发报警推送机制。</p><p>本文为本次学习任务的Task01，旨在对matplotlib的功能有个最初步的了解和认识，希望能够帮助到有需要的learner！</p><h1 id="1-导入三方模块"><a href="#1-导入三方模块" class="headerlink" title="1.导入三方模块"></a>1.导入三方模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br></pre></td></tr></table></figure><h1 id="2-举一个简单的绘图例子"><a href="#2-举一个简单的绘图例子" class="headerlink" title="2.举一个简单的绘图例子"></a>2.举一个简单的绘图例子</h1><p>Matplotlib的图像是画在figure上的，可以看作是一块大画布；<br>每一个figure（画布）又包含了一个或多个axes（一个可以指定坐标系的子画布）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化一个大画布，以及创建一个子画布</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214222816359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>若没有指定创建子画布的命令，那么matplotlib会自动创建一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214222854621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="3-Figure的组成"><a href="#3-Figure的组成" class="headerlink" title="3.Figure的组成"></a>3.Figure的组成</h1><p>figure包括四个层级-容器（container）<br>    1.figure - 顶层级，用来容纳所有的绘图元素<br>    2.axes - 核心层（子图），一个figure可以由多个子图构成<br>    3.axis - axes的下属层级，用于处理所有坐标轴及网格相关元素<br>    4.tick - axis的下属层级，用于处理所有刻度相关元素<br>    <img src="https://img-blog.csdnimg.cn/20201214222929434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="3-1创建带有多个子图的画布"><a href="#3-1创建带有多个子图的画布" class="headerlink" title="3.1创建带有多个子图的画布"></a>3.1创建带有多个子图的画布</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空白的画布无法进行绘图，可以使用add_subplots方法创建一个或多个子图</span></span><br><span class="line"><span class="comment">## 创建一个画布</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment">## (2, 2, 1)表示画布是2*2的，且现在选择的是四个中的第一个（序号从1开始）</span></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">ax3 = fig.add_subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223011823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这样就能得到带有三个空白的子图的画布</p><h2 id="3-2调整子图周围的间距"><a href="#3-2调整子图周围的间距" class="headerlink" title="3.2调整子图周围的间距"></a>3.2调整子图周围的间距</h2><p>默认情况下，matplotlib会在子图的外部和子图之间留出一定的间距（相对于图的高度和宽度来定）<br>我们可以使用subplots_adjust方法更改间距，也可以用作顶层函数</p><p>subplots_adjust(left&#x3D;None, bottom&#x3D;None, right&#x3D;None, top&#x3D;None, wspace&#x3D;None, hspace&#x3D;None)<br>其中，wspace和hspace分别控制图片的宽度和高度的百分比，以用作子图间的间距</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尝试不断调整间距，一直缩小到0</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        axes[i, j].hist(np.random.randn(<span class="number">500</span>), bins=<span class="number">50</span>, color=<span class="string">&#x27;k&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">## 这里可以自己调试间距</span></span><br><span class="line">plt.subplots_adjust(wspace=<span class="number">0</span>, hspace=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223058929.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="4-两种绘图接口"><a href="#4-两种绘图接口" class="headerlink" title="4.两种绘图接口"></a>4.两种绘图接口</h1><h2 id="4-1OO模式-显示和创建figure和axes"><a href="#4-1OO模式-显示和创建figure和axes" class="headerlink" title="4.1OO模式 - 显示和创建figure和axes"></a>4.1OO模式 - 显示和创建figure和axes</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">2</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>numpy.linspace(start, stop, num)<br>    在指定的间隔内返回均匀间隔的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以看一下生成的矩阵</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223254742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OO模式创建画布和子图</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 曲线y=x</span></span><br><span class="line">ax.plot(x, x, label=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 曲线y=x**2</span></span><br><span class="line">ax.plot(x, x**<span class="number">2</span>, label=<span class="string">&#x27;quadratic&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 曲线y=x**3</span></span><br><span class="line">ax.plot(x, x**<span class="number">3</span>, label=<span class="string">&#x27;cubic&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置x轴名称</span></span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x label&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置y轴名称</span></span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y label&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置图形名称</span></span><br><span class="line">ax.set_title(<span class="string">&#x27;Simple Plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图例</span></span><br><span class="line">ax.legend()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223319795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="4-2pyplot模式-即自动创建画布和子画布"><a href="#4-2pyplot模式-即自动创建画布和子画布" class="headerlink" title="4.2pyplot模式 - 即自动创建画布和子画布"></a>4.2pyplot模式 - 即自动创建画布和子画布</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()  </span><br><span class="line">ax.plot(x, x, label=<span class="string">&#x27;linear&#x27;</span>)  </span><br><span class="line">ax.plot(x, x**<span class="number">2</span>, label=<span class="string">&#x27;quadratic&#x27;</span>)  </span><br><span class="line">ax.plot(x, x**<span class="number">3</span>, label=<span class="string">&#x27;cubic&#x27;</span>)  </span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;x label&#x27;</span>) </span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;y label&#x27;</span>) </span><br><span class="line">ax.set_title(<span class="string">&quot;Simple Plot&quot;</span>)  </span><br><span class="line">ax.legend()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223402890.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到，两者输出的图片并没有什么不同的地方，后者的代码更为简介；<br>交互绘图的时候使用pyplot模式，会更加方便直观；<br>非交互绘图使用OO模式，采用多个子图同时展示的时候，这种方式逻辑更加清晰</p><h1 id="5-图形拓展设置"><a href="#5-图形拓展设置" class="headerlink" title="5.图形拓展设置"></a>5.图形拓展设置</h1><h2 id="5-1颜色、标记和线类型"><a href="#5-1颜色、标记和线类型" class="headerlink" title="5.1颜色、标记和线类型"></a>5.1颜色、标记和线类型</h2><p>matplotlib的主函数plot接受带有x和y轴的数组以及一些可选的字符串缩写参数来指明颜色和线类型<br>很多颜色缩写被用于常用颜色，但是可以通过指定16进制颜色代码来指定任意颜色（例如‘#CECECE’）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.random.randn(<span class="number">30</span>).cumsum(), <span class="string">&#x27;ko--&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223503719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对于折线图，后续的点默认是线性内插的，可以通过drawstyle选项进行更改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写的显式一些</span></span><br><span class="line">data = np.random.randn(<span class="number">30</span>).cumsum()</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020121422353420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(data, <span class="string">&#x27;k--&#x27;</span>, label=<span class="string">&#x27;Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 阶梯式折线图</span></span><br><span class="line">plt.plot(data, <span class="string">&#x27;k--&#x27;</span>, drawstyle=<span class="string">&#x27;steps-post&#x27;</span>, label=<span class="string">&#x27;steps-post&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择图例位置自动最佳</span></span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223551943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="5-2注释和子图加工"><a href="#5-2注释和子图加工" class="headerlink" title="5.2注释和子图加工"></a>5.2注释和子图加工</h2><p>有时，我们需要在图表上绘制自己的注释，而且注释中可能包含文本、箭头和其他图形<br>我们可以使用text，arrow，annote方法来添加注释和文本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">time1 = datetime(<span class="number">2007</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line">time2 = datetime(<span class="number">2008</span>, <span class="number">3</span>, <span class="number">12</span>)</span><br><span class="line">time3 = datetime(<span class="number">2008</span>, <span class="number">9</span>, <span class="number">15</span>)</span><br><span class="line">time4 = datetime(<span class="number">2007</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">time5 = datetime(<span class="number">2011</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入金融危机数据源</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;spx.csv&#x27;</span>, index_col=<span class="number">0</span>, parse_dates=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">spx = data[<span class="string">&#x27;SPX&#x27;</span>]</span><br><span class="line">spx.plot(ax=ax, style=<span class="string">&#x27;k-&#x27;</span>)</span><br><span class="line"></span><br><span class="line">crisis_data = [</span><br><span class="line">    (time1, <span class="string">&#x27;Peak of bull market&#x27;</span>),</span><br><span class="line">    (time2, <span class="string">&#x27;Bear Stearns Fails&#x27;</span>),</span><br><span class="line">    (time3, <span class="string">&#x27;Lehman Bankruptcy&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date, label <span class="keyword">in</span> crisis_data:</span><br><span class="line">    <span class="comment"># 在指定的点上绘制标签</span></span><br><span class="line">    ax.annotate(label, xy=(date, spx.asof(date) + <span class="number">75</span>),</span><br><span class="line">               xytext=(date, spx.asof(date) + <span class="number">225</span>),</span><br><span class="line">               arrowprops=<span class="built_in">dict</span>(facecolor=<span class="string">&#x27;black&#x27;</span>, headwidth=<span class="number">4</span>, width=<span class="number">2</span>, headlength=<span class="number">4</span>),</span><br><span class="line">               horizontalalignment=<span class="string">&#x27;left&#x27;</span>, verticalalignment=<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放大2007-2010年（手动设置图表边界）</span></span><br><span class="line">ax.set_xlim([time4, time5])</span><br><span class="line">ax.set_ylim([<span class="number">600</span>, <span class="number">1800</span>])</span><br><span class="line"></span><br><span class="line">ax.set_title(<span class="string">&#x27;Important dates in the 2008-2009 financial crisis&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201214223641781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出，我们要关注的点可以通过手动加注释来强调说明</p><h2 id="5-3保存生成的图片"><a href="#5-3保存生成的图片" class="headerlink" title="5.3保存生成的图片"></a>5.3保存生成的图片</h2><p>使用plt.savefig()方法将图片保存到文件<br>若将图片保存为SVG，则代码为plt.savesif(‘figpath.svg’)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如，我将上图保存</span></span><br><span class="line">plt.savefig(<span class="string">&#x27;fig_financial.svg&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这时候，我工作路径下就会生成一个名为fig_financial的svg文件<br><img src="https://img-blog.csdnimg.cn/20201214223811227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="6-结尾"><a href="#6-结尾" class="headerlink" title="6.结尾"></a>6.结尾</h1><p>这期主要是大家了解认识一下matplotlib，它的功能远远不止如此，一些好看的第三方可视化库如seaborn或者pandas自带的绘图都是基于matplotlib实现，后续会带大家一一了解。<br>希望能对大家有帮助～for the learner!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-序&quot;&gt;&lt;a href=&quot;#0-序&quot; class=&quot;headerlink&quot; title=&quot;0.序&quot;&gt;&lt;/a&gt;0.序&lt;/h1&gt;&lt;p&gt;（当是完成本节课的小作业啦～）&lt;br&gt;answer：你在工作或学习中通常何时会用到数据可视化，希望通过可视化达到什么目的？&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Matplotlib入门" scheme="http://example.com/categories/Matplotlib%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>机器学习训练_金融风控_Task5_模型融合</title>
    <link href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    <id>http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/</id>
    <published>2023-04-24T08:04:28.388Z</published>
    <updated>2023-04-24T08:18:27.375Z</updated>
    
    <content type="html"><![CDATA[<h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><p>模型融合是比赛后期上分的重要手段，特别是多人组队学习的比赛中，将不同队友的模型进行融合，可能会收获意想不到的效果哦，往往模型相差越大且模型表现都不错的前提下，模型融合后结果会有大幅提升。</p><h2 id="简单加权平均-结果直接融合"><a href="#简单加权平均-结果直接融合" class="headerlink" title="# 简单加权平均-结果直接融合"></a># 简单加权平均-结果直接融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">生成一些简单的样本数据,</span></span><br><span class="line"><span class="string">test_prei - 代表第i个模型的预测值</span></span><br><span class="line"><span class="string">y_test_true - 代表真实值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><h2 id="定义结果的加权平均函数-根据加权计算"><a href="#定义结果的加权平均函数-根据加权计算" class="headerlink" title="# 定义结果的加权平均函数 - 根据加权计算"></a># 定义结果的加权平均函数 - 根据加权计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_method</span>(<span class="params">test_pre1, test_pre2, test_pre3, w=[<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>]</span>):</span><br><span class="line">    weighted_result = w[<span class="number">0</span>] * pd.Series(test_pre1) + w[<span class="number">1</span>] * pd.Series(test_pre2) + w[<span class="number">2</span>] * pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> weighted_result</span><br></pre></td></tr></table></figure><h2 id="根据各模型的预测结果计算MAE"><a href="#根据各模型的预测结果计算MAE" class="headerlink" title="# 根据各模型的预测结果计算MAE"></a># 根据各模型的预测结果计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">metrics.mean_absolute_error - 多维数组MAE的计算方法</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred1 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred2 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred3 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre3))</span><br><span class="line"></span><br><span class="line">Pred1 MAE: <span class="number">0.1750000000000001</span></span><br><span class="line">Pred2 MAE: <span class="number">0.07499999999999993</span></span><br><span class="line">Pred3 MAE: <span class="number">0.10000000000000009</span></span><br></pre></td></tr></table></figure><h2 id="根据加权计算MAE"><a href="#根据加权计算MAE" class="headerlink" title="# 根据加权计算MAE"></a># 根据加权计算MAE</h2><h2 id="定义比重权值"><a href="#定义比重权值" class="headerlink" title="## 定义比重权值"></a>## 定义比重权值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]</span><br><span class="line">weighted_pre = weighted_method(test_pre1, test_pre2, test_pre3, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Weighted_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, weighted_pre))</span><br><span class="line"></span><br><span class="line">Weighted_pre MAE: <span class="number">0.05750000000000027</span></span><br></pre></td></tr></table></figure><h3 id="定义结果的加权平均函数-mean平均"><a href="#定义结果的加权平均函数-mean平均" class="headerlink" title="# 定义结果的加权平均函数 - mean平均"></a># 定义结果的加权平均函数 - mean平均</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    mean_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                             pd.Series(test_pre2),</span><br><span class="line">                             pd.Series(test_pre3)], axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mean_result</span><br></pre></td></tr></table></figure><h2 id="根据均值计算MAE"><a href="#根据均值计算MAE" class="headerlink" title="# 根据均值计算MAE"></a># 根据均值计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Mean_pre = mean_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Mean_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Mean_pre MAE: <span class="number">0.06666666666666693</span></span><br></pre></td></tr></table></figure><h2 id="定义结果的加权平均函数-median平均"><a href="#定义结果的加权平均函数-median平均" class="headerlink" title="# 定义结果的加权平均函数 - median平均"></a># 定义结果的加权平均函数 - median平均</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">median_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    median_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                               pd.Series(test_pre2),</span><br><span class="line">                               pd.Series(test_pre3)], axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> median_result</span><br></pre></td></tr></table></figure><h2 id="根据中位数计算MAE"><a href="#根据中位数计算MAE" class="headerlink" title="# 根据中位数计算MAE"></a># 根据中位数计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Median_pre = median_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Median_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Median_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Median_pre MAE: <span class="number">0.07500000000000007</span></span><br></pre></td></tr></table></figure><h1 id="Stacking融合-回归"><a href="#Stacking融合-回归" class="headerlink" title="# Stacking融合(回归)"></a># Stacking融合(回归)</h1><h2 id="定义Stacking融合函数"><a href="#定义Stacking融合函数" class="headerlink" title="# 定义Stacking融合函数"></a># 定义Stacking融合函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Stacking_method</span>(<span class="params">train_reg1, train_reg2, train_reg3,</span></span><br><span class="line"><span class="params">                    y_train_true,</span></span><br><span class="line"><span class="params">                    test_pre1, test_pre2, test_pre3,</span></span><br><span class="line"><span class="params">                    model_L2=linear_model.LinearRegression(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param train_reg1:  第一个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg2:  第二个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg3:  第三个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param y_train_true:    train真实的标签</span></span><br><span class="line"><span class="string">    :param test_pre1:   第一个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre2:   第二个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre3:   第三个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param model_L2:    次级模型:以真实训练集的标签为标签,以多个模型训练训练集后得到的标签合并后的数据集为特征进行训练</span></span><br><span class="line"><span class="string">                        注意:次级模型不宜选取的太复杂,这样会导致模型在训练集上过拟合,测试集泛化效果差</span></span><br><span class="line"><span class="string">    :return:            训练好的次机模型预测test数据集得到的预测值 - Stacking_result</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), pd.Series(train_reg3)], axis=<span class="number">1</span>).values,</span><br><span class="line">                 y_train_true)      <span class="comment"># 次级模型训练</span></span><br><span class="line">    stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),</span><br><span class="line">                                                  pd.Series(test_pre2), pd.Series(test_pre3)], axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> stacking_result</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值"><a href="#生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值" class="headerlink" title="# 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值"></a># 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><h2 id="看一下Stacking融合的效果"><a href="#看一下Stacking融合的效果" class="headerlink" title="# 看一下Stacking融合的效果"></a># 看一下Stacking融合的效果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_L2 = linear_model.LinearRegression()      <span class="comment"># 不设定这个参数也可以,创建函数的时候默认了</span></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,</span><br><span class="line">                               test_pre1, test_pre2, test_pre3, model_L2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Stacking_pre MAE: &#x27;</span>, metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Stacking_pre MAE:  <span class="number">0.042134831460675204</span></span><br><span class="line"><span class="comment"># 发现模型效果相对于之前有了更近一步的提升</span></span><br></pre></td></tr></table></figure><h1 id="分类模型融合-Voting-Stacking…"><a href="#分类模型融合-Voting-Stacking…" class="headerlink" title="# 分类模型融合 - Voting,Stacking…"></a># 分类模型融合 - Voting,Stacking…</h1><h2 id="Voting投票机制"><a href="#Voting投票机制" class="headerlink" title="# Voting投票机制"></a># Voting投票机制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Voting - 投票机制</span></span><br><span class="line"><span class="string">        1.硬投票 - 对多个模型直接进行投票,不区分模型结果的相对重要度,最终投票数最多的类为最终被预测的类</span></span><br><span class="line"><span class="string">        2.软投票 - 和硬投票原理相同,增加了设置权重的功能,可以为不同模型设置不同权重,进而区别模型不同的重要度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 硬投票</span></span><br><span class="line">iris = datasets.load_iris()     <span class="comment"># 读取鸢尾花数据集 - 分类问题</span></span><br><span class="line"></span><br><span class="line">x = iris.data   <span class="comment"># 分离特征集和标签</span></span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)    <span class="comment"># 训练集和测试集按照7:3比例切分</span></span><br></pre></td></tr></table></figure><h2 id="用XGB分类模型训练数据"><a href="#用XGB分类模型训练数据" class="headerlink" title="# 用XGB分类模型训练数据"></a># 用XGB分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">colsample_bytree - 训练每棵树时，使用的特征占全部特征的比例</span></span><br><span class="line"><span class="string">objective - 目标函数</span></span><br><span class="line"><span class="string">            二分类问题 - binary:logistic - 返回概率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="用随机森林分类模型训练数据"><a href="#用随机森林分类模型训练数据" class="headerlink" title="# 用随机森林分类模型训练数据"></a># 用随机森林分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">n_estimators - 随机森林中决策树的个数</span></span><br><span class="line"><span class="string">max_depth - 决策树的最大深度</span></span><br><span class="line"><span class="string">            如果值为None,那么会扩展节点,直到所有的叶子是纯净的,或者直到所有叶子包含少于min_sample_split的样本</span></span><br><span class="line"><span class="string">min_samples_split - 分割内部节点所需要的最小样本数量</span></span><br><span class="line"><span class="string">min_samples_leaf - 需要在叶子结点上的最小样本数量</span></span><br><span class="line"><span class="string">oob_score - 是否使用袋外样本来估计泛化精度</span></span><br><span class="line"><span class="string">            树的生成过程并不会使用所有的样本,未使用的样本就叫(out_of_bag)oob袋外样本,通过袋外样本,可以评估这个树的准确度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="用SVC训练数据"><a href="#用SVC训练数据" class="headerlink" title="# 用SVC训练数据"></a># 用SVC训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">支持向量机 - 分类算法，但是也可以做回归,根据输入的数据不同可做不同的模型</span></span><br><span class="line"><span class="string">            1.若输入标签为连续值则做回归</span></span><br><span class="line"><span class="string">            2.若输入标签为分类值则用SVC()做分类</span></span><br><span class="line"><span class="string">            支持向量机的学习策略是间隔最大化，最终可转化为一个凸二次规划问题的求解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">C - 惩罚参数;   值越大,对误分类的惩罚大,不容犯错,于是训练集测试准确率高,但是泛化能力弱</span></span><br><span class="line"><span class="string">                值越小,对误分类的惩罚小,允许犯错,泛化能力较强</span></span><br><span class="line"><span class="string">probability - 是否采用概率估计,默认为False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h2 id="硬投票"><a href="#硬投票" class="headerlink" title="# 硬投票"></a># 硬投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">eclf - 其实就是三个模型的集成算法,硬投票决定最终被预测的类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)     <span class="comment"># 本质是Ensemble</span></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.05</span>) [Ensemble]</span><br></pre></td></tr></table></figure><h2 id="软投票"><a href="#软投票" class="headerlink" title="# 软投票"></a># 软投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.8</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.8</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)</span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;soft&#x27;</span>, weights=[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [Ensemble]</span><br></pre></td></tr></table></figure><h1 id="分类的Stacking-x2F-Blending融合"><a href="#分类的Stacking-x2F-Blending融合" class="headerlink" title="# 分类的Stacking&#x2F;Blending融合"></a># 分类的Stacking&#x2F;Blending融合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Stacking是一种分层模型集成框架,以两层为例</span></span><br><span class="line"><span class="string">        第一层由多个基学习器组成,其输入为原始训练集</span></span><br><span class="line"><span class="string">        第二层的模型则是以第一层学习器的输出作为训练集进行再训练,从而得到完整的stacking模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># ## 创建训练用的数据集</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]  <span class="comment"># 100个样本</span></span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 模型融合中使用到的各个单模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LogisticRegression()</span></span><br><span class="line"><span class="string">            solver - 用来优化权重     &#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;,默认adam,</span></span><br><span class="line"><span class="string">                                        lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好</span></span><br><span class="line"><span class="string">                                        sgd - 随机梯度下降 </span></span><br><span class="line"><span class="string">                                        adam - 机遇随机梯度的优化器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RandomForestClassifier()</span></span><br><span class="line"><span class="string">            n_estimators - 决策树个数</span></span><br><span class="line"><span class="string">            n_jobs - 用于拟合和预测的并行运行的工作数量,如果值为-1,那么工作数量被设置为核的数量</span></span><br><span class="line"><span class="string">            criterion - 衡量分裂质量的性能</span></span><br><span class="line"><span class="string">                        1.gini - Gini impurity衡量的是从一个集合中随机选择一个元素</span></span><br><span class="line"><span class="string">                                基于该集合中标签的概率分布为元素分配标签的错误率</span></span><br><span class="line"><span class="string">                                Gini impurity的计算就非常简单了,即1减去所有分类正确的概率,得到的就是分类不正确的概率</span></span><br><span class="line"><span class="string">                                若元素数量非常多,且所有元素单独属于一个分类时，Gini不纯度达到极小值0</span></span><br><span class="line"><span class="string">                        2.entropy - 信息增益熵</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">ExtraTreesClassifier() - 极端随机树</span></span><br><span class="line"><span class="string">    该算法与随机森林算法十分相似,都是由许多决策树构成,但该算法与随机森林有两点主要的区别:</span></span><br><span class="line"><span class="string">        1.随机森林应用的是Bagging模型,而ET是使用所有的训练样本得到每棵决策树,也就是每棵决策树应用的是相同的全部训练样本</span></span><br><span class="line"><span class="string">            关于Bagging和Boosting的差别,可以参考 https://www.cnblogs.com/earendil/p/8872001.html</span></span><br><span class="line"><span class="string">        2.随机森林是在一个随机子集内得到最佳分叉属性,而ET是完全随机的得到分叉值,从而实现对决策树进行分叉的</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"> Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                            1.lad - 绝对损失</span></span><br><span class="line"><span class="string">                                            2.huber - Huber损失</span></span><br><span class="line"><span class="string">                                            3.quantile - 分位数损失</span></span><br><span class="line"><span class="string">                                            4.ls - 均方差损失(默认)</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">                                    alpha - 当我们使用Huber损失和分位数损失&#x27;quantile&#x27;时,需要指定分位数的值,只有regressor有</span></span><br><span class="line"><span class="string">                                    </span></span><br><span class="line"><span class="string">                            GradientBoostingClassifier() - 参数绝大多数和Regressor相同,不同的是loss函数</span></span><br><span class="line"><span class="string">                                            1.deviance - 对数似然损失函数(默认)</span></span><br><span class="line"><span class="string">                                            2.exponential - 指数损失函数       </span></span><br><span class="line"><span class="string">参考网址: https://www.cnblogs.com/pinard/p/6143927.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((X.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))  <span class="comment"># 全零数组,行取训练集的个数,列取模型个数</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 全零数组,行取测试集的个数,列取模型个数</span></span><br></pre></td></tr></table></figure><h2 id="5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练"><a href="#5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练" class="headerlink" title="# 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练"></a># 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)     <span class="comment"># # 分层交叉验证,每一折中都保持着原始数据中各个类别的比例关系(测试集和训练集分离)</span></span><br><span class="line">skf = skf.split(X, y)     <span class="comment"># 把特征和标签分离</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">enumerate() - 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列,同时列出数据和数据下标,一般用在for循环当中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 30行5列的全0数组</span></span><br><span class="line">    <span class="comment"># 五折交叉训练,使用第i个部分作为预测集,剩余部分为验证集,获得的预测值成为第i部分的新特征</span></span><br><span class="line">    <span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf):</span><br><span class="line">        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        <span class="comment"># 将对测试集的概率预测第二列(也就是结果为1)的概率装进y_submission中</span></span><br><span class="line">        y_submission = clf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">        dataset_blend_train[test, j] = y_submission     <span class="comment"># 把预测验证集(比如第一折)的结果依次对应装进dataset_blend_train中</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        predict_proba() - 返回的是一个n行k列的数组</span></span><br><span class="line"><span class="string">                            第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率,并且每一行的概率和为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        因为我们采取到的数据集的标签只有0或1,所以predict_proba返回的概率只有两个</span></span><br><span class="line"><span class="string">                    如果左边的概率大于0.5,那么预测值为0</span></span><br><span class="line"><span class="string">                    如果右边的概率大于0.5,那么预测值为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># # 将对测试集的概率预测的第二列(也就是结果为1)的概率装进dataset_blend_test_j中</span></span><br><span class="line">        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对于测试集,直接用这5个模型的预测值均值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = dataset_blend_test_j.mean(<span class="number">1</span>)     <span class="comment"># mean(1) - 求每行数的平均值(五折预测测试集的平均值)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>)    <span class="comment"># 次级学习器再次训练</span></span><br><span class="line">clf.fit(dataset_blend_train, y)     <span class="comment"># 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_blend_test)[:, <span class="number">1</span>]  <span class="comment"># 把第一层预测测试集的结果作为新特征,预测测试集的标签</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ROC曲线和AUC - 用来评价一个二值分类器(binary classifier)的优劣,用于衡量&#x27;二分类问题&#x27;机器学习算法性能(泛化能力)</span></span><br><span class="line"><span class="string">AUC - ROC曲线下的面积</span></span><br><span class="line"><span class="string">      AUC的取值范围在0.5和1之间</span></span><br><span class="line"><span class="string">      使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好</span></span><br><span class="line"><span class="string">      而作为一个数值,对应AUC更大的分类器效果更好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure><h2 id="Blending-和Stacking类似-不同点在于"><a href="#Blending-和Stacking类似-不同点在于" class="headerlink" title="# Blending - 和Stacking类似,不同点在于:"></a># Blending - 和Stacking类似,不同点在于:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.Stacking - 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string">2.Blending - 把第一层得到训练集中的30%的验证集的结果作为新特征继续训练,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Blending优点 - 比stacking简单,因为不用进行k次的交叉验证来获得stacker feature</span></span><br><span class="line"><span class="string">                避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</span></span><br><span class="line"><span class="string">Blending缺点- 使用了很少的数据,可能会过拟合,没有stacking使用多次的交叉验证来的稳健</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把训练数据分成d1(子训练集),d2(验证集)两部分 - 对半分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))   <span class="comment"># 35行5列的全0数组</span></span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))      <span class="comment"># 30行5列的全0数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 用子训练集依次训练各个模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    <span class="comment"># 返回模型对验证集的预测值为1的概率</span></span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 结果装进dataset_d1中 - 表示用子训练集训练的模型预测验证集标签的结果 - 就是上文说的30%的数据</span></span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment"># 建立第二层模型的特征 - 用第一层模型预测测试集的结果作为新的特征</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 看一下预测的预测集标签和真实的预测集标签的roc_auc_score</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用第二层模型训练特征</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)   <span class="comment"># 用验证集的第一层模型预测结果作为特征,用验证集的真实标签作为标签,再次训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, <span class="number">1</span>]  <span class="comment"># 用第一层模型预测测试集的结果作为特征,用第二层模型预测训练集返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Blending: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">Val auc Score of Blending: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure><h2 id="利用mlxtend进行分类的Stacking融合"><a href="#利用mlxtend进行分类的Stacking融合" class="headerlink" title="# 利用mlxtend进行分类的Stacking融合"></a># 利用mlxtend进行分类的Stacking融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, <span class="number">1</span>:<span class="number">3</span>], iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=<span class="number">1</span>)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">StackingClassifier() - 快速Stacking融合的方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">classifiers - 一级分类器列表</span></span><br><span class="line"><span class="string">meta_classifier - 二级分类器(元分类器)</span></span><br><span class="line"><span class="string">use_probas - 如果为True,则基于预测的概率而不是类标签来训练元分类器,默认为False</span></span><br><span class="line"><span class="string">average_probas - 如果为真,将概率平均为元特征,默认为False</span></span><br><span class="line"><span class="string">verbose - 是否输出到日志</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                          meta_classifier=lr)</span><br><span class="line">label = [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>, <span class="string">&#x27;Stacking Classifier&#x27;</span>]</span><br><span class="line">clf_list = [clf1, clf2, clf3, sclf]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># 网格布局,每行2个,每列2个</span></span><br><span class="line">grid = itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)  <span class="comment"># 求多个可迭代对象的笛卡尔积,其实就是更加灵活调整网格的大小</span></span><br><span class="line"></span><br><span class="line">clf_cv_mean = []    <span class="comment"># 存放每个模型的准确率的均值</span></span><br><span class="line">clf_cv_std = []     <span class="comment"># 存放每个模型的准确率的标准差</span></span><br><span class="line"><span class="keyword">for</span> clf, label, grd <span class="keyword">in</span> <span class="built_in">zip</span>(clf_list, label, grid):</span><br><span class="line"></span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 3折交叉验证,评分标准为模型准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.2f (+/- %.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line">    clf_cv_mean.append(scores.mean())</span><br><span class="line">    clf_cv_std.append(scores.std())</span><br><span class="line"></span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(label)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.02</span>) [Stacking Classifier]</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出,融合后的曲线更加优秀</p><h2 id="一些其它方法"><a href="#一些其它方法" class="headerlink" title="# 一些其它方法"></a># 一些其它方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将特征放进模型中预测,并将预测结果变换并作为新的特征加入原有特征中,再经过模型预测结果(Stacking变化)</span></span><br><span class="line"><span class="string">可以反复预测多次将结果加入最后的特征中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ensemble_add_feature</span>(<span class="params">train, test, target, clfs</span>):</span><br><span class="line">    <span class="comment"># n_folds = 5</span></span><br><span class="line">    <span class="comment"># skf = list(StratifiedKFold(y, n_folds=n_folds))</span></span><br><span class="line">    train_ = np.zeros((train.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    test_ = np.zeros((test.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="comment"># 依次训练单个模型</span></span><br><span class="line">        <span class="built_in">print</span>(j, clf)</span><br><span class="line">        <span class="comment"># 使用第1部分作为预测,第2部分来训练模型(第1部分预测的输出作为第2部分的新特征)</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train]</span></span><br><span class="line">        clf.fit(train, target)  <span class="comment"># 训练模型</span></span><br><span class="line">        y_train = clf.predict(train)    <span class="comment"># 模型在训练集中的预测值</span></span><br><span class="line">        y_test = clf.predict(test)      <span class="comment"># 模型在测试集中的预测值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成新特征</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        j 从0开始递增,构建新的特征集,特征为训练集和测试集各自的预测值的平方</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_[:, j*<span class="number">2</span>] = y_train ** <span class="number">2</span></span><br><span class="line">        test_[:, j*<span class="number">2</span>] = y_test ** <span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)    <span class="comment"># np.exp(a) - 返回e的a次方</span></span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method:&#x27;</span>, j)</span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf = LogisticRegression()  <span class="comment"># 次级模型</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.3</span>)</span><br><span class="line">x_train = pd.DataFrame(x_train)     <span class="comment"># 转换成DataFrame格式,方便后续构造新特征</span></span><br><span class="line">x_test = pd.DataFrame(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给出模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 新特征的构造 - 用上面的各个单模型预测训练集和测试集的结果,作为新特征</span></span><br><span class="line">New_train, New_test = ensemble_add_feature(x_train, x_test, y_train, clfs)</span><br><span class="line">clf.fit(New_train, y_train)     <span class="comment"># 用训练集的新特征和训练集的真实标签训练数据</span></span><br><span class="line">y_emb = clf.predict_proba(New_test)[:, <span class="number">1</span>]   <span class="comment"># 用训练好的模型得到新的测试集特征返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_test, y_emb)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method: <span class="number">4</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure><p>关于模型融合的理论和方法具体可以参考：<a href="https://github.com/datawhalechina/team-learning-data-mining/blob/master/FinancialRiskControl/Task5%20%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88.md">https://github.com/datawhalechina/team-learning-data-mining/blob/master/FinancialRiskControl/Task5%20%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88.md</a></p><p>这边文章由于时间关系，只提供了一个大致的思路，具体调参以及之后的融合可以参照task4和上文提供的链接尝试。</p><h1 id="导入三方模块"><a href="#导入三方模块" class="headerlink" title="导入三方模块"></a>导入三方模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, train_test_split, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns = <span class="literal">None</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, <span class="keyword">lambda</span> x: <span class="string">&#x27;%.2f&#x27;</span> % x)</span><br></pre></td></tr></table></figure><p><strong>做模型融合之前，我要做两件事情：特征筛选和模型选择</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">特征筛选：随机森林或SFS挑选最优特征</span></span><br><span class="line"><span class="string">模型选择：xgboost, lightgbm, logistic 加权融合。或者再加线性回归模型嵌套一个弱分类器学习预测</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="不同模型的数据准备注意点"><a href="#不同模型的数据准备注意点" class="headerlink" title="不同模型的数据准备注意点"></a>不同模型的数据准备注意点</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">使用xgboost要注意的点：</span></span><br><span class="line"><span class="string">    1.在Xgb中需要将离散特征one-hot编码再和连续特征一起输入训练，这样做是为了达到在cart树中处理离散特征的方式一致</span></span><br><span class="line"><span class="string">    2.无需处理缺失值，在Xgb中处理稀疏数据时，没有值的特征是走默认的分支，所以在Xgb中缺省值也是走默认分支</span></span><br><span class="line"><span class="string">lightgbm:</span></span><br><span class="line"><span class="string">    1.由于使用直方图算法，LightGBM直接支持类别特征，对类别特征不必进行独热编码处理（与xgboost不同）</span></span><br><span class="line"><span class="string">    2.也可以直接处理缺失值</span></span><br><span class="line"><span class="string">logistic:</span></span><br><span class="line"><span class="string">    1.数据要进行缺失值和异常值的处理</span></span><br><span class="line"><span class="string">    2.类别数据要做one-hot编码</span></span><br><span class="line"><span class="string">    3.对于某些方差大的特征，建议做归一化处理以增强模型稳定性</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="读取、压缩数据"><a href="#读取、压缩数据" class="headerlink" title="读取、压缩数据"></a>读取、压缩数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\train.csv&#x27;</span>)</span><br><span class="line">testA = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\testA.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>  <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:  <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:  <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>    <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))  <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = reduce_mem_usage(train)</span><br><span class="line">testA = reduce_mem_usage(testA)</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.2&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.3&#x27;</span>]</span><br></pre></td></tr></table></figure><p><strong>为了方便起见，把训练集和测试集合并处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = pd.concat([train, testA], axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="lgb"><a href="#lgb" class="headerlink" title="lgb"></a>lgb</h1><h2 id="字段-employmentLength-10年以上算10年，1年一下算0年；然后转化成数值"><a href="#字段-employmentLength-10年以上算10年，1年一下算0年；然后转化成数值" class="headerlink" title="字段 employmentLength - 10年以上算10年，1年一下算0年；然后转化成数值"></a>字段 employmentLength - 10年以上算10年，1年一下算0年；然后转化成数值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;10+ years&#x27;</span>, value=<span class="string">&#x27;10 years&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;&lt; 1 year&#x27;</span>, value=<span class="string">&#x27;0 year&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">employmentLength_to_int</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> pd.isnull(s):</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.int8(s.split()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>] = data[<span class="string">&#x27;employmentLength&#x27;</span>].apply(employmentLength_to_int)</span><br></pre></td></tr></table></figure><h2 id="字段-earliesCreditLine-分别提取年份和月份做拼接"><a href="#字段-earliesCreditLine-分别提取年份和月份做拼接" class="headerlink" title="字段 earliesCreditLine - 分别提取年份和月份做拼接"></a>字段 earliesCreditLine - 分别提取年份和月份做拼接</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[-<span class="number">4</span>:])</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">month_re</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x == <span class="string">&#x27;Jan&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;01&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Feb&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;02&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Mar&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;03&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Apr&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;04&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;May&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;05&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jun&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;06&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jul&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;07&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Aug&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;08&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Sep&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;09&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Oct&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Nov&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;11&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;12&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>].apply(<span class="keyword">lambda</span> x: month_re(x))</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] + data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="字段-issueDate-从2017年6月1日开始；数据按照此节点统计天数"><a href="#字段-issueDate-从2017年6月1日开始；数据按照此节点统计天数" class="headerlink" title="字段 issueDate - 从2017年6月1日开始；数据按照此节点统计天数"></a>字段 issueDate - 从2017年6月1日开始；数据按照此节点统计天数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data[<span class="string">&#x27;issueDate&#x27;</span>], <span class="built_in">format</span>=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;issueDateDt&#x27;</span>] = data[<span class="string">&#x27;issueDate&#x27;</span>].apply(<span class="keyword">lambda</span> x: x - startdate).dt.days</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;issueDate&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="看一下特征的类别分布情况"><a href="#看一下特征的类别分布情况" class="headerlink" title="看一下特征的类别分布情况"></a>看一下特征的类别分布情况</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cate_features = [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> cate <span class="keyword">in</span> cate_features:</span><br><span class="line">    <span class="built_in">print</span>(cate, <span class="string">&#x27;类型数&#x27;</span>, data[cate].nunique())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">不适合做one-hot编码的是</span></span><br><span class="line"><span class="string">    employmentTitle 类型数 298101</span></span><br><span class="line"><span class="string">    postCode 类型数 935</span></span><br><span class="line"><span class="string">    title 类型数 6712</span></span><br><span class="line"><span class="string">    regionCode 类型数 51 - 大于50的先不处理了，维度还是比较高的</span></span><br><span class="line"><span class="string">    policyCode 类型数 1 - 无分析价值，可直接删除</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;policyCode&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="对于高维类别特征，进行转换，取他们同类型的数量值和排名值"><a href="#对于高维类别特征，进行转换，取他们同类型的数量值和排名值" class="headerlink" title="对于高维类别特征，进行转换，取他们同类型的数量值和排名值"></a>对于高维类别特征，进行转换，取他们同类型的数量值和排名值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;title&#x27;</span>]:</span><br><span class="line">    data[f + <span class="string">&#x27;_counts&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">    data[f + <span class="string">&#x27;_rank&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].rank(ascending=<span class="literal">False</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">del</span> data[f]</span><br><span class="line"></span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">train_lgb = data[data.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA_lgb = data[data.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="保存数据待用"><a href="#保存数据待用" class="headerlink" title="保存数据待用"></a>保存数据待用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_lgb.to_csv(<span class="string">&#x27;./data/train_data_for_lgb.csv&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line">testA_lgb.to_csv(<span class="string">&#x27;./data/testA_data_for_lgb.csv&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h1 id="xgb-主要one-hot类别特征"><a href="#xgb-主要one-hot类别特征" class="headerlink" title="xgb - 主要one-hot类别特征"></a>xgb - 主要one-hot类别特征</h1><h2 id="对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码"><a href="#对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码" class="headerlink" title="对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码"></a>对于维度大于1且不会形成高维稀疏矩阵的特征，进行one-hot编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_xgb = data</span><br><span class="line">data_xgb = pd.get_dummies(data_xgb, columns=[<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>,</span><br><span class="line">                                             <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>], drop_first=<span class="literal">True</span>)</span><br><span class="line">train_xgb = data_xgb[data_xgb.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA_xgb = data_xgb[data_xgb.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="保存数据待用-1"><a href="#保存数据待用-1" class="headerlink" title="保存数据待用"></a>保存数据待用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_xgb.to_csv(<span class="string">&#x27;./data/train_data_for_xgb.csv&#x27;</span>)</span><br><span class="line">testA_xgb.to_csv(<span class="string">&#x27;./data/testA_data_for_xgb.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="logistic-主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶"><a href="#logistic-主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶" class="headerlink" title="logistic - 主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶"></a>logistic - 主要处理特征的缺失值、异常值、连续特征的归一化及数据分桶</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_logistic = data_xgb</span><br></pre></td></tr></table></figure><h2 id="缺失值的处理"><a href="#缺失值的处理" class="headerlink" title="缺失值的处理"></a>缺失值的处理</h2><p>我没有选择删除数据（每条数据都是宝贵的），能补就补</p><h2 id="看看缺失数据"><a href="#看看缺失数据" class="headerlink" title="看看缺失数据"></a>看看缺失数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">missing = data_logistic.isnull().<span class="built_in">sum</span>()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure><p><strong>可以发现，有缺失值的特征不多，就22个；其中缺失率比较高的有16个，基本都是匿名特征且缺失率均在10%以下</strong></p><h2 id="看下匿名特征的分布情况，判断一下是离散特征还是连续特征"><a href="#看下匿名特征的分布情况，判断一下是离散特征还是连续特征" class="headerlink" title="看下匿名特征的分布情况，判断一下是离散特征还是连续特征"></a>看下匿名特征的分布情况，判断一下是离散特征还是连续特征</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_features = [<span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>, <span class="string">&#x27;n12&#x27;</span>, <span class="string">&#x27;n13&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_features:</span><br><span class="line">    <span class="built_in">print</span>(n, <span class="string">&#x27;类型数&#x27;</span>, data_logistic[n].nunique())</span><br><span class="line"></span><br><span class="line">data_logistic[[<span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>]].head(<span class="number">50</span>)</span><br><span class="line">data_logistic[[<span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>]].info()</span><br></pre></td></tr></table></figure><p><strong>去重以后值不多，看着像离散型特征</strong></p><h2 id="我先统一将这些匿名特征的缺失值单独分一类"><a href="#我先统一将这些匿名特征的缺失值单独分一类" class="headerlink" title="我先统一将这些匿名特征的缺失值单独分一类"></a>我先统一将这些匿名特征的缺失值单独分一类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">missing_features_part = [<span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n11&#x27;</span>, <span class="string">&#x27;n12&#x27;</span>, <span class="string">&#x27;n13&#x27;</span>,</span><br><span class="line">                         <span class="string">&#x27;n14&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> missfea <span class="keyword">in</span> missing_features_part:</span><br><span class="line">    data_logistic.fillna(&#123;missfea: -<span class="number">99</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">    data_logistic[missfea] = data_logistic[missfea].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; 处理完成&#x27;</span>.<span class="built_in">format</span>(missfea))</span><br></pre></td></tr></table></figure><h2 id="剩下有缺失数据的特征如下"><a href="#剩下有缺失数据的特征如下" class="headerlink" title="剩下有缺失数据的特征如下"></a>剩下有缺失数据的特征如下</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">employmentLength           58541</span></span><br><span class="line"><span class="string">dti                          300</span></span><br><span class="line"><span class="string">pubRecBankruptcies           521</span></span><br><span class="line"><span class="string">revolUtil                    658</span></span><br><span class="line"><span class="string">employmentTitle_counts         1</span></span><br><span class="line"><span class="string">postCode_counts                1</span></span><br><span class="line"><span class="string">title_counts                   1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值"><a href="#employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值" class="headerlink" title="employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值"></a>employmentLength是就业年限，和逾期应该不是强关联的特征，另外几个特征缺失很少，使用出现最多的分类变量来代替缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fea_miss = [<span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;revolUtil&#x27;</span>, <span class="string">&#x27;pubRecBankruptcies&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;employmentTitle_counts&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;postCode_counts&#x27;</span>, <span class="string">&#x27;title_counts&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="employmentLength"><a href="#employmentLength" class="headerlink" title="employmentLength"></a>employmentLength</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;10&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;employmentLength&#x27;</span>: <span class="number">10</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="revolUtil"><a href="#revolUtil" class="headerlink" title="revolUtil"></a>revolUtil</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;revolUtil&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;0.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;revolUtil&#x27;</span>: <span class="number">0.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="pubRecBankruptcies"><a href="#pubRecBankruptcies" class="headerlink" title="pubRecBankruptcies"></a>pubRecBankruptcies</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;pubRecBankruptcies&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;0.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;pubRecBankruptcies&#x27;</span>: <span class="number">0.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="dti"><a href="#dti" class="headerlink" title="dti"></a>dti</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;dti&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;16.80&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;dti&#x27;</span>: <span class="number">16.80</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="employmentTitle-counts"><a href="#employmentTitle-counts" class="headerlink" title="employmentTitle_counts"></a>employmentTitle_counts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;employmentTitle_counts&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;1.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;employmentTitle_counts&#x27;</span>: <span class="number">1.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="postCode-counts"><a href="#postCode-counts" class="headerlink" title="postCode_counts"></a>postCode_counts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;postCode_counts&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;11092.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;postCode_counts&#x27;</span>: <span class="number">11092.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="title-counts"><a href="#title-counts" class="headerlink" title="title_counts"></a>title_counts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_logistic.groupby(<span class="string">&#x27;title_counts&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].count().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;491400.00&#x27;&#x27;&#x27;</span></span><br><span class="line">data_logistic.fillna(&#123;<span class="string">&#x27;title_counts&#x27;</span>: <span class="number">491400.00</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="随机森林筛选重要特征"><a href="#随机森林筛选重要特征" class="headerlink" title="随机森林筛选重要特征"></a>随机森林筛选重要特征</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_features = <span class="built_in">list</span>(data_logistic.select_dtypes(exclude=[<span class="string">&#x27;category&#x27;</span>]).columns)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;数值型特征&#x27;&#x27;&#x27;</span></span><br><span class="line">cate_features = <span class="built_in">list</span>(data_logistic.select_dtypes(include=[<span class="string">&#x27;category&#x27;</span>]).columns)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;类别型特征&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> cate_features:</span><br><span class="line">    data_logistic[fea] = data_logistic[fea].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 分离训练集和测试集</span></span><br><span class="line">train_logistic_forest = data_logistic[data_logistic.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> train_logistic_forest.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">label = [<span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line">X_train_logistic_forest = train_logistic_forest[features]</span><br><span class="line">y_train_logistic_forest = train_logistic_forest[label]</span><br><span class="line"><span class="comment"># ## 使用随机森林训练</span></span><br><span class="line">clf_forest = RandomForestClassifier()</span><br><span class="line">clf_forest.fit(X_train_logistic_forest, y_train_logistic_forest)</span><br></pre></td></tr></table></figure><h2 id="得到特征重要性"><a href="#得到特征重要性" class="headerlink" title="得到特征重要性"></a>得到特征重要性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">forest_importances = <span class="built_in">list</span>(clf_forest.feature_importances_)</span><br><span class="line">feature_importances = [(feature, <span class="built_in">round</span>(importance, <span class="number">2</span>)) <span class="keyword">for</span> feature, importance <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">    features, forest_importances)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 重要性排序</span></span><br><span class="line">feature_importances = <span class="built_in">sorted</span>(feature_importances, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(feature_importances)</span><br></pre></td></tr></table></figure><p><strong>重要性为0的特征就不保留了，不然内存占用太大了</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">important_features = [<span class="string">&#x27;interestRate&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revolBal&#x27;</span>, <span class="string">&#x27;revolUtil&#x27;</span>, <span class="string">&#x27;earliesCreditLine_date&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;title_rank&#x27;</span>, <span class="string">&#x27;loanAmnt&#x27;</span>, <span class="string">&#x27;installment&#x27;</span>, <span class="string">&#x27;annualIncome&#x27;</span>, <span class="string">&#x27;totalAcc&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;issueDateDt&#x27;</span>, <span class="string">&#x27;employmentTitle_rank&#x27;</span>, <span class="string">&#x27;postCode_counts&#x27;</span>, <span class="string">&#x27;postCode_rank&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;regionCode_rank&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;ficoRangeLow&#x27;</span>, <span class="string">&#x27;ficoRangeHigh&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;openAcc&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;employmentTitle_counts&#x27;</span>, <span class="string">&#x27;regionCode_counts&#x27;</span>, <span class="string">&#x27;delinquency_2years&#x27;</span>, <span class="string">&#x27;pubRec&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>, <span class="string">&#x27;title_counts&#x27;</span>, <span class="string">&#x27;grade_E&#x27;</span>, <span class="string">&#x27;homeOwnership_1&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;interestRate&#x27;, 0.05), (&#x27;dti&#x27;, 0.04), (&#x27;revolBal&#x27;, 0.04), (&#x27;revolUtil&#x27;, 0.04), </span></span><br><span class="line"><span class="string">(&#x27;earliesCreditLine_date&#x27;, 0.04), (&#x27;title_rank&#x27;, 0.04), (&#x27;loanAmnt&#x27;, 0.03), (&#x27;installment&#x27;, 0.03), </span></span><br><span class="line"><span class="string">(&#x27;annualIncome&#x27;, 0.03), (&#x27;totalAcc&#x27;, 0.03), (&#x27;issueDateDt&#x27;, 0.03), (&#x27;employmentTitle_rank&#x27;, 0.03), </span></span><br><span class="line"><span class="string">(&#x27;postCode_counts&#x27;, 0.03), (&#x27;postCode_rank&#x27;, 0.03), (&#x27;regionCode_rank&#x27;, 0.03), (&#x27;term&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;employmentLength&#x27;, 0.02), (&#x27;ficoRangeLow&#x27;, 0.02), (&#x27;ficoRangeHigh&#x27;, 0.02), (&#x27;openAcc&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;n1&#x27;, 0.02), (&#x27;n4&#x27;, 0.02), (&#x27;n5&#x27;, 0.02), (&#x27;n6&#x27;, 0.02), (&#x27;n7&#x27;, 0.02), (&#x27;n8&#x27;, 0.02), (&#x27;n10&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;n14&#x27;, 0.02), (&#x27;employmentTitle_counts&#x27;, 0.02), (&#x27;regionCode_counts&#x27;, 0.02), </span></span><br><span class="line"><span class="string">(&#x27;delinquency_2years&#x27;, 0.01), (&#x27;pubRec&#x27;, 0.01), (&#x27;n0&#x27;, 0.01), (&#x27;n2&#x27;, 0.01), (&#x27;n2.1&#x27;, 0.01), </span></span><br><span class="line"><span class="string">(&#x27;n9&#x27;, 0.01), (&#x27;title_counts&#x27;, 0.01), (&#x27;grade_E&#x27;, 0.01), (&#x27;homeOwnership_1&#x27;, 0.01), </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">columns = [<span class="string">&#x27;interestRate&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revolBal&#x27;</span>, <span class="string">&#x27;revolUtil&#x27;</span>, <span class="string">&#x27;earliesCreditLine_date&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;title_rank&#x27;</span>, <span class="string">&#x27;loanAmnt&#x27;</span>, <span class="string">&#x27;installment&#x27;</span>, <span class="string">&#x27;annualIncome&#x27;</span>, <span class="string">&#x27;totalAcc&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;issueDateDt&#x27;</span>, <span class="string">&#x27;employmentTitle_rank&#x27;</span>, <span class="string">&#x27;postCode_counts&#x27;</span>, <span class="string">&#x27;postCode_rank&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;regionCode_rank&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;employmentLength&#x27;</span>, <span class="string">&#x27;ficoRangeLow&#x27;</span>, <span class="string">&#x27;ficoRangeHigh&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;openAcc&#x27;</span>, <span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>, <span class="string">&#x27;employmentTitle_counts&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;regionCode_counts&#x27;</span>, <span class="string">&#x27;delinquency_2years&#x27;</span>, <span class="string">&#x27;pubRec&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;title_counts&#x27;</span>, <span class="string">&#x27;grade_E&#x27;</span>, <span class="string">&#x27;homeOwnership_1&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 将重要性为零的特征都不保留了</span></span><br><span class="line">data_logistic = data_logistic[columns]</span><br></pre></td></tr></table></figure><h2 id="异常数据处理"><a href="#异常数据处理" class="headerlink" title="异常数据处理"></a>异常数据处理</h2><p><strong>对于异常值，这里不做处理，将数值型特征做归一化处理，降低异常值的干扰。归一化前先去对数</strong><br>为什么要取对数 - 数据集中有负数就不能取对数了 - 实践中,取对数的一般是水平量,而不是比例数据<br>1.缩小数据的绝对数值,方便计算<br>2.取对数后,可以将乘法计算转换称加法计算<br>3.对数值小的部分差异的敏感程度比数值大的部分的差异敏感程度更高<br>4.取对数之后不会改变数据的性质和相关关系,但压缩了变量的尺度<br>5.所得到的数据易消除异方差问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">min_max_scaler</span>(<span class="params">data, fea</span>):</span><br><span class="line">    data[fea] = np.log(data[fea] + <span class="number">2</span>)   <span class="comment"># 数据中有-1</span></span><br><span class="line">    data[fea] = ((data[fea] - np.<span class="built_in">min</span>(data[fea])) / (np.<span class="built_in">max</span>(data[fea]) - np.<span class="built_in">min</span>(data[fea]))) <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">min_max_columns = <span class="built_in">list</span>(data_logistic.select_dtypes(exclude=[<span class="string">&#x27;uint8&#x27;</span>]).columns)</span><br><span class="line">min_max_columns.remove(<span class="string">&#x27;isDefault&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> min_max_columns:</span><br><span class="line">    min_max_scaler(data_logistic, col)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_logistic.info()</span><br></pre></td></tr></table></figure><h2 id="两个数据集分开"><a href="#两个数据集分开" class="headerlink" title="两个数据集分开"></a>两个数据集分开</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_logistic = data_logistic[data_logistic.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA_logistic = data_logistic[data_logistic.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_logistic.to_csv(<span class="string">&#x27;./data/train_data_for_logistic.csv&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line">testA_logistic.to_csv(<span class="string">&#x27;./data/testA_data_for_logistic.csv&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="logistic"><a href="#logistic" class="headerlink" title="logistic"></a>logistic</h2><p><strong>数据准备</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_logistic = pd.read_csv(<span class="string">&#x27;./data/train_data_for_logistic.csv&#x27;</span>)</span><br><span class="line">testA_logistic = pd.read_csv(<span class="string">&#x27;./data/testA_data_for_logistic.csv&#x27;</span>)</span><br><span class="line">missing_fea = [<span class="string">&#x27;n1&#x27;</span>, <span class="string">&#x27;n4&#x27;</span>, <span class="string">&#x27;n5&#x27;</span>, <span class="string">&#x27;n6&#x27;</span>, <span class="string">&#x27;n7&#x27;</span>, <span class="string">&#x27;n8&#x27;</span>, <span class="string">&#x27;n10&#x27;</span>, <span class="string">&#x27;n14&#x27;</span>, <span class="string">&#x27;n0&#x27;</span>, <span class="string">&#x27;n2&#x27;</span>, <span class="string">&#x27;n2.1&#x27;</span>, <span class="string">&#x27;n9&#x27;</span>]</span><br><span class="line">train_logistic.info()</span><br><span class="line">train_logistic[missing_fea] = train_logistic[missing_fea].fillna(train_logistic[missing_fea].median())</span><br><span class="line">testA_logistic[missing_fea] = testA_logistic[missing_fea].fillna(testA_logistic[missing_fea].median())</span><br></pre></td></tr></table></figure><h2 id="划分训练集的特征和标签"><a href="#划分训练集的特征和标签" class="headerlink" title="划分训练集的特征和标签"></a>划分训练集的特征和标签</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> train_logistic.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">label = [<span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line">X_train_logistic = train_logistic[features]</span><br><span class="line">y_train_logistic = train_logistic[label]</span><br></pre></td></tr></table></figure><h2 id="将训练集分为5份，4份作为训练集，1份作为验证集"><a href="#将训练集分为5份，4份作为训练集，1份作为验证集" class="headerlink" title="将训练集分为5份，4份作为训练集，1份作为验证集"></a>将训练集分为5份，4份作为训练集，1份作为验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X_train_logistic_split, X_val_logistic, y_train_logistic_split, y_val_logistic = train_test_split(</span><br><span class="line">    X_train_logistic, y_train_logistic, test_size=<span class="number">0.2</span>)</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr = lr.fit(X_train_logistic_split, y_train_logistic_split)</span><br><span class="line">y_val_logistic_pre = lr.predict(X_val_logistic)</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_val_logistic, y_val_logistic_pre)</span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;未调参前逻辑回归在验证集上的AUC： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(roc_auc))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">未调参前逻辑回归在验证集上的AUC： 0.531000169605175</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">欠拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>找出相关性高的特征</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_)</span><br><span class="line">m = &#123;&#125;</span><br><span class="line">col_name = <span class="built_in">list</span>(X_train_logistic_split.columns)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(col_name)):</span><br><span class="line">    <span class="comment"># 若没有key,加入key</span></span><br><span class="line">    m.setdefault(col_name[i], <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 这里取绝对值，主要看特征的相关性</span></span><br><span class="line">    m[col_name[i]] = <span class="built_in">abs</span>(lr.coef_[<span class="number">0</span>][i])</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(m.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>大家这里可以尝试用相关性高的特征加以处理或特征构造，重新训练模型。我这里也没有调参，可以通过调参提高结果分数</strong></p><h2 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_xgb = pd.read_csv(<span class="string">&#x27;./data/train_data_for_xgb.csv&#x27;</span>)</span><br><span class="line">testA_xgb = pd.read_csv(<span class="string">&#x27;./data/testA_data_for_xgb.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">features_xgb = [f <span class="keyword">for</span> f <span class="keyword">in</span> train_xgb.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">X_train_xgb = train_xgb[features_xgb]</span><br><span class="line">y_train_xgb = train_xgb[<span class="string">&#x27;isDefault&#x27;</span>]</span><br></pre></td></tr></table></figure><h1 id="xgb-梯度提升决策树"><a href="#xgb-梯度提升决策树" class="headerlink" title="## xgb - 梯度提升决策树"></a>## xgb - 梯度提升决策树</h1><p>‘’’<br>XGBRegressor - 梯度提升回归树,也叫梯度提升机</p><pre><code>            采用连续的方式构造树,每棵树都试图纠正前一棵树的错误            与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝            从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快            gamma - 定了节点分裂所需的最小损失函数下降值,这个参数的值越大,算法越保守            subsample - 这个参数控制对于每棵树随机采样的比例,减小这个参数的值,算法会更加保守,避免过拟合            colsample_bytree - 用来控制每棵随机采样的列数的占比            learning_rate - 学习速率,用于控制树的权重,xgb模型在进行完每一轮迭代之后,会将叶子节点的分数乘上该系数,                            以便于削弱各棵树的影响,避免过拟合</code></pre><p>‘’’</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_xgb</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    model = xgb.XGBRegressor(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.08</span>, gamma=<span class="number">0</span>,</span><br><span class="line">                             subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">    model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># xgb五折交叉验证</span></span><br><span class="line">xgr = xgb.XGBClassifier(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.1</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scores_train = []   <span class="comment"># 每次模型训练训练集中子训练集的得分</span></span><br><span class="line">scores = []         <span class="comment"># 每次模型训练训练集中验证集的得分</span></span><br><span class="line"></span><br><span class="line">sk = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">0</span>)  <span class="comment"># shuffle判断是否在每次抽样时对样本进行清洗</span></span><br><span class="line"><span class="keyword">for</span> train_ind, val_ind <span class="keyword">in</span> sk.split(X_train_xgb, y_train_xgb):</span><br><span class="line">    train_x = X_train_xgb.iloc[train_ind].values</span><br><span class="line">    train_y = y_train_xgb.iloc[train_ind]</span><br><span class="line">    val_x = X_train_xgb.iloc[val_ind].values</span><br><span class="line">    val_y = y_train_xgb.iloc[val_ind]</span><br><span class="line"></span><br><span class="line">    xgr.fit(train_x, train_y)</span><br><span class="line">    pred_train_xgb = xgr.predict(train_x)   <span class="comment"># 子训练集的预测值</span></span><br><span class="line">    pre_xgb = xgr.predict(val_x)            <span class="comment"># 验证集的预测值</span></span><br><span class="line"></span><br><span class="line">    scores_train.append(roc_auc_score(train_y, pred_train_xgb))</span><br><span class="line">    scores.append(roc_auc_score(val_y, pre_xgb))                    <span class="comment"># 统计验证集的mae</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train mae:&#x27;</span>, np.mean(scores_train))  <span class="comment"># 统计mae均值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val mae:&#x27;</span>, np.mean(scores))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Train mae: 0.5548858423493594</span></span><br><span class="line"><span class="string">Val mae: 0.5458927786959327</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>结果也不是很好，同样可以通过特征重新筛选和调参来提高</strong></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>至于lgb部分，可以参考Task4的代码，思路就是使用lgb,xgb,lr同时找到合适的特征并加以调参。训练完后三个模型可以使用文初提到的链接使用stacking、blending、加权融合或者投票（硬投票、软投票等方法）尝试模型融合。</p><p>在task5之后，我自己还会做一个task6，尝试完整的完成预测，再到线上提交加以迭代。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h1&gt;&lt;p&gt;模型融合是比赛后期上分的重要手段，特别是多人组队学习的比赛中，将不同队友的模型进行融合，可能会收获意想不到的效果哦，往往模型相差越大且模型表现都不错</summary>
      
    
    
    
    <category term="金融风控" scheme="http://example.com/categories/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>机器学习训练_金融风控_Task4_建模调参</title>
    <link href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task4_%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/"/>
    <id>http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task4_%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/</id>
    <published>2023-04-24T08:04:20.032Z</published>
    <updated>2023-04-24T08:18:21.858Z</updated>
    
    <content type="html"><![CDATA[<h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><p>特征工程之后，我们基本了解了数据集的概貌，通过缺失值处理、异常值处理、归一化、独热编码、特征构造等一系列方法对数据进行了预处理，并根据不同模型的数据要求对数据进行了一定的转化，从而进行下一步模型的学习过程。以下就是对数据进行处理后，训练模型的过程代码。其实可以先使用随机森林等方法先做一步特征筛选的工作，我这里没有做特征的筛选，而且先复现了数据准备，模型构造和调参的过程。若是模型初步表现不错且较稳定，我会后续做特征筛选或特征构造，进一步提高模型的分数。</p><h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><h2 id="导入第三方库"><a href="#导入第三方库" class="headerlink" title="导入第三方库"></a>导入第三方库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, train_test_split, cross_val_score, GridSearchCV, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sns 相关设置</span></span><br><span class="line"><span class="string">@return:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># 声明使用 Seaborn 样式</span></span><br><span class="line"><span class="string">sns.set()</span></span><br><span class="line"><span class="string"># 有五种seaborn的绘图风格，它们分别是：darkgrid, whitegrid, dark, white, ticks。默认的主题是darkgrid。</span></span><br><span class="line"><span class="string">sns.set_style(&quot;whitegrid&quot;)</span></span><br><span class="line"><span class="string"># 有四个预置的环境，按大小从小到大排列分别为：paper, notebook, talk, poster。其中，notebook是默认的。</span></span><br><span class="line"><span class="string">sns.set_context(&#x27;talk&#x27;)</span></span><br><span class="line"><span class="string"># 中文字体设置-黑体</span></span><br><span class="line"><span class="string">plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]</span></span><br><span class="line"><span class="string"># 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line"><span class="string">plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False</span></span><br><span class="line"><span class="string"># 解决Seaborn中文显示问题并调整字体大小</span></span><br><span class="line"><span class="string">sns.set(font=&#x27;SimHei&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns = <span class="literal">None</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, <span class="keyword">lambda</span> x: <span class="string">&#x27;%.2f&#x27;</span> % x)</span><br></pre></td></tr></table></figure><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\train.csv&#x27;</span>)</span><br><span class="line">testA = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\testA.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="压缩数据"><a href="#压缩数据" class="headerlink" title="压缩数据"></a>压缩数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>  <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:  <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:  <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>    <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))  <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = reduce_mem_usage(train)</span><br><span class="line">testA = reduce_mem_usage(testA)</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.2&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.3&#x27;</span>]</span><br></pre></td></tr></table></figure><h1 id="简单建模"><a href="#简单建模" class="headerlink" title="简单建模"></a>简单建模</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Tips1：金融风控的实际项目多涉及到信用评分，因此需要模型特征具有较好的可解释性，所以目前在实际项目中多还是以逻辑回归作为基础模型。</span></span><br><span class="line"><span class="string">        但是在比赛中以得分高低为准，不需要严谨的可解释性，所以大多基于集成算法进行建模。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Tips2：因为逻辑回归的算法特性，需要提前对异常值、缺失值数据进行处理(参考task3部分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Tips3：基于树模型的算法特性，异常值、缺失值处理可以跳过，但是对于业务较为了解的同学也可以自己对缺失异常值进行处理，效果可能会更优于模型处理的结果。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">注：以下建模的源数据参考baseline进行了相应的特征工程，对于异常缺失值未进行相应的处理操作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="建模之前的数据处理"><a href="#建模之前的数据处理" class="headerlink" title="建模之前的数据处理"></a>建模之前的数据处理</h2><p>为了方便起见，把训练集和测试集合并处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = pd.concat([train, testA], axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>category特征不能直接训练，需要处理转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;grade&#x27;, &#x27;subGrade&#x27;, &#x27;employmentLength&#x27;, &#x27;issueDate&#x27;, &#x27;earliesCreditLine&#x27;]</span></span><br><span class="line"><span class="string">先处理&#x27;employmentLength&#x27;, &#x27;issueDate&#x27;, &#x27;earliesCreditLine&#x27;这三个特征；&#x27;grade&#x27;和&#x27;subGrade&#x27;做one-hot编码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="‘employmentLength’-转换为数值"><a href="#‘employmentLength’-转换为数值" class="headerlink" title="‘employmentLength’ - 转换为数值"></a>‘employmentLength’ - 转换为数值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;10年以上算10年，1年一下算0年&#x27;&#x27;&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;10+ years&#x27;</span>, value=<span class="string">&#x27;10 years&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;&lt; 1 year&#x27;</span>, value=<span class="string">&#x27;0 year&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">employmentLength_to_int</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> pd.isnull(s):</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.int8(s.split()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>] = data[<span class="string">&#x27;employmentLength&#x27;</span>].apply(employmentLength_to_int)</span><br></pre></td></tr></table></figure><h3 id="‘earliesCreditLine’-分别提取年份和月份做拼接"><a href="#‘earliesCreditLine’-分别提取年份和月份做拼接" class="headerlink" title="‘earliesCreditLine’ - 分别提取年份和月份做拼接"></a>‘earliesCreditLine’ - 分别提取年份和月份做拼接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[-<span class="number">4</span>:])</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">month_re</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x == <span class="string">&#x27;Jan&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;01&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Feb&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;02&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Mar&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;03&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Apr&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;04&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;May&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;05&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jun&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;06&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jul&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;07&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Aug&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;08&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Sep&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;09&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Oct&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Nov&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;11&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;12&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>].apply(<span class="keyword">lambda</span> x: month_re(x))</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] + data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="‘issueDate’-从数据可以看出，issueDate从2017年6月1日开始；数据按照此节点统计天数"><a href="#‘issueDate’-从数据可以看出，issueDate从2017年6月1日开始；数据按照此节点统计天数" class="headerlink" title="‘issueDate’ - 从数据可以看出，issueDate从2017年6月1日开始；数据按照此节点统计天数"></a>‘issueDate’ - 从数据可以看出，issueDate从2017年6月1日开始；数据按照此节点统计天数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data[<span class="string">&#x27;issueDate&#x27;</span>], <span class="built_in">format</span>=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;issueDateDt&#x27;</span>] = data[<span class="string">&#x27;issueDate&#x27;</span>].apply(<span class="keyword">lambda</span> x: x - startdate).dt.days</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;issueDate&#x27;</span>]</span><br></pre></td></tr></table></figure><p>除了本身是category类型的特征，还有一些数值特征表现出的也是类别型的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;将1类以上且非高维稀疏的特征进行one-hot编码&#x27;&#x27;&#x27;</span></span><br><span class="line">cate_features = [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> cate <span class="keyword">in</span> cate_features:</span><br><span class="line">    <span class="built_in">print</span>(cate, <span class="string">&#x27;类型数&#x27;</span>, data[cate].nunique())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">grade 类型数 7</span></span><br><span class="line"><span class="string">subGrade 类型数 35</span></span><br><span class="line"><span class="string">employmentTitle 类型数 298101</span></span><br><span class="line"><span class="string">homeOwnership 类型数 6</span></span><br><span class="line"><span class="string">verificationStatus 类型数 3</span></span><br><span class="line"><span class="string">purpose 类型数 14</span></span><br><span class="line"><span class="string">postCode 类型数 935</span></span><br><span class="line"><span class="string">regionCode 类型数 51</span></span><br><span class="line"><span class="string">applicationType 类型数 2</span></span><br><span class="line"><span class="string">initialListStatus 类型数 2</span></span><br><span class="line"><span class="string">title 类型数 6712</span></span><br><span class="line"><span class="string">policyCode 类型数 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">不适合做one-hot编码的是</span></span><br><span class="line"><span class="string">    employmentTitle 类型数 298101</span></span><br><span class="line"><span class="string">    postCode 类型数 935</span></span><br><span class="line"><span class="string">    title 类型数 6712</span></span><br><span class="line"><span class="string">    regionCode 类型数 51 - 大于50的先不处理了，维度还是比较高的</span></span><br><span class="line"><span class="string">    policyCode 类型数 1 - 无分析价值，可直接删除</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;policyCode&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.get_dummies(data, columns=[<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>,</span><br><span class="line">                                     <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>], drop_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>对于高维类别特征，进行转换，取他们同类型的数量值和排名值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;title&#x27;</span>]:</span><br><span class="line">    data[f + <span class="string">&#x27;_counts&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">    data[f + <span class="string">&#x27;_rank&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].rank(ascending=<span class="literal">False</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">del</span> data[f]</span><br></pre></td></tr></table></figure><h2 id="准备训练集和测试集"><a href="#准备训练集和测试集" class="headerlink" title="准备训练集和测试集"></a>准备训练集和测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">train = data[data.isDefault.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">testA = data[data.isDefault.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">x_train = train[features]</span><br><span class="line">y_train = train[<span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line">x_testA = testA[features]</span><br></pre></td></tr></table></figure><h2 id="五折交叉验证准备"><a href="#五折交叉验证准备" class="headerlink" title="五折交叉验证准备"></a>五折交叉验证准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">folds = <span class="number">5</span></span><br><span class="line">seed = <span class="number">2020</span></span><br><span class="line">kf = KFold(n_splits=folds, shuffle=<span class="literal">True</span>, random_state=seed)</span><br></pre></td></tr></table></figure><h1 id="建模-Lightgbm"><a href="#建模-Lightgbm" class="headerlink" title="建模 - Lightgbm"></a>建模 - Lightgbm</h1><h2 id="将训练集分为5份，4份作为训练集，1份作为验证集"><a href="#将训练集分为5份，4份作为训练集，1份作为验证集" class="headerlink" title="将训练集分为5份，4份作为训练集，1份作为验证集"></a>将训练集分为5份，4份作为训练集，1份作为验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_split, X_val, y_train_split, y_val = train_test_split(x_train, y_train, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>将数据集转化成能用于lgb训练的形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_split_matrix = lgb.Dataset(X_train_split, label=y_train_split)</span><br><span class="line">val_matrix = lgb.Dataset(X_val, label=y_val)</span><br></pre></td></tr></table></figure><p>初步自定义lgb参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>, <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>, <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1e-3</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">31</span>, <span class="string">&#x27;max_depth&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">2020</span>, <span class="string">&#x27;nthread&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将训练集丢入lgb模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = lgb.train(params, train_set=train_split_matrix, valid_sets=val_matrix, num_boost_round=<span class="number">20000</span>,</span><br><span class="line">                  verbose_eval=<span class="number">1000</span>, early_stopping_rounds=<span class="number">200</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training until validation scores don&#x27;t improve for 200 rounds</span></span><br><span class="line"><span class="string">Early stopping, best iteration is:</span></span><br><span class="line"><span class="string">[419]valid_0&#x27;s auc: 0.735017</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>用训练好的模型预测验证集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)</span><br></pre></td></tr></table></figure><p>计算roc的相关指标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">真正类率(True Positive Rate)TPR: TP / (TP + FN),代表分类器预测的正类中实际正实例占所有正实例的比例</span></span><br><span class="line"><span class="string">纵轴TPR：TPR越大，预测正类中实际正类越多</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">负正类率(False Positive Rate)FPR: FP / (FP + TN)，代表分类器预测的正类中实际负实例占所有负实例的比例</span></span><br><span class="line"><span class="string">横轴FPR:FPR越大，预测正类中实际负类越多</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">理想目标：TPR=1，FPR=0,即roc图中的(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_val, val_pre_lgb)</span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;未调参前lgb在验证集上的AUC： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(roc_auc))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;未调参前lgb在验证集上的AUC： 0.7350165705811689&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>画出roc曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Val ROC&#x27;</span>)</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Val AUC = %0.4f&#x27;</span> % roc_auc)  <span class="comment"># 保留四位小数</span></span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;r--&#x27;</span>)     <span class="comment"># 对角线</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200924142454966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>使用5折交叉验证进行模型性能评估</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">cv_scores = []  <span class="comment"># 用于存放每次验证的得分</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 五折交叉验证评估模型</span></span><br><span class="line"><span class="keyword">for</span> i, (train_index, val_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(x_train, y_train)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*** &#123;&#125; ***&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(i+<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    X_train_split, y_train_split, X_val, y_val = x_train.iloc[train_index], y_train[train_index], \</span><br><span class="line">                                                 x_train.iloc[val_index], y_train[val_index]</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;划分训练集和验证集&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)</span><br><span class="line">    val_matrix = lgb.Dataset(X_val, label=y_val)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;转换成lgb训练的数据形式&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>, <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>, <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1e-3</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">31</span>, <span class="string">&#x27;max_depth&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;seed&#x27;</span>: <span class="number">2020</span>, <span class="string">&#x27;nthread&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;给定lgb参数&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    model = lgb.train(params, train_set=train_matrix, num_boost_round=<span class="number">20000</span>, valid_sets=val_matrix, verbose_eval=<span class="number">1000</span>,</span><br><span class="line">                      early_stopping_rounds=<span class="number">200</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;训练模型&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;预测验证集结果&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    cv_scores.append(roc_auc_score(y_val, val_pre_lgb))</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;将auc加入列表&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(cv_scores)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">*** 1 ***</span></span><br><span class="line"><span class="string">Training until validation scores don&#x27;t improve for 200 rounds</span></span><br><span class="line"><span class="string">Early stopping, best iteration is:</span></span><br><span class="line"><span class="string">[480]valid_0&#x27;s auc: 0.735706</span></span><br><span class="line"><span class="string">[0.7357056028032594]</span></span><br><span class="line"><span class="string">*** 2 ***</span></span><br><span class="line"><span class="string">Training until validation scores don&#x27;t improve for 200 rounds</span></span><br><span class="line"><span class="string">Early stopping, best iteration is:</span></span><br><span class="line"><span class="string">[394]valid_0&#x27;s auc: 0.732804</span></span><br><span class="line"><span class="string">[0.7357056028032594, 0.7328044319912592]</span></span><br><span class="line"><span class="string">*** 3 ***</span></span><br><span class="line"><span class="string">Training until validation scores don&#x27;t improve for 200 rounds</span></span><br><span class="line"><span class="string">Early stopping, best iteration is:</span></span><br><span class="line"><span class="string">[469]valid_0&#x27;s auc: 0.736296</span></span><br><span class="line"><span class="string">[0.7357056028032594, 0.7328044319912592, 0.736295686606251]</span></span><br><span class="line"><span class="string">*** 4 ***</span></span><br><span class="line"><span class="string">Training until validation scores don&#x27;t improve for 200 rounds</span></span><br><span class="line"><span class="string">Early stopping, best iteration is:</span></span><br><span class="line"><span class="string">[480]valid_0&#x27;s auc: 0.735153</span></span><br><span class="line"><span class="string">[0.7357056028032594, 0.7328044319912592, 0.736295686606251, 0.7351530881059898]</span></span><br><span class="line"><span class="string">*** 5 ***</span></span><br><span class="line"><span class="string">Training until validation scores don&#x27;t improve for 200 rounds</span></span><br><span class="line"><span class="string">Early stopping, best iteration is:</span></span><br><span class="line"><span class="string">[481]valid_0&#x27;s auc: 0.734523</span></span><br><span class="line"><span class="string">[0.7357056028032594, 0.7328044319912592, 0.736295686606251, 0.7351530881059898, 0.7345226943030314]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h1><h2 id="贪心调参"><a href="#贪心调参" class="headerlink" title="贪心调参"></a>贪心调参</h2><p>先使用当前对模型影响最大的参数进行调优，达到当前参数下的模型最优化，再使用对模型影响次之的参数进行调优，如此下去，直到所有的参数调整完毕。<br>这个方法的缺点就是可能会调到局部最优而不是全局最优，但是只需要一步一步的进行参数最优化调试即可，容易理解。<br>需要注意的是在树模型中参数调整的顺序，也就是各个参数对模型的影响程度，列举一下日常调参过程中常用的参数和调参顺序:<br>    max_depth、num_leaves<br>    min_data_in_leaf、min_child_weight<br>    bagging_fraction、 feature_fraction、bagging_freq<br>    reg_lambda、reg_alpha<br>    min_split_gain</p><h3 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">best_obj = <span class="built_in">dict</span>()</span><br><span class="line">objective = [<span class="string">&#x27;regression&#x27;</span>, <span class="string">&#x27;regression_l2&#x27;</span>, <span class="string">&#x27;regression_l1&#x27;</span>, <span class="string">&#x27;huber&#x27;</span>, <span class="string">&#x27;fair&#x27;</span>, <span class="string">&#x27;poisson&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;binary&#x27;</span>, <span class="string">&#x27;lambdarank&#x27;</span>, <span class="string">&#x27;multiclass&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=obj)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;针对每种学习目标参数，分别把5次交叉验证的结果取平均值放入字典&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;regression&#x27;: 0.7317571771311902, &#x27;regression_l2&#x27;: 0.7317571771311902, &#x27;regression_l1&#x27;: 0.5254673662915372, </span></span><br><span class="line"><span class="string">&#x27;huber&#x27;: 0.7317930010205694, &#x27;fair&#x27;: 0.7299013530452948, &#x27;poisson&#x27;: 0.7276315321558192, </span></span><br><span class="line"><span class="string">&#x27;binary&#x27;: 0.7325703837580402, &#x27;lambdarank&#x27;: nan, &#x27;multiclass&#x27;: nan&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">分数最高的objective是&#x27;binary&#x27;: 0.7325703837580402</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="max-depth"><a href="#max-depth" class="headerlink" title="max_depth"></a>max_depth</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">best_depth = <span class="built_in">dict</span>()</span><br><span class="line">max_depth = [<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>]</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=depth)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_depth[depth] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;4: 0.7289917272476384, 6: 0.7318582290988798, 8: 0.7326689825432566, </span></span><br><span class="line"><span class="string">10: 0.7327216337284277, 12: 0.7326861296973519&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">分数最高的depth是 10: 0.7327216337284277</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="num-leaves-为了防止过拟合，num-leaves要小于2-max-depth-2-10-x3D-1024"><a href="#num-leaves-为了防止过拟合，num-leaves要小于2-max-depth-2-10-x3D-1024" class="headerlink" title="num_leaves - 为了防止过拟合，num_leaves要小于2^max_depth(2^10&#x3D;1024)"></a>num_leaves - 为了防止过拟合，num_leaves要小于2^max_depth(2^10&#x3D;1024)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">best_leaves = <span class="built_in">dict</span>()</span><br><span class="line">num_leaves = [<span class="number">60</span>, <span class="number">80</span>, <span class="number">100</span>, <span class="number">120</span>, <span class="number">140</span>, <span class="number">160</span>, <span class="number">180</span>, <span class="number">200</span>]</span><br><span class="line"><span class="keyword">for</span> leaf <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=leaf)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_leaves[leaf] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;60: 0.7338063124202595, 80: 0.7340086888735147, 100: 0.7340517113255459, 120: 0.7339504337283304, </span></span><br><span class="line"><span class="string">140: 0.733943621732856, 160: 0.7340382165600425, 180: 0.7335684540056998, 200: 0.7331373764276772&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">分数最高的num_leaves是 num_leaves 100: 0.7340517113255459</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="min-data-in-leaf"><a href="#min-data-in-leaf" class="headerlink" title="min_data_in_leaf"></a>min_data_in_leaf</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">best_min_leaves = <span class="built_in">dict</span>()</span><br><span class="line">min_data_in_leaf = [<span class="number">14</span>, <span class="number">18</span>, <span class="number">22</span>, <span class="number">26</span>, <span class="number">30</span>, <span class="number">34</span>]</span><br><span class="line"><span class="keyword">for</span> min_leaf <span class="keyword">in</span> min_data_in_leaf:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=min_leaf)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_min_leaves[min_leaf] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;14: 0.7338644336034048, 18: 0.7340150561766138, 22: 0.7340158598138881, 26: 0.7341871752335695, </span></span><br><span class="line"><span class="string">30: 0.7340615684229571, 34: 0.7340519101378781&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">分数最高的 min_leaf是 26: 0.7341871752335695</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="min-child-weight"><a href="#min-child-weight" class="headerlink" title="min_child_weight"></a>min_child_weight</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">best_weight = <span class="built_in">dict</span>()</span><br><span class="line">min_child_weight = [<span class="number">0.002</span>, <span class="number">0.004</span>, <span class="number">0.006</span>, <span class="number">0.008</span>, <span class="number">0.010</span>, <span class="number">0.012</span>]</span><br><span class="line"><span class="keyword">for</span> min_weight <span class="keyword">in</span> min_child_weight:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=<span class="number">26</span>,</span><br><span class="line">                              min_child_weight=min_weight)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_weight[min_weight] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0.002: 0.7341871752335695, 0.004: 0.7341871752335695, 0.006: 0.7341871752335695, 0.008: 0.7341871752335695, </span></span><br><span class="line"><span class="string">0.01: 0.7341871752335695, 0.012: 0.7341871752335695&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">都一样，说明min_data_in_leaf和min_child_weight应该是对应的？</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="bagging-fraction-bagging-freq"><a href="#bagging-fraction-bagging-freq" class="headerlink" title="bagging_fraction + bagging_freq"></a>bagging_fraction + bagging_freq</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">bagging_fraction+bagging_freq参数必须同时设置，bagging_fraction相当于subsample样本采样，可以使bagging更快的运行，同时也可以降拟合。</span></span><br><span class="line"><span class="string">bagging_freq默认0，表示bagging的频率，0意味着没有使用bagging，k意味着每k轮迭代进行一次bagging</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">best_bagging_fraction = <span class="built_in">dict</span>()</span><br><span class="line">bagging_fraction = [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.95</span>]</span><br><span class="line"><span class="keyword">for</span> bagging <span class="keyword">in</span> bagging_fraction:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=<span class="number">26</span>,</span><br><span class="line">                              bagging_fraction=bagging)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_bagging_fraction[bagging] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0.5: 0.7341871752335695, 0.6: 0.734187175233</span></span><br><span class="line"><span class="string">5695, 0.7: 0.7341871752335695, 0.8: 0.7341871752335695, 0.9: 0.7341871752335695, 0.95: 0.7341871752335695&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">没变化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="feature-fraction"><a href="#feature-fraction" class="headerlink" title="feature_fraction"></a>feature_fraction</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">best_feature_fraction = <span class="built_in">dict</span>()</span><br><span class="line">feature_fraction = [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.95</span>]</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> feature_fraction:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=<span class="number">26</span>,</span><br><span class="line">                              feature_fraction=feature)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_feature_fraction[feature] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0.5: 0.7341332691040499, 0.6: 0.7342461204659492, 0.7: 0.7340950793860553, 0.8: 0.734168394330798, </span></span><br><span class="line"><span class="string">0.9: 0.7342417001187209, 0.95: 0.7340419425131396&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">虽然0.6会高一些，但是出于样本特征的使用率我还是想用0.9</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="reg-lambda"><a href="#reg-lambda" class="headerlink" title="reg_lambda"></a>reg_lambda</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">best_reg_lambda = <span class="built_in">dict</span>()</span><br><span class="line">reg_lambda = [<span class="number">0</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.08</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> reg_lambda:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=<span class="number">26</span>,</span><br><span class="line">                              feature_fraction=<span class="number">0.9</span>, reg_lambda=lam)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_reg_lambda[lam] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0: 0.7342417001187209, 0.001: 0.7340521878374329, 0.01: 0.7342087379791171, </span></span><br><span class="line"><span class="string">0.03: 0.7342072587501143, 0.08: 0.7341178131960189, 0.3: 0.7342923823693306, 0.5: 0.7342815855243002&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">reg_lambda 最优值为 0.3: 0.7342923823693306</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="reg-alpha"><a href="#reg-alpha" class="headerlink" title="reg_alpha"></a>reg_alpha</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">best_reg_alpha = <span class="built_in">dict</span>()</span><br><span class="line">reg_alpha = [<span class="number">0</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.08</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]</span><br><span class="line"><span class="keyword">for</span> alp <span class="keyword">in</span> reg_alpha:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=<span class="number">26</span>,</span><br><span class="line">                              feature_fraction=<span class="number">0.9</span>, reg_lambda=<span class="number">0.3</span>, reg_alpha=alp)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_reg_alpha[alp] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0: 0.7342923823693306, 0.001: 0.7342141300723407, 0.01: 0.7342716599336013, 0.03: 0.7342356374031566, </span></span><br><span class="line"><span class="string">0.08: 0.7342509380457417, 0.3: 0.7341836259662214, 0.5: 0.7342654379571296&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">reg_alpha 为0时最高</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="learning-rate"><a href="#learning-rate" class="headerlink" title="learning_rate"></a>learning_rate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">best_learning_rate = <span class="built_in">dict</span>()</span><br><span class="line">learning_rate = [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.08</span>, <span class="number">0.1</span>, <span class="number">0.12</span>]</span><br><span class="line"><span class="keyword">for</span> learn <span class="keyword">in</span> learning_rate:</span><br><span class="line">    model = lgb.LGBMRegressor(objective=<span class="string">&#x27;binary&#x27;</span>, max_depth=<span class="number">10</span>, num_leaves=<span class="number">100</span>, min_data_in_leaf=<span class="number">26</span>,</span><br><span class="line">                              feature_fraction=<span class="number">0.9</span>, reg_lambda=<span class="number">0.3</span>, learning_rate=learn)</span><br><span class="line">    score = cross_val_score(model, x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span>).mean()</span><br><span class="line">    best_learning_rate[learn] = score</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0.01: 0.719100817422237, 0.05: 0.7315198412233572, 0.08: 0.733713956417723, 0.1: 0.7342923823693306, </span></span><br><span class="line"><span class="string">0.12: 0.7341688215024998&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">learning_rate 为0.1时最好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="网格调参"><a href="#网格调参" class="headerlink" title="网格调参"></a>网格调参</h2><p>网格调参+五折交叉验证非常非常非常耗时，建议开始步长选大一点，我步长较小，导致调参耗时非常可怕</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sklearn 提供GridSearchCV用于进行网格搜索，只需要把模型的参数输进去，就能给出最优化的结果和参数。</span></span><br><span class="line"><span class="string">相比起贪心调参，网格搜索的结果会更优，但是网格搜索只适合于小数据集，一旦数据的量级上去了，很难得出结果</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_best_cv_params</span>(<span class="params">learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">800</span>, num_leaves=<span class="number">100</span>, max_depth=<span class="number">10</span>, feature_fraction=<span class="number">0.9</span>,</span></span><br><span class="line"><span class="params">                       min_data_in_leaf=<span class="number">26</span>, reg_lambda=<span class="number">0.3</span>, reg_alpha=<span class="number">0</span>, objective=<span class="string">&#x27;binary&#x27;</span>, param_grid=<span class="literal">None</span></span>):</span><br><span class="line">    cv_fold = StratifiedKFold(n_splits=<span class="number">5</span>, random_state=<span class="number">2020</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;设置五折交叉验证&#x27;&#x27;&#x27;</span></span><br><span class="line">    model_lgb = lgb.LGBMRegressor(learning_rate=learning_rate, n_estimators=n_estimators, num_leaves=num_leaves,</span><br><span class="line">                                  max_depth=max_depth, feature_fraction=feature_fraction,</span><br><span class="line">                                  min_data_in_leaf=min_data_in_leaf, reg_lambda=reg_lambda, reg_alpha=reg_alpha,</span><br><span class="line">                                  objective=objective, n_jobs=-<span class="number">1</span>)</span><br><span class="line">    grid_search = GridSearchCV(estimator=model_lgb, cv=cv_fold, param_grid=param_grid, scoring=<span class="string">&#x27;roc_auc&#x27;</span>)</span><br><span class="line">    grid_search.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优参数为： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优得分为： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_score_))</span><br></pre></td></tr></table></figure><h3 id="先调-max-depth和-num-leaves"><a href="#先调-max-depth和-num-leaves" class="headerlink" title="先调 max_depth和 num_leaves"></a>先调 max_depth和 num_leaves</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">80</span>, <span class="number">120</span>, <span class="number">5</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">6</span>, <span class="number">14</span>, <span class="number">2</span>)&#125;</span><br><span class="line">get_best_cv_params(param_grid=lgb_params)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">模型当前最优参数为： &#123;&#x27;max_depth&#x27;: 6, &#x27;num_leaves&#x27;: 80&#125;</span></span><br><span class="line"><span class="string">模型当前最优得分为： 0.7349883936428184</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="min-data-in-leaf和min-child-weight"><a href="#min-data-in-leaf和min-child-weight" class="headerlink" title="min_data_in_leaf和min_child_weight"></a>min_data_in_leaf和min_child_weight</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="built_in">range</span>(<span class="number">20</span>, <span class="number">60</span>, <span class="number">5</span>)&#125;</span><br><span class="line">get_best_cv_params(param_grid=lgb_params, max_depth=<span class="number">6</span>, num_leaves=<span class="number">80</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">模型当前最优参数为： &#123;&#x27;min_data_in_leaf&#x27;: 45&#125;</span></span><br><span class="line"><span class="string">模型当前最优得分为： 0.7352238437118113</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="feature-fraction-1"><a href="#feature-fraction-1" class="headerlink" title="feature_fraction"></a>feature_fraction</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;feature_fraction&#x27;</span>: [i / <span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>, <span class="number">10</span>, <span class="number">1</span>)]&#125;</span><br><span class="line">get_best_cv_params(param_grid=lgb_params, max_depth=<span class="number">6</span>, num_leaves=<span class="number">80</span>, min_data_in_leaf=<span class="number">45</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">模型当前最优参数为： &#123;&#x27;feature_fraction&#x27;: 0.5&#125;</span></span><br><span class="line"><span class="string">模型当前最优得分为： 0.7357516064800039</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="reg-lambda-和-reg-alpha"><a href="#reg-lambda-和-reg-alpha" class="headerlink" title="reg_lambda 和 reg_alpha"></a>reg_lambda 和 reg_alpha</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;reg_alpha&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>], <span class="string">&#x27;reg_lambda&#x27;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">get_best_cv_params(param_grid=lgb_params, max_depth=<span class="number">6</span>, num_leaves=<span class="number">80</span>, min_data_in_leaf=<span class="number">45</span>, feature_fraction=<span class="number">0.5</span>, )</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">模型当前最优参数为： &#123;&#x27;reg_alpha&#x27;: 0.5, &#x27;reg_lambda&#x27;: 0.4&#125;</span></span><br><span class="line"><span class="string">模型当前最优得分为： 0.7358840540809432</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>总之，看似每一个参数的选择非常简短快速，实际调参过程非常漫长，建议增大步长缩小范围以后再精细调参。</p><h1 id="贝叶斯调参"><a href="#贝叶斯调参" class="headerlink" title="贝叶斯调参"></a>贝叶斯调参</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">贝叶斯优化是一种用模型找到函数最小值方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">贝叶斯方法与随机或网格搜索的不同之处在于:它在尝试下一组超参数时,会参考之前的评估结果,因此可以省去很多无用功</span></span><br><span class="line"><span class="string">贝叶斯调参法使用不断更新的概率模型,通过推断过去的结果来&#x27;集中&#x27;有希望的超参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">贝叶斯优化问题的四个部分</span></span><br><span class="line"><span class="string">            1.目标函数 - 机器学习模型使用该组超参数在验证集上的损失</span></span><br><span class="line"><span class="string">                        它的输入为一组超参数,输出需要最小化的值(交叉验证损失)</span></span><br><span class="line"><span class="string">            2.域空间 - 要搜索的超参数的取值范围</span></span><br><span class="line"><span class="string">                        在搜索的每次迭代中,贝叶斯优化算法将从域空间为每个超参数选择一个值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                        当我们进行随机或网格搜索时,域空间是一个网格</span></span><br><span class="line"><span class="string">                        而在贝叶斯优化中,不是按照顺序()网格)或者随机选择一个超参数,而是按照每个超参数的概率分布选择</span></span><br><span class="line"><span class="string">            3.优化算法 - 构造替代函数并选择下一个超参数值进行评估的方法</span></span><br><span class="line"><span class="string">            4.来自目标函数评估的存储结果,包括超参数和验证集上的损失</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>定义目标函数，我们要这个目标函数输出的值最大</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rf_cv_lgb</span>(<span class="params">num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf,</span></span><br><span class="line"><span class="params">              min_child_weight, min_split_gain, reg_lambda, reg_alpha</span>):</span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        lgb.LGBMRegressor(</span><br><span class="line">            boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, objective=<span class="string">&#x27;binary&#x27;</span>, metrics=<span class="string">&#x27;auc&#x27;</span>, learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">5000</span>,</span><br><span class="line">            num_leaves=<span class="built_in">int</span>(num_leaves), max_depth=<span class="built_in">int</span>(max_depth), bagging_fraction=<span class="built_in">round</span>(bagging_fraction, <span class="number">2</span>),</span><br><span class="line">            feature_fraction=<span class="built_in">round</span>(feature_fraction, <span class="number">2</span>), bagging_freq=<span class="built_in">int</span>(bagging_freq),</span><br><span class="line">            min_data_in_leaf=<span class="built_in">int</span>(min_data_in_leaf), min_child_weight=min_child_weight,</span><br><span class="line">            min_split_gain=min_split_gain, reg_lambda=reg_lambda, reg_alpha=reg_alpha, n_jobs=-<span class="number">1</span></span><br><span class="line">        ), x_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;roc_auc&#x27;</span></span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure><p>定义优化参数（域空间）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">    rf_cv_lgb,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: (<span class="number">10</span>, <span class="number">200</span>),</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: (<span class="number">3</span>, <span class="number">20</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: (<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: (<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: (<span class="number">0</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: (<span class="number">10</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_child_weight&#x27;</span>: (<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;min_split_gain&#x27;</span>: (<span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;reg_alpha&#x27;</span>: (<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>: (<span class="number">0.0</span>, <span class="number">10</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>开始优化，这里我会有15次迭代后的得分，我取了最高的一次贴上来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rf_bo.maximize(n_iter=<span class="number">10</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">|   iter    |  target   | baggin... | baggin... | featur... | max_depth | min_ch... | min_da... | min_sp... | num_le...</span></span><br><span class="line"><span class="string"> | reg_alpha | reg_la... |</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">|  14       |  0.7367   |  0.8748   |  21</span></span><br><span class="line"><span class="string">.07    |  0.9624   |  4.754    |  0.3129   |  21.14    |  0.4187   |  178.2   </span></span><br><span class="line"><span class="string"> |  9.991    |  9.528    |</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>根据优化后的参数建立新的模型，降低学习率并寻找最优模型迭代次数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;调整一个较小的学习率，并通过cv函数确定当前最优的迭代次数&#x27;&#x27;&#x27;</span></span><br><span class="line">base_params_lgb = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">178</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">0.31</span>,</span><br><span class="line">    <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.88</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.96</span>,</span><br><span class="line">    <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">9.5</span>,</span><br><span class="line">    <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0.42</span>,</span><br><span class="line">    <span class="string">&#x27;nthread&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&#x27;seed&#x27;</span>: <span class="number">2020</span>,</span><br><span class="line">    <span class="string">&#x27;silent&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">train_matrix = lgb.Dataset(x_train, label=y_train)</span><br><span class="line">cv_result_lgb = lgb.cv(</span><br><span class="line">    train_set=train_matrix,</span><br><span class="line">    early_stopping_rounds=<span class="number">1000</span>,</span><br><span class="line">    num_boost_round=<span class="number">20000</span>,</span><br><span class="line">    nfold=<span class="number">5</span>,</span><br><span class="line">    stratified=<span class="literal">True</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    params=base_params_lgb,</span><br><span class="line">    metrics=<span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    seed=<span class="number">2020</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;迭代次数 &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(cv_result_lgb[<span class="string">&#x27;auc-mean&#x27;</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最终模型的AUC为 &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">max</span>(cv_result_lgb[<span class="string">&#x27;auc-mean&#x27;</span>])))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">迭代次数 9364</span></span><br><span class="line"><span class="string">最终模型的AUC为 0.7378500759884923</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>模型参数已经确定，建立最终模型并对验证集进行验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">cv_scores = []</span><br><span class="line"><span class="keyword">for</span> i, (train_index, valid_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(x_train, y_train)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*** &#123;&#125; ***&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(i+<span class="number">1</span>)))</span><br><span class="line">    x_train_split, y_train_split, x_valid, y_valid = x_train.iloc[train_index], y_train[train_index], \</span><br><span class="line">                                                     x_train.iloc[valid_index], y_train[valid_index]</span><br><span class="line">    train_matrix = lgb.Dataset(x_train_split, label=y_train_split)</span><br><span class="line">    valid_matrix = lgb.Dataset(x_valid, label=y_valid)</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">178</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">21</span>,</span><br><span class="line">        <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">0.31</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.88</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.96</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">21</span>,</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">9.5</span>,</span><br><span class="line">        <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0.42</span>,</span><br><span class="line">        <span class="string">&#x27;nthread&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">2020</span>,</span><br><span class="line">        <span class="string">&#x27;silent&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    model = lgb.train(params, train_set=train_matrix, num_boost_round=<span class="number">9364</span>, valid_sets=valid_matrix,</span><br><span class="line">                      verbose_eval=<span class="number">1000</span>, early_stopping_rounds=<span class="number">200</span>)</span><br><span class="line">    val_pred = model.predict(x_valid, num_iteration=model.best_iteration)</span><br><span class="line">    cv_scores.append(roc_auc_score(y_valid, val_pred))</span><br><span class="line">    <span class="built_in">print</span>(cv_scores)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lgb_scotrainre_list: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(cv_scores))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lgb_score_mean: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(np.mean(cv_scores)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;lgb_score_std: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(np.std(cv_scores)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">lgb_scotrainre_list: [0.7386297996035015, 0.7356995636689628, 0.73900352698853, 0.7382979036633256, 0.7369681848895435]</span></span><br><span class="line"><span class="string">lgb_score_mean: 0.7377197957627727</span></span><br><span class="line"><span class="string">lgb_score_std: 0.0012211910753377566</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>使用训练集数据进行模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">final_model_lgb = lgb.train(base_params_lgb, train_set=train_matrix, valid_sets=valid_matrix, num_boost_round=<span class="number">13000</span>,</span><br><span class="line">                            verbose_eval=<span class="number">1000</span>, early_stopping_rounds=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><p>预测，并计算roc的相关指标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val_pred_lgb = final_model_lgb.predict(x_valid)</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_valid, val_pred_lgb)</span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;调参后lgb在验证集上的AUC： &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(roc_auc))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;调参后lgb在验证集上的AUC： 0.7369681848895435&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Validation ROC&#x27;</span>)</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Val AUC = %0.4f&#x27;</span> % roc_auc)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;ROC&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;r--&#x27;</span>)</span><br></pre></td></tr></table></figure><p>我的图片没有保存，总之结果和调参前相差不大</p><h2 id="保存模型到本地"><a href="#保存模型到本地" class="headerlink" title="保存模型到本地"></a>保存模型到本地</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dump(final_model_lgb, <span class="built_in">open</span>(<span class="string">&#x27;model/model_lgb_1.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这次调参下来，发现费了很大的功夫，模型的效果提升微乎其微，所以调参的优先级应该排在特征工程之后。选择什么样的模型，以及选择哪些数据作为特征训练，特征应该进行怎样的处理，这些特征工程对于分数的提高应该更大。<br>下一节在尝试模型融合之前，我会尝试用不同的模型先简单测试，看一下哪些模型适合该场景，另外在训练前，我需要先根据特征的重要性做一个特征的筛选，最后训练2-3个模型后再进行融合。希望分数能有一个大的提高。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h1&gt;&lt;p&gt;特征工程之后，我们基本了解了数据集的概貌，通过缺失值处理、异常值处理、归一化、独热编码、特征构造等一系列方法对数据进行了预处理，并根据不同模型的数据</summary>
      
    
    
    
    <category term="金融风控" scheme="http://example.com/categories/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>机器学习训练_金融风控_Task3_特征工程</title>
    <link href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task3_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task3_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</id>
    <published>2023-04-24T08:04:04.176Z</published>
    <updated>2023-04-24T08:18:15.677Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>特征工程为建模提前加工原料，不同的模型对数据的类型和形式也有所不同，对数据的处理也不尽相同。下面主要以代码展示特征工程环节，为下一节的建模调参做准备。</p><h1 id="导入第三方模块"><a href="#导入第三方模块" class="headerlink" title="导入第三方模块"></a>导入第三方模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold, KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score, roc_auc_score, log_loss</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns = <span class="literal">None</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, <span class="keyword">lambda</span> x: <span class="string">&#x27;%.2f&#x27;</span> % x)</span><br></pre></td></tr></table></figure><h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\train.csv&#x27;</span>)</span><br><span class="line">testA = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\testA.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="压缩数据"><a href="#压缩数据" class="headerlink" title="压缩数据"></a>压缩数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>  <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:  <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:  <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>    <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))  <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = reduce_mem_usage(train)</span><br><span class="line">testA = reduce_mem_usage(testA)</span><br></pre></td></tr></table></figure><h2 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h2><p>首先我们查找出数据中的标签、对象特征和数值特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numerical_fea = <span class="built_in">list</span>(train.select_dtypes(exclude=[<span class="string">&#x27;category&#x27;</span>]).columns)</span><br><span class="line">category_fea = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> numerical_fea, <span class="built_in">list</span>(train.columns)))</span><br><span class="line">label = <span class="string">&#x27;isDefault&#x27;</span></span><br><span class="line">numerical_fea.remove(label)</span><br></pre></td></tr></table></figure><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 查看缺失值情况</span></span><br><span class="line">train.info()</span><br><span class="line">train.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><h3 id="将训练集的缺失数据可视化"><a href="#将训练集的缺失数据可视化" class="headerlink" title="将训练集的缺失数据可视化"></a>将训练集的缺失数据可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">missing = train.isnull().<span class="built_in">sum</span>()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">missing.plot.bar()</span><br><span class="line"></span><br><span class="line">msno.matrix(train.sample(<span class="number">250</span>))</span><br><span class="line">msno.bar(train.sample(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020092123511651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="缺失数据并不多"><a href="#缺失数据并不多" class="headerlink" title="缺失数据并不多"></a>缺失数据并不多</h4><h3 id="按照平均数填充数值型特征"><a href="#按照平均数填充数值型特征" class="headerlink" title="按照平均数填充数值型特征"></a>按照平均数填充数值型特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train[numerical_fea] = train[numerical_fea].fillna(train[numerical_fea].median())</span><br><span class="line"></span><br><span class="line">testA[numerical_fea] = testA[numerical_fea].fillna(testA[numerical_fea].median())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;特征处理的时候，测试集要做同样的处理&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="按照众数填充类别型特征"><a href="#按照众数填充类别型特征" class="headerlink" title="按照众数填充类别型特征"></a>按照众数填充类别型特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train[category_fea] = train[category_fea].fillna(train[category_fea].mode())</span><br><span class="line">testA[category_fea] = testA[category_fea].fillna(testA[category_fea].mode())</span><br><span class="line">train.isnull().<span class="built_in">sum</span>()</span><br><span class="line">train.head()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;发现employmentLength没有按照众数填充，因为不是数值&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="处理category特征"><a href="#处理category特征" class="headerlink" title="处理category特征"></a>处理category特征</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">category_fea</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;[&#x27;grade&#x27;, &#x27;subGrade&#x27;, &#x27;employmentLength&#x27;, &#x27;issueDate&#x27;, &#x27;earliesCreditLine&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="时间格式处理"><a href="#时间格式处理" class="headerlink" title="时间格式处理"></a>时间格式处理</h2><h3 id="issueDate-贷款发放月份"><a href="#issueDate-贷款发放月份" class="headerlink" title="issueDate - 贷款发放月份"></a>issueDate - 贷款发放月份</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [train, testA]:</span><br><span class="line">    data[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data[<span class="string">&#x27;issueDate&#x27;</span>], <span class="built_in">format</span>=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">    <span class="comment"># 从数据可以看出，issueDate从2017年6月1日开始</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;train.groupby(&#x27;issueDate&#x27;)[&#x27;id&#x27;].count()&#x27;&#x27;&#x27;</span></span><br><span class="line">    startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">    data[<span class="string">&#x27;issueDateDT&#x27;</span>] = data[<span class="string">&#x27;issueDate&#x27;</span>].apply(<span class="keyword">lambda</span> x: x - startdate).dt.days</span><br></pre></td></tr></table></figure><h3 id="employmentLength-就业年限（年）-不满一年按0年算"><a href="#employmentLength-就业年限（年）-不满一年按0年算" class="headerlink" title="employmentLength - 就业年限（年） 不满一年按0年算"></a>employmentLength - 就业年限（年） 不满一年按0年算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">employment_re</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> pd.isnull(x):</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;&lt; 1 year&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;1 year&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;2 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;3 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;4 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;5 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">5</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;6 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;7 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">7</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;8 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">8</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;9 years&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">9</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;employmentLength&#x27;</span>] = train[<span class="string">&#x27;employmentLength&#x27;</span>].apply(<span class="keyword">lambda</span> x: employment_re(x))</span><br><span class="line">train[<span class="string">&#x27;employmentLength&#x27;</span>] = train[<span class="string">&#x27;employmentLength&#x27;</span>].fillna(train[<span class="string">&#x27;employmentLength&#x27;</span>].mode())</span><br><span class="line">testA[<span class="string">&#x27;employmentLength&#x27;</span>] = testA[<span class="string">&#x27;employmentLength&#x27;</span>].fillna(testA[<span class="string">&#x27;employmentLength&#x27;</span>].mode())</span><br></pre></td></tr></table></figure><h3 id="earliesCreditLine-借款人最早报告的信用额度开立的月份"><a href="#earliesCreditLine-借款人最早报告的信用额度开立的月份" class="headerlink" title="earliesCreditLine - 借款人最早报告的信用额度开立的月份"></a>earliesCreditLine - 借款人最早报告的信用额度开立的月份</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [train, testA]:</span><br><span class="line">    data[<span class="string">&#x27;earliesCreditLine&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> s: <span class="built_in">int</span>(s[-<span class="number">4</span>:]))</span><br></pre></td></tr></table></figure><h1 id="类别特征处理"><a href="#类别特征处理" class="headerlink" title="类别特征处理"></a>类别特征处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cate_features = [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, \</span><br><span class="line">                 <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> cate_features:</span><br><span class="line">    <span class="built_in">print</span>(f, <span class="string">&#x27;类型数：&#x27;</span>, train[f].nunique())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">grade 类型数： 7</span></span><br><span class="line"><span class="string">subGrade 类型数： 35</span></span><br><span class="line"><span class="string">employmentTitle 类型数： 248683</span></span><br><span class="line"><span class="string">homeOwnership 类型数： 6</span></span><br><span class="line"><span class="string">verificationStatus 类型数： 3</span></span><br><span class="line"><span class="string">purpose 类型数： 14</span></span><br><span class="line"><span class="string">postCode 类型数： 932</span></span><br><span class="line"><span class="string">regionCode 类型数： 51</span></span><br><span class="line"><span class="string">applicationType 类型数： 2</span></span><br><span class="line"><span class="string">initialListStatus 类型数： 2</span></span><br><span class="line"><span class="string">title 类型数： 6509</span></span><br><span class="line"><span class="string">policyCode 类型数： 1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="异常值处理-这里封装了箱线图的代码用于处理异常值，尺度scale为3，可以视情况缩小"><a href="#异常值处理-这里封装了箱线图的代码用于处理异常值，尺度scale为3，可以视情况缩小" class="headerlink" title="异常值处理 - 这里封装了箱线图的代码用于处理异常值，尺度scale为3，可以视情况缩小"></a>异常值处理 - 这里封装了箱线图的代码用于处理异常值，尺度scale为3，可以视情况缩小</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outliers_proc</span>(<span class="params">data, col_name, scale=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    用于清洗异常值,默认用box_plot(scale=3)进行清洗 - 箱线图处理异常值</span></span><br><span class="line"><span class="string">    :param data: 接收pandas数据格式</span></span><br><span class="line"><span class="string">    :param col_name: pandas列名</span></span><br><span class="line"><span class="string">    :param scale: 尺度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">box_plot_outliers</span>(<span class="params">data_ser, box_scale</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        利用箱线图去除异常值</span></span><br><span class="line"><span class="string">        :param data_ser: 接收pandas.Series数据格式</span></span><br><span class="line"><span class="string">        :param box_scale: 箱线图尺度</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># quantile(0.75) - 求数据的上四分位数 - Q3</span></span><br><span class="line">        <span class="comment"># quantile(0.25) - 求数据的下四分位数 - Q1</span></span><br><span class="line">        <span class="comment"># data_ser.quantile(0.75) - data_ser.quantile(0.25) = Q3 - Q1 = ΔQ --&gt; 四分位距</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        boxplot默认的上边缘到上四分位数的间距是1.5ΔQ,即 scale=1.5</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        这里设定的为3ΔQ:</span></span><br><span class="line"><span class="string">        超过了上边缘Q3+3ΔQ和下边缘Q1-3ΔQ的部分视为异常值</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        iqr = box_scale * (data_ser.quantile(<span class="number">0.75</span>) - data_ser.quantile(<span class="number">0.25</span>))  <span class="comment"># iqr - 上边缘到上四分位数的间距,即3ΔQ</span></span><br><span class="line">        val_low = data_ser.quantile(<span class="number">0.25</span>) - iqr  <span class="comment"># 下边缘 Q1-3ΔQ</span></span><br><span class="line">        val_up = data_ser.quantile(<span class="number">0.75</span>) + iqr  <span class="comment"># 上边缘 Q3+3ΔQ</span></span><br><span class="line">        rule_low = (data_ser &lt; val_low)  <span class="comment"># 低于下边缘 Q1-3ΔQ的为异常值</span></span><br><span class="line">        rule_up = (data_ser &gt; val_up)  <span class="comment"># 高于上边缘 Q3+3ΔQ的为异常值</span></span><br><span class="line">        <span class="keyword">return</span> (rule_low, rule_up), (val_low, val_up)  <span class="comment"># 得到异常值 / 上边缘与下边缘之间的值</span></span><br><span class="line"></span><br><span class="line">    data_n = data.copy()  <span class="comment"># 拷贝一份数据的副本</span></span><br><span class="line">    data_series = data_n[col_name]  <span class="comment"># 转化成pandas.Series数据格式</span></span><br><span class="line">    rule, value = box_plot_outliers(data_series, box_scale=scale)</span><br><span class="line">    <span class="comment"># data_series.shape[0] - 看data_series这个一维数组有几行,即原数据集的总列数</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    np.arange() - 函数返回一个有终点和起点的固定步长的排列</span></span><br><span class="line"><span class="string">                    一个参数时:参数值为终点,起点取默认值0,步长取默认值1</span></span><br><span class="line"><span class="string">                    两个参数时:第一个参数为起点,第二个参数为终点,步长取默认值1</span></span><br><span class="line"><span class="string">                    三个参数时:第一个参数为起点,第二个参数为终点,第三个参数为步长,其中步长支持小数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># np.arange(data_series.shape[0]) - 取N个数,N为数据集字段数,步长为1  --&gt; 生成的是列表</span></span><br><span class="line">    index = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>] | rule[<span class="number">1</span>]]  <span class="comment"># 挑出位于异常值区间的序号,放进标记为index的列表中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Delete number is: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(index)))  <span class="comment"># 输出要删除多少个异常值</span></span><br><span class="line"></span><br><span class="line">    data_n = data_n.drop(index)  <span class="comment"># 按索引查找并删除</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    reset_index() - 重塑索引 (因为有时候对dataframe做处理后索引可能是乱的,就像上面删除了异常值一样)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数详解:</span></span><br><span class="line"><span class="string">    drop - True:把原来的索引index列去掉,重置index      False:保留原来的索引，添加重置的index</span></span><br><span class="line"><span class="string">    inplace - True:原数组不变，对数据进行修改之后结果给新的数组     False:直接在原数组上对数据进行修改</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    data_n.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Now column number is: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data_n.shape[<span class="number">0</span>]))  <span class="comment"># 打印出现在的行数,即正常值的个数</span></span><br><span class="line"></span><br><span class="line">    index_low = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>]]  <span class="comment"># 挑出位于下异常值区间的序号,放进标记为index_low的列表中</span></span><br><span class="line">    outliers_low = data_series.iloc[index_low]  <span class="comment"># 把位于下异常值区间的数据放进outliers中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Description of data less than the lower bound is: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pd.Series(outliers_low).describe())  <span class="comment"># 对于位于下异常值区间的数据,做一个统计描述</span></span><br><span class="line"></span><br><span class="line">    index_up = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">1</span>]]  <span class="comment"># 挑出位于上异常值区间的序号,放进标记为index_up的列表中</span></span><br><span class="line">    outliers_up = data_series.iloc[index_up]  <span class="comment"># 把位于上异常值区间的数据放进outliers_up中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Description of data larger than the lower bound is: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pd.Series(outliers_up).describe())  <span class="comment"># 对于位于上异常值区间的数据,再做一个统计描述</span></span><br><span class="line"></span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    sns.boxplot - 箱线图</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数详解:</span></span><br><span class="line"><span class="string">    x, y, hue - 数据或向量数据的变量名称</span></span><br><span class="line"><span class="string">    data - 用于绘图的数据集</span></span><br><span class="line"><span class="string">    palette - 调色板名称</span></span><br><span class="line"><span class="string">    ax - 绘图时使用的matplotlib轴对象</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    sns.boxplot(y=data[col_name], data=data, palette=<span class="string">&#x27;Set1&#x27;</span>, ax=ax[<span class="number">0</span>])</span><br><span class="line">    sns.boxplot(y=data_n[col_name], data=data_n, palette=<span class="string">&#x27;Set1&#x27;</span>, ax=ax[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> data_n</span><br></pre></td></tr></table></figure><h1 id="以年收入annualIncome为例"><a href="#以年收入annualIncome为例" class="headerlink" title="以年收入annualIncome为例"></a>以年收入annualIncome为例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outliers_proc(train, <span class="string">&#x27;annualIncome&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200921235743859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>特征的相关行分析和特征筛选我想放在task4做。我对特征工程的理解就是处理数据，把脏数据清洗之后按照不同的模型做相应的变换，如one-hot编码，归一化处理等等。<br>具体看task4选择哪些模型在做响应的特征的处理。这边文章主要讲的是方法，具体涉及到每个特征的处理，还需要自己慢慢去尝试。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;特征工程&quot;&gt;&lt;a href=&quot;#特征工程&quot; class=&quot;headerlink&quot; title=&quot;特征工程&quot;&gt;&lt;/a&gt;特征工程&lt;/h1&gt;&lt;p&gt;特征工程为建模提前加工原料，不同的模型对数据的类型和形式也有所不同，对数据的处理也不尽相同。下面主要以代码展示特征工程环节，</summary>
      
    
    
    
    <category term="金融风控" scheme="http://example.com/categories/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>机器学习训练_金融风控_Task2_EDA</title>
    <link href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task2_EDA/"/>
    <id>http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task2_EDA/</id>
    <published>2023-04-24T08:03:55.571Z</published>
    <updated>2023-04-24T08:18:09.018Z</updated>
    
    <content type="html"><![CDATA[<p>此部分为零基础入门金融风控的 Task2 数据分析部分，带你来了解数据，熟悉数据，为后续的特征工程做准备，欢迎大家后续多多交流。</p><p>赛题：零基础入门数据挖掘 - 零基础入门金融风控之贷款违约</p><p>目的：</p><p>1.EDA价值主要在于熟悉了解整个数据集的基本情况（缺失值，异常值），对数据集进行验证是否可以进行接下来的机器学习或者深度学习建模.</p><p>2.了解变量间的相互关系、变量与预测值之间的存在关系。</p><p>3.为特征工程做准备</p><p><strong>这里主要记录了代码部分</strong></p><h1 id="一、导入第三方模块"><a href="#一、导入第三方模块" class="headerlink" title="一、导入第三方模块"></a>一、导入第三方模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns = <span class="literal">None</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, <span class="keyword">lambda</span> x: <span class="string">&#x27;%.2f&#x27;</span> % x)</span><br></pre></td></tr></table></figure><h1 id="二、数据读取"><a href="#二、数据读取" class="headerlink" title="二、数据读取"></a>二、数据读取</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><p>由于数据量有 80w 左右，做一个数据的压缩处理，方便计算和训练，这里创建一个reduce_mem_usage函数,通过调整数据类型,减少数据在内存中占用的空间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>  <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:  <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:  <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span> ** <span class="number">2</span>    <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))  <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><p>压缩数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train = reduce_mem_usage(train)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Memory usage of dataframe is 286.87 MB</span></span><br><span class="line"><span class="string">Memory usage after optimization is 69.46 MB</span></span><br><span class="line"><span class="string">Decreased by 75.8%</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>可以看出，压缩率不低。</p><h1 id="三、仔细看看特征"><a href="#三、仔细看看特征" class="headerlink" title="三、仔细看看特征"></a>三、仔细看看特征</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">train.info()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loanAmnt - 贷款金额</span></span><br><span class="line"><span class="string">term - 贷款期限（年）</span></span><br><span class="line"><span class="string">interestRate - 贷款利率</span></span><br><span class="line"><span class="string">installment - 分期付款金额</span></span><br><span class="line"><span class="string">grade - 贷款等级</span></span><br><span class="line"><span class="string">subGrade - 贷款等级之子集</span></span><br><span class="line"><span class="string">employmentTitle - 就业职称</span></span><br><span class="line"><span class="string">employmentLength - 就业年限（年）</span></span><br><span class="line"><span class="string">homeOwnership - 借款人在登记时提供的房屋所有权状况 --- int型，建议检查是否转化为类别型</span></span><br><span class="line"><span class="string">annualIncome - 年收入</span></span><br><span class="line"><span class="string">verificationStatus - 验证状态 --- int型，建议检查是否转化为类别型</span></span><br><span class="line"><span class="string">issueDate - 贷款发放的月份 --- category型，建议检查是否转化为日期格式</span></span><br><span class="line"><span class="string">purpose - 借款人在贷款申请时的贷款用途类别 --- int型，建议检查是否转化为类别型</span></span><br><span class="line"><span class="string">postCode - 借款人在贷款申请中提供的邮政编码的前3位数字 --- float型，建议检查是否转化为类别型/category</span></span><br><span class="line"><span class="string">regionCode - 地区编码 --- int型，建议检查是否转化为类别型/category</span></span><br><span class="line"><span class="string">dti - 债务收入比</span></span><br><span class="line"><span class="string">delinquency_2years - 借款人过去2年信用档案中逾期30天以上的违约事件数 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">ficoRangeLow - 借款人在贷款发放时的fico所属的下限范围</span></span><br><span class="line"><span class="string">ficoRangeHigh - 借款人在贷款发放时的fico所属的上限范围</span></span><br><span class="line"><span class="string">openAcc - 借款人信用档案中未结信用额度的数量 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">pubRec - 贬损公共记录的数量 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">pubRecBankruptcies - 公开记录清除的数量 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">revolBal - 信贷周转余额合计</span></span><br><span class="line"><span class="string">revolUtil - 循环额度利用率，或借款人使用的相对于所有可用循环信贷的信贷金额</span></span><br><span class="line"><span class="string">totalAcc - 借款人信用档案中当前的信用额度总数</span></span><br><span class="line"><span class="string">initialListStatus - 贷款的初始列表状态 --- int型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">applicationType - 表明贷款是个人申请还是与两个共同借款人的联合申请 --- int型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">earliesCreditLine 借款人最早报告的信用额度开立的月份 --- category型，建议检查是否转化为日期格式</span></span><br><span class="line"><span class="string">title - 借款人提供的贷款名称 --- float型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">policyCode - 公开可用的策略_代码 = 1 / 新产品不公开可用的策略_代码 = 2 --- float型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">n0-n14 - 匿名特征</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>分析可疑待调整数据类型的特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">homeOwnership - 借款人在登记时提供的房屋所有权状况 --- int型，建议检查是否转化为类别型</span></span><br><span class="line"><span class="string">verificationStatus - 验证状态 --- int型，建议检查是否转化为类别型</span></span><br><span class="line"><span class="string">issueDate - 贷款发放的月份 --- category型，建议检查是否转化为日期格式</span></span><br><span class="line"><span class="string">purpose - 借款人在贷款申请时的贷款用途类别 --- int型，建议检查是否转化为类别型</span></span><br><span class="line"><span class="string">postCode - 借款人在贷款申请中提供的邮政编码的前3位数字 --- float型，建议检查是否转化为类别型/category</span></span><br><span class="line"><span class="string">regionCode - 地区编码 --- int型，建议检查是否转化为类别型/category</span></span><br><span class="line"><span class="string">delinquency_2years - 借款人过去2年信用档案中逾期30天以上的违约事件数 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">openAcc - 借款人信用档案中未结信用额度的数量 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">pubRec - 贬损公共记录的数量 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">pubRecBankruptcies - 公开记录清除的数量 --- float型，建议检查是否转化为int型</span></span><br><span class="line"><span class="string">initialListStatus - 贷款的初始列表状态 --- int型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">applicationType - 表明贷款是个人申请还是与两个共同借款人的联合申请 --- int型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">earliesCreditLine 借款人最早报告的信用额度开立的月份 --- category型，建议检查是否转化为日期格式</span></span><br><span class="line"><span class="string">title - 借款人提供的贷款名称 --- float型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">policyCode - 公开可用的策略_代码 = 1 / 新产品不公开可用的策略_代码 = 2 --- float型，建议检查是否转化成类别型/category</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="homeOwnership-借款人在登记时提供的房屋所有权状况-—-int型，建议检查是否转化为类别型"><a href="#homeOwnership-借款人在登记时提供的房屋所有权状况-—-int型，建议检查是否转化为类别型" class="headerlink" title="homeOwnership - 借款人在登记时提供的房屋所有权状况 — int型，建议检查是否转化为类别型"></a>homeOwnership - 借款人在登记时提供的房屋所有权状况 — int型，建议检查是否转化为类别型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;homeOwnership&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">这个字段大概率为借款人的房屋拥有量，无需转化为类别型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">homeOwnership</span></span><br><span class="line"><span class="string">0    395732</span></span><br><span class="line"><span class="string">1    317660</span></span><br><span class="line"><span class="string">2     86309</span></span><br><span class="line"><span class="string">3       185</span></span><br><span class="line"><span class="string">4        33</span></span><br><span class="line"><span class="string">5        81</span></span><br><span class="line"><span class="string">Name: id, dtype: int64</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="verificationStatus-验证状态-—-int型，建议检查是否转化为类别型"><a href="#verificationStatus-验证状态-—-int型，建议检查是否转化为类别型" class="headerlink" title="verificationStatus - 验证状态 — int型，建议检查是否转化为类别型"></a>verificationStatus - 验证状态 — int型，建议检查是否转化为类别型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;verificationStatus&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成类别型&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;verificationStatus&#x27;</span>] = train[<span class="string">&#x27;verificationStatus&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="issueDate-贷款发放的月份-—-category型，建议检查是否转化为日期格式"><a href="#issueDate-贷款发放的月份-—-category型，建议检查是否转化为日期格式" class="headerlink" title="issueDate - 贷款发放的月份 — category型，建议检查是否转化为日期格式"></a>issueDate - 贷款发放的月份 — category型，建议检查是否转化为日期格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;issueDate&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成时间类型&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;issueDate&#x27;</span>] = train[<span class="string">&#x27;issueDate&#x27;</span>].astype(<span class="string">&#x27;datetime64&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="purpose-借款人在贷款申请时的贷款用途类别-—-int型，建议检查是否转化为类别型"><a href="#purpose-借款人在贷款申请时的贷款用途类别-—-int型，建议检查是否转化为类别型" class="headerlink" title="purpose - 借款人在贷款申请时的贷款用途类别 — int型，建议检查是否转化为类别型"></a>purpose - 借款人在贷款申请时的贷款用途类别 — int型，建议检查是否转化为类别型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;purpose&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成类别型&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;purpose&#x27;</span>] = train[<span class="string">&#x27;purpose&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="postCode-借款人在贷款申请中提供的邮政编码的前3位数字-—-float型，建议检查是否转化为类别型-x2F-category"><a href="#postCode-借款人在贷款申请中提供的邮政编码的前3位数字-—-float型，建议检查是否转化为类别型-x2F-category" class="headerlink" title="postCode - 借款人在贷款申请中提供的邮政编码的前3位数字 — float型，建议检查是否转化为类别型&#x2F;category"></a>postCode - 借款人在贷款申请中提供的邮政编码的前3位数字 — float型，建议检查是否转化为类别型&#x2F;category</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;postCode&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成类别型,并去掉后三位字符&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;postCode&#x27;</span>] = train[<span class="string">&#x27;postCode&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">train[<span class="string">&#x27;postCode&#x27;</span>] = train[<span class="string">&#x27;postCode&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x)[:-<span class="number">2</span>])</span><br></pre></td></tr></table></figure><h2 id="regionCode-地区编码-—-int型，建议检查是否转化为类别型-x2F-category"><a href="#regionCode-地区编码-—-int型，建议检查是否转化为类别型-x2F-category" class="headerlink" title="regionCode - 地区编码 — int型，建议检查是否转化为类别型&#x2F;category"></a>regionCode - 地区编码 — int型，建议检查是否转化为类别型&#x2F;category</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;regionCode&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成类别型&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;regionCode&#x27;</span>] = train[<span class="string">&#x27;regionCode&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="delinquency-2years-借款人过去2年信用档案中逾期30天以上的违约事件数-—-float型，建议检查是否转化为int型"><a href="#delinquency-2years-借款人过去2年信用档案中逾期30天以上的违约事件数-—-float型，建议检查是否转化为int型" class="headerlink" title="delinquency_2years - 借款人过去2年信用档案中逾期30天以上的违约事件数 — float型，建议检查是否转化为int型"></a>delinquency_2years - 借款人过去2年信用档案中逾期30天以上的违约事件数 — float型，建议检查是否转化为int型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;delinquency_2years&#x27;</span>].head(<span class="number">50</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成整形&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;delinquency_2years&#x27;</span>] = train[<span class="string">&#x27;delinquency_2years&#x27;</span>].astype(<span class="string">&#x27;int8&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="openAcc-借款人信用档案中未结信用额度的数量-—-float型，建议检查是否转化为int型"><a href="#openAcc-借款人信用档案中未结信用额度的数量-—-float型，建议检查是否转化为int型" class="headerlink" title="openAcc - 借款人信用档案中未结信用额度的数量 — float型，建议检查是否转化为int型"></a>openAcc - 借款人信用档案中未结信用额度的数量 — float型，建议检查是否转化为int型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;openAcc&#x27;</span>].head(<span class="number">50</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成整形&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;openAcc&#x27;</span>] = train[<span class="string">&#x27;openAcc&#x27;</span>].astype(<span class="string">&#x27;int8&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="pubRec-贬损公共记录的数量-—-float型，建议检查是否转化为int型"><a href="#pubRec-贬损公共记录的数量-—-float型，建议检查是否转化为int型" class="headerlink" title="pubRec - 贬损公共记录的数量 — float型，建议检查是否转化为int型"></a>pubRec - 贬损公共记录的数量 — float型，建议检查是否转化为int型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;pubRec&#x27;</span>].head(<span class="number">50</span>)</span><br><span class="line">train.groupby(<span class="string">&#x27;pubRec&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成整形&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;pubRec&#x27;</span>] = train[<span class="string">&#x27;pubRec&#x27;</span>].astype(<span class="string">&#x27;int8&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="pubRecBankruptcies-公开记录清除的数量-—-float型，建议检查是否转化为int型"><a href="#pubRecBankruptcies-公开记录清除的数量-—-float型，建议检查是否转化为int型" class="headerlink" title="pubRecBankruptcies - 公开记录清除的数量 — float型，建议检查是否转化为int型"></a>pubRecBankruptcies - 公开记录清除的数量 — float型，建议检查是否转化为int型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;pubRecBankruptcies&#x27;</span>].head(<span class="number">50</span>)</span><br><span class="line">train.groupby(<span class="string">&#x27;pubRecBankruptcies&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;将缺失值标记为-99后，转化成整形&#x27;&#x27;&#x27;</span></span><br><span class="line">train.fillna(&#123;<span class="string">&#x27;pubRecBankruptcies&#x27;</span>: -<span class="number">99</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">train[<span class="string">&#x27;pubRecBankruptcies&#x27;</span>] = train[<span class="string">&#x27;pubRecBankruptcies&#x27;</span>].astype(<span class="string">&#x27;int8&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="initialListStatus-贷款的初始列表状态-—-int型，建议检查是否转化成类别型-x2F-category"><a href="#initialListStatus-贷款的初始列表状态-—-int型，建议检查是否转化成类别型-x2F-category" class="headerlink" title="initialListStatus - 贷款的初始列表状态 — int型，建议检查是否转化成类别型&#x2F;category"></a>initialListStatus - 贷款的初始列表状态 — int型，建议检查是否转化成类别型&#x2F;category</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;initialListStatus&#x27;</span>].head(<span class="number">50</span>)</span><br><span class="line">train.groupby(<span class="string">&#x27;initialListStatus&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转化为类别型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">initialListStatus</span></span><br><span class="line"><span class="string">0    466438</span></span><br><span class="line"><span class="string">1    333562</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;initialListStatus&#x27;</span>] = train[<span class="string">&#x27;initialListStatus&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="applicationType-表明贷款是个人申请还是与两个共同借款人的联合申请-—-int型，建议检查是否转化成类别型-x2F-category"><a href="#applicationType-表明贷款是个人申请还是与两个共同借款人的联合申请-—-int型，建议检查是否转化成类别型-x2F-category" class="headerlink" title="applicationType - 表明贷款是个人申请还是与两个共同借款人的联合申请 — int型，建议检查是否转化成类别型&#x2F;category"></a>applicationType - 表明贷款是个人申请还是与两个共同借款人的联合申请 — int型，建议检查是否转化成类别型&#x2F;category</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;applicationType&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转化为类别型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">applicationType</span></span><br><span class="line"><span class="string">0    784586</span></span><br><span class="line"><span class="string">1     15414</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;applicationType&#x27;</span>] = train[<span class="string">&#x27;applicationType&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="earliesCreditLine-借款人最早报告的信用额度开立的月份-—-category型，建议检查是否转化为日期格式"><a href="#earliesCreditLine-借款人最早报告的信用额度开立的月份-—-category型，建议检查是否转化为日期格式" class="headerlink" title="earliesCreditLine 借款人最早报告的信用额度开立的月份 — category型，建议检查是否转化为日期格式"></a>earliesCreditLine 借款人最早报告的信用额度开立的月份 — category型，建议检查是否转化为日期格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;earliesCreditLine&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line">train[<span class="string">&#x27;earliesCreditLine&#x27;</span>].head()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;转化成时间类型&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="创建辅助列"><a href="#创建辅助列" class="headerlink" title="创建辅助列"></a>创建辅助列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] = train[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[-<span class="number">4</span>:])</span><br><span class="line">train[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = train[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> x: x[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">month_re</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x == <span class="string">&#x27;Jan&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;01&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Feb&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;02&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Mar&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;03&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Apr&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;04&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;May&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;05&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jun&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;06&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jul&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;07&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Aug&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;08&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Sep&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;09&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Oct&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Nov&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;11&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;12&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;earliesCreditLine_month_2&#x27;</span>] = train[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>].apply(<span class="keyword">lambda</span> x: month_re(x))</span><br><span class="line">train[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = train[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] + <span class="string">&#x27;-&#x27;</span> + train[<span class="string">&#x27;earliesCreditLine_month_2&#x27;</span>]</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;earliesCreditLine&#x27;</span>] = train[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>].astype(<span class="string">&#x27;datetime64&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;earliesCreditLine_month_2&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="title-借款人提供的贷款名称-—-float型，建议检查是否转化成类别型-x2F-category"><a href="#title-借款人提供的贷款名称-—-float型，建议检查是否转化成类别型-x2F-category" class="headerlink" title="title - 借款人提供的贷款名称 — float型，建议检查是否转化成类别型&#x2F;category"></a>title - 借款人提供的贷款名称 — float型，建议检查是否转化成类别型&#x2F;category</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;title&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;类别很多，不适合转化为category，转化成object&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;title&#x27;</span>] = train[<span class="string">&#x27;title&#x27;</span>].astype(<span class="string">&#x27;str&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="policyCode-公开可用的策略-代码-x3D-1-x2F-新产品不公开可用的策略-代码-x3D-2-—-float型，建议检查是否转化成类别型-x2F-category"><a href="#policyCode-公开可用的策略-代码-x3D-1-x2F-新产品不公开可用的策略-代码-x3D-2-—-float型，建议检查是否转化成类别型-x2F-category" class="headerlink" title="policyCode - 公开可用的策略_代码 &#x3D; 1 &#x2F; 新产品不公开可用的策略_代码 &#x3D; 2 — float型，建议检查是否转化成类别型&#x2F;category"></a>policyCode - 公开可用的策略_代码 &#x3D; 1 &#x2F; 新产品不公开可用的策略_代码 &#x3D; 2 — float型，建议检查是否转化成类别型&#x2F;category</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;policyCode&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;全是类别1，这个字段没用，可以删了&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;policyCode&#x27;</span>]</span><br></pre></td></tr></table></figure><h1 id="四、看一下各个特征的缺失情况"><a href="#四、看一下各个特征的缺失情况" class="headerlink" title="四、看一下各个特征的缺失情况"></a>四、看一下各个特征的缺失情况</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;There are <span class="subst">&#123;train.isnull().<span class="built_in">any</span>().<span class="built_in">sum</span>()&#125;</span> columns in train dataset with miss values.&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;There are 20 columns in train dataset with miss values.&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="进一步查看特征缺失率"><a href="#进一步查看特征缺失率" class="headerlink" title="进一步查看特征缺失率"></a>进一步查看特征缺失率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">have_null_fea_dict = (train.isnull().<span class="built_in">sum</span>() / <span class="built_in">len</span>(train)).to_dict()</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> have_null_fea_dict.items():</span><br><span class="line">    <span class="built_in">print</span>(key, value)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">整体上没有缺失比例很高的特征</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">id 0.0</span></span><br><span class="line"><span class="string">loanAmnt 0.0</span></span><br><span class="line"><span class="string">term 0.0</span></span><br><span class="line"><span class="string">interestRate 0.0</span></span><br><span class="line"><span class="string">installment 0.0</span></span><br><span class="line"><span class="string">grade 0.0</span></span><br><span class="line"><span class="string">subGrade 0.0</span></span><br><span class="line"><span class="string">employmentTitle 1.25e-06</span></span><br><span class="line"><span class="string">employmentLength 0.05849875</span></span><br><span class="line"><span class="string">homeOwnership 0.0</span></span><br><span class="line"><span class="string">annualIncome 0.0</span></span><br><span class="line"><span class="string">verificationStatus 0.0</span></span><br><span class="line"><span class="string">issueDate 0.0</span></span><br><span class="line"><span class="string">isDefault 0.0</span></span><br><span class="line"><span class="string">purpose 0.0</span></span><br><span class="line"><span class="string">postCode 1.25e-06</span></span><br><span class="line"><span class="string">regionCode 0.0</span></span><br><span class="line"><span class="string">dti 0.00029875</span></span><br><span class="line"><span class="string">delinquency_2years 0.0</span></span><br><span class="line"><span class="string">ficoRangeLow 0.0</span></span><br><span class="line"><span class="string">ficoRangeHigh 0.0</span></span><br><span class="line"><span class="string">openAcc 0.0</span></span><br><span class="line"><span class="string">pubRec 0.0</span></span><br><span class="line"><span class="string">pubRecBankruptcies 0.0</span></span><br><span class="line"><span class="string">revolBal 0.0</span></span><br><span class="line"><span class="string">revolUtil 0.00066375</span></span><br><span class="line"><span class="string">totalAcc 0.0</span></span><br><span class="line"><span class="string">initialListStatus 0.0</span></span><br><span class="line"><span class="string">applicationType 0.0</span></span><br><span class="line"><span class="string">earliesCreditLine 0.0</span></span><br><span class="line"><span class="string">title 0.0</span></span><br><span class="line"><span class="string">n0 0.0503375</span></span><br><span class="line"><span class="string">n1 0.0503375</span></span><br><span class="line"><span class="string">n2 0.0503375</span></span><br><span class="line"><span class="string">n2.1 0.0503375</span></span><br><span class="line"><span class="string">n4 0.04154875</span></span><br><span class="line"><span class="string">n5 0.0503375</span></span><br><span class="line"><span class="string">n6 0.0503375</span></span><br><span class="line"><span class="string">n7 0.0503375</span></span><br><span class="line"><span class="string">n8 0.05033875</span></span><br><span class="line"><span class="string">n9 0.0503375</span></span><br><span class="line"><span class="string">n10 0.04154875</span></span><br><span class="line"><span class="string">n11 0.08719</span></span><br><span class="line"><span class="string">n12 0.0503375</span></span><br><span class="line"><span class="string">n13 0.0503375</span></span><br><span class="line"><span class="string">n14 0.0503375</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="绘图看下"><a href="#绘图看下" class="headerlink" title="绘图看下"></a>绘图看下</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">missing = train.isnull().<span class="built_in">sum</span>() / <span class="built_in">len</span>(train)</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># missing.plot.bar()</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200918223837584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="查看特征的数值类型有哪些，对象类型有哪些，将数值型的特征挑出"><a href="#查看特征的数值类型有哪些，对象类型有哪些，将数值型的特征挑出" class="headerlink" title="查看特征的数值类型有哪些，对象类型有哪些，将数值型的特征挑出"></a>查看特征的数值类型有哪些，对象类型有哪些，将数值型的特征挑出</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数值型特征本是可以直接入模的，但往往风控人员要对其做分箱，转化为WOE编码进而做标准评分卡等操作。</span></span><br><span class="line"><span class="string">从模型效果上来看，特征分箱主要是为了降低变量的复杂性，减少变量噪音对模型的影响，提高自变量和因变量的相关度，从而使模型更加稳定</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">numerical_fea = <span class="built_in">list</span>(train.select_dtypes(exclude=[<span class="string">&#x27;category&#x27;</span>, <span class="string">&#x27;object&#x27;</span>]).columns)</span><br><span class="line">category_fea = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> numerical_fea, <span class="built_in">list</span>(train.columns)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">select_dtypes()函数能够根据数据类型选择特征</span></span><br><span class="line"><span class="string">filter(f, list)</span></span><br><span class="line"><span class="string">    filter()接收一个函数 f 和一个list，这个函数 f 的作用是对每个元素进行判断，返回 True或 False</span></span><br><span class="line"><span class="string">    filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="数值型变量分析，数值型包括连续型变量和离散型变量的"><a href="#数值型变量分析，数值型包括连续型变量和离散型变量的" class="headerlink" title="数值型变量分析，数值型包括连续型变量和离散型变量的"></a>数值型变量分析，数值型包括连续型变量和离散型变量的</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_numerical_serial_fea</span>(<span class="params">data, feas</span>):</span><br><span class="line">    numerical_serial_fea = []</span><br><span class="line">    numerical_noserial_fea = []</span><br><span class="line">    <span class="keyword">for</span> fea <span class="keyword">in</span> feas:</span><br><span class="line">        temp = data[fea].nunique()</span><br><span class="line">        <span class="comment"># 如果特征的数值数据不超过10中，那么纳入离散特征范畴，反之纳入连续性数值范畴</span></span><br><span class="line">        <span class="keyword">if</span> temp &lt;= <span class="number">10</span>:</span><br><span class="line">            numerical_noserial_fea.append(fea)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        numerical_serial_fea.append(fea)</span><br><span class="line">    <span class="keyword">return</span> numerical_serial_fea, numerical_noserial_fea</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">numerical_serial_fea, numerical_noserial_fea = get_numerical_serial_fea(train, numerical_fea)</span><br><span class="line"></span><br><span class="line">numerical_serial_fea.remove(<span class="string">&#x27;earliesCreditLine&#x27;</span>)</span><br><span class="line">numerical_serial_fea.remove(<span class="string">&#x27;issueDate&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="看看还有有哪些离散型的数值变量"><a href="#看看还有有哪些离散型的数值变量" class="headerlink" title="看看还有有哪些离散型的数值变量"></a>看看还有有哪些离散型的数值变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(numerical_noserial_fea)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;[&#x27;term&#x27;, &#x27;homeOwnership&#x27;, &#x27;isDefault&#x27;, &#x27;n11&#x27;, &#x27;n12&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="term-贷款期限（年）"><a href="#term-贷款期限（年）" class="headerlink" title="term - 贷款期限（年）"></a>term - 贷款期限（年）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;term&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;只有3年和5年两种，可以考虑转换成类别型数据&#x27;&#x27;&#x27;</span></span><br><span class="line">train[<span class="string">&#x27;term&#x27;</span>] = train[<span class="string">&#x27;term&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="‘homeOwnership’已经分析过，-‘isDefault’是标签，不动"><a href="#‘homeOwnership’已经分析过，-‘isDefault’是标签，不动" class="headerlink" title="‘homeOwnership’已经分析过， ‘isDefault’是标签，不动"></a>‘homeOwnership’已经分析过， ‘isDefault’是标签，不动</h2><h2 id="‘n11’-匿名变量"><a href="#‘n11’-匿名变量" class="headerlink" title="‘n11’ 匿名变量"></a>‘n11’ 匿名变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;n11&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;分类相差悬殊，几乎都是0.00，无分析价值直接剔除&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;n11&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="‘n12’-匿名变量"><a href="#‘n12’-匿名变量" class="headerlink" title="‘n12’ 匿名变量"></a>‘n12’ 匿名变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train.groupby(<span class="string">&#x27;n12&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;同n11一样，分类相差悬殊，几乎都是0.00，无分析价值直接剔除&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&#x27;n12&#x27;</span>]</span><br></pre></td></tr></table></figure><h1 id="五、数值连续型变量分析"><a href="#五、数值连续型变量分析" class="headerlink" title="五、数值连续型变量分析"></a>五、数值连续型变量分析</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">f = pd.melt(train, value_vars=numerical_serial_fea)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;melt() - 行列自定义转化&#x27;&#x27;&#x27;</span></span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">&#x27;variable&#x27;</span>, col_wrap=<span class="number">2</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sns.FaceGrid()</span></span><br><span class="line"><span class="string">一个FacetGrid可以与多达三个维度可以得出：row，col，和hue。前两个与得到的轴阵列有明显的对应关系; </span></span><br><span class="line"><span class="string">将色调变量视为沿深度轴的第三个维度，其中不同的级别用不同的颜色绘制。</span></span><br><span class="line"><span class="string">通过使用FacetGrid数据框初始化对象以及将形成网格的行，列或色调维度的变量名称来使用该类。</span></span><br><span class="line"><span class="string">这些变量应该是分类的或离散的，然后变量的每个级别的数据将用于沿该轴的小平面。</span></span><br><span class="line"><span class="string">    data - 处理后的（“长格式”）dataframe数据，其中每一列都是一个变量（特征），每一行都是一个样本</span></span><br><span class="line"><span class="string">    col - 定义数据子集的变量，这些变量将在网格的不同方面绘制</span></span><br><span class="line"><span class="string">    col_wrap - 图网格列维度限制，比如col_wrap =3，那么在这个画布里最多只能画3列，行不限制，这样就限制了列的个数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">g = g.<span class="built_in">map</span>(sns.distplot, <span class="string">&#x27;value&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200918224456639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="查看某一个数值型变量的分布，查看变量是否符合正态分布，如果不符合正太分布的变量可以log化后再观察下是否符合正态分布"><a href="#查看某一个数值型变量的分布，查看变量是否符合正态分布，如果不符合正太分布的变量可以log化后再观察下是否符合正态分布" class="headerlink" title="查看某一个数值型变量的分布，查看变量是否符合正态分布，如果不符合正太分布的变量可以log化后再观察下是否符合正态分布"></a>查看某一个数值型变量的分布，查看变量是否符合正态分布，如果不符合正太分布的变量可以log化后再观察下是否符合正态分布</h2><h1 id="六、变量分布可视化"><a href="#六、变量分布可视化" class="headerlink" title="六、变量分布可视化"></a>六、变量分布可视化</h1><h2 id="单一变量分布可视化，以employmentLength为例"><a href="#单一变量分布可视化，以employmentLength为例" class="headerlink" title="单一变量分布可视化，以employmentLength为例"></a>单一变量分布可视化，以employmentLength为例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">sns.barplot(train[<span class="string">&#x27;employmentLength&#x27;</span>].value_counts(dropna=<span class="literal">False</span>)[:<span class="number">20</span>],</span><br><span class="line">            train[<span class="string">&#x27;employmentLength&#x27;</span>].value_counts(dropna=<span class="literal">False</span>).keys()[:<span class="number">20</span>])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200918224725571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="根据标签不同可视化x某个特征的分布"><a href="#根据标签不同可视化x某个特征的分布" class="headerlink" title="根据标签不同可视化x某个特征的分布"></a>根据标签不同可视化x某个特征的分布</h2><p>首先查看类别型变量在不同标签值上的分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;将不同标签的数据分开&#x27;&#x27;&#x27;</span></span><br><span class="line">train_loan_fr = train.loc[train[<span class="string">&#x27;isDefault&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line">train_loan_nofr = train.loc[train[<span class="string">&#x27;isDefault&#x27;</span>] == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">15</span>, <span class="number">8</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;建立画布&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">train_loan_fr.groupby(<span class="string">&#x27;grade&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax1, title=<span class="string">&#x27;Count of grade fraud&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;逾期数据下，不同grade的分布情况，画在子画布1上&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">train_loan_nofr.groupby(<span class="string">&#x27;grade&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax2, title=<span class="string">&#x27;Count of grade non-fraud&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;非逾期数据下，不同grade的分布情况，画在子画布2上&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">train_loan_fr.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax3,</span><br><span class="line">                                                             title=<span class="string">&#x27;Count of employmentLength fraud&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;逾期数据下，不同employmentLength的分布情况，画在子画布3上&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">train_loan_nofr.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax4,</span><br><span class="line">                                                               title=<span class="string">&#x27;Count of employmentLength non-fraud&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;非逾期数据下，不同employmentLength的分布情况，画在子画布4上&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200918225145816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="再查看连续数值型变量在不同标签值上的分布，以loanAmnt为例"><a href="#再查看连续数值型变量在不同标签值上的分布，以loanAmnt为例" class="headerlink" title="再查看连续数值型变量在不同标签值上的分布，以loanAmnt为例"></a>再查看连续数值型变量在不同标签值上的分布，以loanAmnt为例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig2, ((ax5, ax6)) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">15</span>, <span class="number">6</span>))</span><br><span class="line">train_loan_fr[<span class="string">&#x27;loanAmnt&#x27;</span>].apply(np.log).plot(kind=<span class="string">&#x27;hist&#x27;</span>, bins=<span class="number">100</span>,</span><br><span class="line">                                             title=<span class="string">&#x27;Log Loan Amnt - Fraud&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>, xlim=(-<span class="number">3</span>, <span class="number">10</span>), ax=ax5)</span><br><span class="line"></span><br><span class="line">train_loan_nofr[<span class="string">&#x27;loanAmnt&#x27;</span>].apply(np.log).plot(kind=<span class="string">&#x27;hist&#x27;</span>, bins=<span class="number">100</span>,</span><br><span class="line">                                               title=<span class="string">&#x27;Log Loan Amnt - noFraud&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>, xlim=(-<span class="number">3</span>, <span class="number">10</span>), ax=ax6)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200918225227330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h1><p>EDA阶段是对数据的一个探索，大致了解一下数据的类型、分布、基本质量、相关性等。详细的处理在下一章节特征工程会涉及。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;此部分为零基础入门金融风控的 Task2 数据分析部分，带你来了解数据，熟悉数据，为后续的特征工程做准备，欢迎大家后续多多交流。&lt;/p&gt;
&lt;p&gt;赛题：零基础入门数据挖掘 - 零基础入门金融风控之贷款违约&lt;/p&gt;
&lt;p&gt;目的：&lt;/p&gt;
&lt;p&gt;1.EDA价值主要在于熟悉了解整个</summary>
      
    
    
    
    <category term="金融风控" scheme="http://example.com/categories/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>机器学习训练_金融风控_Task1_赛题理解</title>
    <link href="http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task1_%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <id>http://example.com/2023/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83_%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7_Task1_%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</id>
    <published>2023-04-24T08:03:45.121Z</published>
    <updated>2023-04-24T08:18:02.655Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, \</span><br><span class="line">    precision_recall_curve, roc_curve, roc_auc_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">train = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\train.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">testA = pd.read_csv(<span class="string">r&#x27;D:\Users\Felixteng\Documents\Pycharm Files\loanDefaultForecast\data\testA.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.2&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> testA[<span class="string">&#x27;n2.3&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类指标评价</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 混淆矩阵</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">若一个实例是正类，并且被预测为正类，即为真正类TP(True Positive )</span></span><br><span class="line"><span class="string">若一个实例是正类，但是被预测为负类，即为假负类FN(False Negative )</span></span><br><span class="line"><span class="string">若一个实例是负类，但是被预测为正类，即为假正类FP(False Positive )</span></span><br><span class="line"><span class="string">若一个实例是负类，并且被预测为负类，即为真负类TN(True Negative )</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 给出一组预测值和真实值</span></span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 输出混淆矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;混淆矩阵:\n&#x27;</span>, confusion_matrix(y_true, y_pred))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">混淆矩阵:</span></span><br><span class="line"><span class="string"> [[1 1]</span></span><br><span class="line"><span class="string"> [1 1]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">行代表真实，列代表预测</span></span><br><span class="line"><span class="string">第一行第一列的1，代表真实为第一类且预测成第一类的有1个</span></span><br><span class="line"><span class="string">第二行第一列的1，代表真实为第二类且预测成第一类的有1个</span></span><br><span class="line"><span class="string">第一行第二列的1，代表真实为第一类且预测成第二类的有1个</span></span><br><span class="line"><span class="string">第二行第二列的1，代表真实为第二类且预测成第二类的有1个</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 准确率 accuracy</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">准确率是常用的一个评价指标，但是不适合样本不均衡的情况。 </span></span><br><span class="line"><span class="string">Accuracy = (TP+TN) / (P+N)，即样本中预测正确的百分比，通常来说，正确率越高，分类器越好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ACC:&#x27;</span>, accuracy_score(y_true, y_pred))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;ACC: 0.5&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 精确率/查准率 precision</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">precision = TP / (TP + FP)，即样本中判断为正例的样本有多少是真的正例，正确预测为正样本占预测为正样本的百分比</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Precision:&#x27;</span>, precision_score(y_true, y_pred))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;Precision: 0.5&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 召回率/查全率 recall</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">recall = TP / (TP + FN)，即正样本中有多少判断正确，正确预测为正样本占正样本的百分比</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Recall:&#x27;</span>, recall_score(y_true, y_pred))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;Recall: 0.5&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # F1-score 精确率 precision 和召回率 recall 的一种调和平均</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">f1-score = 2 * (precision * recall) / (precision + recall)，f1分数认为召回率和精准率同等重要</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">f2-score = (1 + 4) * (precision * recall) / (4 * precision + recall)，f2分数认为召回率的重要程度是精准率的2倍</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">f0.5-score = (1 + 0.25) * (precision * recall) / (0.25 * precision + recall)，f0.5分数认为召回率的重要程度是精准率的一半</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fβ-score = (1 + β²) * (precision * recall) / (β² * precision + recall)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1-score:&#x27;</span>, f1_score(y_true, y_pred))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;F1-score: 0.5&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # P-R曲线</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">P-R曲线是描述精确率和召回率变化的曲线</span></span><br><span class="line"><span class="string">横轴为召回率recall，纵轴为精确率precision</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">通过置信度对样本进行排序，再逐个样本选择阈值，可以得出每个置信度下的混淆矩阵，再通过计算P和R得到P-R曲线</span></span><br><span class="line"><span class="string">当一个模型a的P-R曲线完全包住另一个模型b的P-R曲线时，即可认为a优于b。其他情况下，可以使用平衡点，也即F1值，或者曲线下的面积来评估模型的好坏</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">当召回率不等于0时，P-R曲线的点和ROC曲线的点都能一一对应，因为两条曲线的点都能对应一个置信度阈值确认的混淆矩阵</span></span><br><span class="line"><span class="string">当一个模型a在P-R上优于b时，a在ROC曲线上也同样会优于b，反过来也同样成立</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">当正样本个数严重小于负样本个数，数据严重倾斜时，P-R曲线相比较于ROC曲线更加适合</span></span><br><span class="line"><span class="string">因为当正样本比例减小时，ROC曲线变化不明显，但是P-R曲线的纵坐标，即准确率会出现明显的衰减</span></span><br><span class="line"><span class="string">因为当样本严重倾斜时，我们假定召回率不变，那么表现较差的模型必然会召回更多的负样本，那么FP(假正例)就会迅速增加，准确率就会大幅衰减</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">结论：</span></span><br><span class="line"><span class="string">    一般情况下，模型评估选择P-R或者ROC没啥区别</span></span><br><span class="line"><span class="string">    但是当正样本的个数严重少于负样本个数时，P-R曲线相比较于ROC曲线能够更加直观的表现模型之间的差异，更加合适</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">precision, recall, thresholds = precision_recall_curve(y_true, y_pred)</span><br><span class="line">plt.plot(recall, precision)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 反观ROC曲线</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">真正类率(True Positive Rate)TPR: TP / (TP + FN),代表分类器预测的正类中实际正实例占所有正实例的比例</span></span><br><span class="line"><span class="string">纵轴TPR：TPR越大，预测正类中实际正类越多</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">负正类率(False Positive Rate)FPR: FP / (FP + TN)，代表分类器预测的正类中实际负实例占所有负实例的比例</span></span><br><span class="line"><span class="string">横轴FPR:FPR越大，预测正类中实际负类越多</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">理想目标：TPR=1，FPR=0,即roc图中的(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">FPR, TPR, thresholds_roc = roc_curve(y_true, y_pred)</span><br><span class="line">plt.plot(FPR, TPR, <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # AUC</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ROC曲线下的面积，越大越好</span></span><br><span class="line"><span class="string">首先AUC值是一个概率值，随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值</span></span><br><span class="line"><span class="string">AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AUC score:&#x27;</span>, roc_auc_score(y_true, y_scores))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;AUC score: 0.75&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # KS（Kolmogorov-Smirnov）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过衡量好坏样本累计分布之间的差值，来评估模型的风险区分能力</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ks和AUC一样，都是利用TPR、FPR两个指标来评价模型的整体训练效果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">不同之处在于，ks取的是TPR和FPR差值的最大值，能够找到一个最优的阈值；AUC只评价了模型的整体训练效果，</span></span><br><span class="line"><span class="string">并没有指出如何划分类别让预估的效果达到最好，就是没有找到好的切分阈值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">与PR曲线相比，AUC和KS受样本不均衡的影响较小，而PR受其影响较大</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ks值&lt;0.2,一般认为模型没有区分能力。</span></span><br><span class="line"><span class="string">    ks值[0.2,0.3],模型具有一定区分能力，勉强可以接受</span></span><br><span class="line"><span class="string">    ks值[0.3,0.5],模型具有较强的区分能力。</span></span><br><span class="line"><span class="string">    ks值大于0.75，往往表示模型有异常。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">FPR, TPR, thresholds_ks = roc_curve(y_true, y_pred)</span><br><span class="line">KS = <span class="built_in">abs</span>(FPR - TPR).<span class="built_in">max</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;KS值：&#x27;</span>, KS)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;KS值： 0.5238095238095237&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单扫一眼数据</span></span><br><span class="line">train.info()</span><br><span class="line">train.groupby(<span class="string">&#x27;isDefault&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">可知训练样本数据有80W条，45个特征，大多数特征类型为数值型</span></span><br><span class="line"><span class="string">标签为字段 &#x27;isDefault&#x27;</span></span><br><span class="line"><span class="string">赛题是个二分类问题，同时正负样本比例不均衡，正：负约1：4</span></span><br><span class="line"><span class="string">有缺失数据需要处理</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="金融风控" scheme="http://example.com/categories/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>二手车交易价格预测_Task5_模型融合</title>
    <link href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    <id>http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task5_%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/</id>
    <published>2023-04-24T08:03:14.616Z</published>
    <updated>2023-04-24T08:17:48.167Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型融合-代码示例部分"><a href="#模型融合-代码示例部分" class="headerlink" title="模型融合_代码示例部分"></a>模型融合_代码示例部分</h1><h2 id="导入工具包"><a href="#导入工具包" class="headerlink" title="#导入工具包"></a>#导入工具包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs     <span class="comment"># 这是打包好的波士顿房价数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier     <span class="comment"># 分类决策树模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier     <span class="comment"># 随机森林回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier       <span class="comment"># 分类投票模型</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier        <span class="comment"># xgboost用于解决f分类问题</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression     <span class="comment"># 逻辑回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC     <span class="comment"># 支持向量机模型 - 用于分类问题</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR     <span class="comment"># 支持向量机模型 - 用于回归问题</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split    <span class="comment"># 用于拆分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons     <span class="comment"># 创建月亮形的数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, roc_auc_score   <span class="comment"># ROC-Auc指标,评价模型得分用的</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score     <span class="comment"># 用于做交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV        <span class="comment"># 用于网格搜索</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold     <span class="comment"># 分层交叉验证,每一折中都保持着原始数据中各个类别的比例关系</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier, GradientBoostingClassifier, GradientBoostingRegressor</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> itertools    <span class="comment"># 用于创建自定义的迭代器</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec  <span class="comment"># 用于调整子图的位置大小</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment"># k近邻分类算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB  <span class="comment"># 先验为高斯分布的朴素贝叶斯</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier   <span class="comment"># 快速完成对sklearn模型的stacking</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_learning_curves   <span class="comment"># 绘制学习曲线</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_decision_regions  <span class="comment"># 绘制决策边界</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing       <span class="comment"># 数据归一化(标准化)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA, FastICA, FactorAnalysis, SparsePCA   <span class="comment"># 用于降维等特征处理</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)   <span class="comment"># 忽略警告</span></span><br></pre></td></tr></table></figure><h2 id="简单加权平均-结果直接融合"><a href="#简单加权平均-结果直接融合" class="headerlink" title="# 简单加权平均-结果直接融合"></a># 简单加权平均-结果直接融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">生成一些简单的样本数据,</span></span><br><span class="line"><span class="string">test_prei - 代表第i个模型的预测值</span></span><br><span class="line"><span class="string">y_test_true - 代表真实值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><h2 id="定义结果的加权平均函数-根据加权计算"><a href="#定义结果的加权平均函数-根据加权计算" class="headerlink" title="# 定义结果的加权平均函数 - 根据加权计算"></a># 定义结果的加权平均函数 - 根据加权计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_method</span>(<span class="params">test_pre1, test_pre2, test_pre3, w=[<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>]</span>):</span><br><span class="line">    weighted_result = w[<span class="number">0</span>] * pd.Series(test_pre1) + w[<span class="number">1</span>] * pd.Series(test_pre2) + w[<span class="number">2</span>] * pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> weighted_result</span><br></pre></td></tr></table></figure><h2 id="根据各模型的预测结果计算MAE"><a href="#根据各模型的预测结果计算MAE" class="headerlink" title="# 根据各模型的预测结果计算MAE"></a># 根据各模型的预测结果计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">metrics.mean_absolute_error - 多维数组MAE的计算方法</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred1 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred2 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pred3 MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, test_pre3))</span><br><span class="line"></span><br><span class="line">Pred1 MAE: <span class="number">0.1750000000000001</span></span><br><span class="line">Pred2 MAE: <span class="number">0.07499999999999993</span></span><br><span class="line">Pred3 MAE: <span class="number">0.10000000000000009</span></span><br></pre></td></tr></table></figure><h2 id="根据加权计算MAE"><a href="#根据加权计算MAE" class="headerlink" title="# 根据加权计算MAE"></a># 根据加权计算MAE</h2><h2 id="定义比重权值"><a href="#定义比重权值" class="headerlink" title="## 定义比重权值"></a>## 定义比重权值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]</span><br><span class="line">weighted_pre = weighted_method(test_pre1, test_pre2, test_pre3, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Weighted_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, weighted_pre))</span><br><span class="line"></span><br><span class="line">Weighted_pre MAE: <span class="number">0.05750000000000027</span></span><br></pre></td></tr></table></figure><h3 id="定义结果的加权平均函数-mean平均"><a href="#定义结果的加权平均函数-mean平均" class="headerlink" title="# 定义结果的加权平均函数 - mean平均"></a># 定义结果的加权平均函数 - mean平均</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    mean_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                             pd.Series(test_pre2),</span><br><span class="line">                             pd.Series(test_pre3)], axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mean_result</span><br></pre></td></tr></table></figure><h2 id="根据均值计算MAE"><a href="#根据均值计算MAE" class="headerlink" title="# 根据均值计算MAE"></a># 根据均值计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Mean_pre = mean_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Mean_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Mean_pre MAE: <span class="number">0.06666666666666693</span></span><br></pre></td></tr></table></figure><h2 id="定义结果的加权平均函数-median平均"><a href="#定义结果的加权平均函数-median平均" class="headerlink" title="# 定义结果的加权平均函数 - median平均"></a># 定义结果的加权平均函数 - median平均</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">median_method</span>(<span class="params">test_pre1, test_pre2, test_pre3</span>):</span><br><span class="line">    median_result = pd.concat([pd.Series(test_pre1),</span><br><span class="line">                               pd.Series(test_pre2),</span><br><span class="line">                               pd.Series(test_pre3)], axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> median_result</span><br></pre></td></tr></table></figure><h2 id="根据中位数计算MAE"><a href="#根据中位数计算MAE" class="headerlink" title="# 根据中位数计算MAE"></a># 根据中位数计算MAE</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Median_pre = median_method(test_pre1, test_pre2, test_pre3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Median_pre MAE:&#x27;</span>, metrics.mean_absolute_error(y_test_true, Median_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Median_pre MAE: <span class="number">0.07500000000000007</span></span><br></pre></td></tr></table></figure><h1 id="Stacking融合-回归"><a href="#Stacking融合-回归" class="headerlink" title="# Stacking融合(回归)"></a># Stacking融合(回归)</h1><h2 id="定义Stacking融合函数"><a href="#定义Stacking融合函数" class="headerlink" title="# 定义Stacking融合函数"></a># 定义Stacking融合函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Stacking_method</span>(<span class="params">train_reg1, train_reg2, train_reg3,</span></span><br><span class="line"><span class="params">                    y_train_true,</span></span><br><span class="line"><span class="params">                    test_pre1, test_pre2, test_pre3,</span></span><br><span class="line"><span class="params">                    model_L2=linear_model.LinearRegression(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param train_reg1:  第一个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg2:  第二个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param train_reg3:  第三个模型预测train得到的标签</span></span><br><span class="line"><span class="string">    :param y_train_true:    train真实的标签</span></span><br><span class="line"><span class="string">    :param test_pre1:   第一个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre2:   第二个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param test_pre3:   第三个模型预测test得到的标签</span></span><br><span class="line"><span class="string">    :param model_L2:    次级模型:以真实训练集的标签为标签,以多个模型训练训练集后得到的标签合并后的数据集为特征进行训练</span></span><br><span class="line"><span class="string">                        注意:次级模型不宜选取的太复杂,这样会导致模型在训练集上过拟合,测试集泛化效果差</span></span><br><span class="line"><span class="string">    :return:            训练好的次机模型预测test数据集得到的预测值 - Stacking_result</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), pd.Series(train_reg3)], axis=<span class="number">1</span>).values,</span><br><span class="line">                 y_train_true)      <span class="comment"># 次级模型训练</span></span><br><span class="line">    stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),</span><br><span class="line">                                                  pd.Series(test_pre2), pd.Series(test_pre3)], axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> stacking_result</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值"><a href="#生成一些简单的样本数据-test-prei代表第i个模型的预测值-y-test-true代表模型的真实值" class="headerlink" title="# 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值"></a># 生成一些简单的样本数据,test_prei代表第i个模型的预测值,y_test_true代表模型的真实值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">test_pre1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pre2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pre3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><h2 id="看一下Stacking融合的效果"><a href="#看一下Stacking融合的效果" class="headerlink" title="# 看一下Stacking融合的效果"></a># 看一下Stacking融合的效果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_L2 = linear_model.LinearRegression()      <span class="comment"># 不设定这个参数也可以,创建函数的时候默认了</span></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,</span><br><span class="line">                               test_pre1, test_pre2, test_pre3, model_L2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Stacking_pre MAE: &#x27;</span>, metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Stacking_pre MAE:  <span class="number">0.042134831460675204</span></span><br><span class="line"><span class="comment"># 发现模型效果相对于之前有了更近一步的提升</span></span><br></pre></td></tr></table></figure><h1 id="分类模型融合-Voting-Stacking…"><a href="#分类模型融合-Voting-Stacking…" class="headerlink" title="# 分类模型融合 - Voting,Stacking…"></a># 分类模型融合 - Voting,Stacking…</h1><h2 id="Voting投票机制"><a href="#Voting投票机制" class="headerlink" title="# Voting投票机制"></a># Voting投票机制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Voting - 投票机制</span></span><br><span class="line"><span class="string">        1.硬投票 - 对多个模型直接进行投票,不区分模型结果的相对重要度,最终投票数最多的类为最终被预测的类</span></span><br><span class="line"><span class="string">        2.软投票 - 和硬投票原理相同,增加了设置权重的功能,可以为不同模型设置不同权重,进而区别模型不同的重要度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 硬投票</span></span><br><span class="line">iris = datasets.load_iris()     <span class="comment"># 读取鸢尾花数据集 - 分类问题</span></span><br><span class="line"></span><br><span class="line">x = iris.data   <span class="comment"># 分离特征集和标签</span></span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)    <span class="comment"># 训练集和测试集按照7:3比例切分</span></span><br></pre></td></tr></table></figure><h2 id="用XGB分类模型训练数据"><a href="#用XGB分类模型训练数据" class="headerlink" title="# 用XGB分类模型训练数据"></a># 用XGB分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">colsample_bytree - 训练每棵树时，使用的特征占全部特征的比例</span></span><br><span class="line"><span class="string">objective - 目标函数</span></span><br><span class="line"><span class="string">            二分类问题 - binary:logistic - 返回概率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="用随机森林分类模型训练数据"><a href="#用随机森林分类模型训练数据" class="headerlink" title="# 用随机森林分类模型训练数据"></a># 用随机森林分类模型训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">n_estimators - 随机森林中决策树的个数</span></span><br><span class="line"><span class="string">max_depth - 决策树的最大深度</span></span><br><span class="line"><span class="string">            如果值为None,那么会扩展节点,直到所有的叶子是纯净的,或者直到所有叶子包含少于min_sample_split的样本</span></span><br><span class="line"><span class="string">min_samples_split - 分割内部节点所需要的最小样本数量</span></span><br><span class="line"><span class="string">min_samples_leaf - 需要在叶子结点上的最小样本数量</span></span><br><span class="line"><span class="string">oob_score - 是否使用袋外样本来估计泛化精度</span></span><br><span class="line"><span class="string">            树的生成过程并不会使用所有的样本,未使用的样本就叫(out_of_bag)oob袋外样本,通过袋外样本,可以评估这个树的准确度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="用SVC训练数据"><a href="#用SVC训练数据" class="headerlink" title="# 用SVC训练数据"></a># 用SVC训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">支持向量机 - 分类算法，但是也可以做回归,根据输入的数据不同可做不同的模型</span></span><br><span class="line"><span class="string">            1.若输入标签为连续值则做回归</span></span><br><span class="line"><span class="string">            2.若输入标签为分类值则用SVC()做分类</span></span><br><span class="line"><span class="string">            支持向量机的学习策略是间隔最大化，最终可转化为一个凸二次规划问题的求解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">C - 惩罚参数;   值越大,对误分类的惩罚大,不容犯错,于是训练集测试准确率高,但是泛化能力弱</span></span><br><span class="line"><span class="string">                值越小,对误分类的惩罚小,允许犯错,泛化能力较强</span></span><br><span class="line"><span class="string">probability - 是否采用概率估计,默认为False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h2 id="硬投票"><a href="#硬投票" class="headerlink" title="# 硬投票"></a># 硬投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">eclf - 其实就是三个模型的集成算法,硬投票决定最终被预测的类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)     <span class="comment"># 本质是Ensemble</span></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.05</span>) [Ensemble]</span><br></pre></td></tr></table></figure><h2 id="软投票"><a href="#软投票" class="headerlink" title="# 软投票"></a># 软投票</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.8</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.8</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)</span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;xgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;soft&#x27;</span>, weights=[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;XGBBoosting&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 以准确度度量评分</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %0.2f (+/- %0.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.92</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [Ensemble]</span><br></pre></td></tr></table></figure><h1 id="分类的Stacking-x2F-Blending融合"><a href="#分类的Stacking-x2F-Blending融合" class="headerlink" title="# 分类的Stacking&#x2F;Blending融合"></a># 分类的Stacking&#x2F;Blending融合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Stacking是一种分层模型集成框架,以两层为例</span></span><br><span class="line"><span class="string">        第一层由多个基学习器组成,其输入为原始训练集</span></span><br><span class="line"><span class="string">        第二层的模型则是以第一层学习器的输出作为训练集进行再训练,从而得到完整的stacking模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># ## 创建训练用的数据集</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]  <span class="comment"># 100个样本</span></span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 模型融合中使用到的各个单模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LogisticRegression()</span></span><br><span class="line"><span class="string">            solver - 用来优化权重     &#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;,默认adam,</span></span><br><span class="line"><span class="string">                                        lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好</span></span><br><span class="line"><span class="string">                                        sgd - 随机梯度下降 </span></span><br><span class="line"><span class="string">                                        adam - 机遇随机梯度的优化器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RandomForestClassifier()</span></span><br><span class="line"><span class="string">            n_estimators - 决策树个数</span></span><br><span class="line"><span class="string">            n_jobs - 用于拟合和预测的并行运行的工作数量,如果值为-1,那么工作数量被设置为核的数量</span></span><br><span class="line"><span class="string">            criterion - 衡量分裂质量的性能</span></span><br><span class="line"><span class="string">                        1.gini - Gini impurity衡量的是从一个集合中随机选择一个元素</span></span><br><span class="line"><span class="string">                                基于该集合中标签的概率分布为元素分配标签的错误率</span></span><br><span class="line"><span class="string">                                Gini impurity的计算就非常简单了,即1减去所有分类正确的概率,得到的就是分类不正确的概率</span></span><br><span class="line"><span class="string">                                若元素数量非常多,且所有元素单独属于一个分类时，Gini不纯度达到极小值0</span></span><br><span class="line"><span class="string">                        2.entropy - 信息增益熵</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">ExtraTreesClassifier() - 极端随机树</span></span><br><span class="line"><span class="string">    该算法与随机森林算法十分相似,都是由许多决策树构成,但该算法与随机森林有两点主要的区别:</span></span><br><span class="line"><span class="string">        1.随机森林应用的是Bagging模型,而ET是使用所有的训练样本得到每棵决策树,也就是每棵决策树应用的是相同的全部训练样本</span></span><br><span class="line"><span class="string">            关于Bagging和Boosting的差别,可以参考 https://www.cnblogs.com/earendil/p/8872001.html</span></span><br><span class="line"><span class="string">        2.随机森林是在一个随机子集内得到最佳分叉属性,而ET是完全随机的得到分叉值,从而实现对决策树进行分叉的</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"> Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                            1.lad - 绝对损失</span></span><br><span class="line"><span class="string">                                            2.huber - Huber损失</span></span><br><span class="line"><span class="string">                                            3.quantile - 分位数损失</span></span><br><span class="line"><span class="string">                                            4.ls - 均方差损失(默认)</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">                                    alpha - 当我们使用Huber损失和分位数损失&#x27;quantile&#x27;时,需要指定分位数的值,只有regressor有</span></span><br><span class="line"><span class="string">                                    </span></span><br><span class="line"><span class="string">                            GradientBoostingClassifier() - 参数绝大多数和Regressor相同,不同的是loss函数</span></span><br><span class="line"><span class="string">                                            1.deviance - 对数似然损失函数(默认)</span></span><br><span class="line"><span class="string">                                            2.exponential - 指数损失函数       </span></span><br><span class="line"><span class="string">参考网址: https://www.cnblogs.com/pinard/p/6143927.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((X.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))  <span class="comment"># 全零数组,行取训练集的个数,列取模型个数</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 全零数组,行取测试集的个数,列取模型个数</span></span><br></pre></td></tr></table></figure><h2 id="5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练"><a href="#5折Stacking-即每次Stacking训练都会在第一层基学习器进行5折交叉验证-再进入第二层学习器训练" class="headerlink" title="# 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练"></a># 5折Stacking - 即每次Stacking训练都会在第一层基学习器进行5折交叉验证,再进入第二层学习器训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)     <span class="comment"># # 分层交叉验证,每一折中都保持着原始数据中各个类别的比例关系(测试集和训练集分离)</span></span><br><span class="line">skf = skf.split(X, y)     <span class="comment"># 把特征和标签分离</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">enumerate() - 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列,同时列出数据和数据下标,一般用在for循环当中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))    <span class="comment"># 30行5列的全0数组</span></span><br><span class="line">    <span class="comment"># 五折交叉训练,使用第i个部分作为预测集,剩余部分为验证集,获得的预测值成为第i部分的新特征</span></span><br><span class="line">    <span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf):</span><br><span class="line">        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        <span class="comment"># 将对测试集的概率预测第二列(也就是结果为1)的概率装进y_submission中</span></span><br><span class="line">        y_submission = clf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">        dataset_blend_train[test, j] = y_submission     <span class="comment"># 把预测验证集(比如第一折)的结果依次对应装进dataset_blend_train中</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        predict_proba() - 返回的是一个n行k列的数组</span></span><br><span class="line"><span class="string">                            第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率,并且每一行的概率和为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        因为我们采取到的数据集的标签只有0或1,所以predict_proba返回的概率只有两个</span></span><br><span class="line"><span class="string">                    如果左边的概率大于0.5,那么预测值为0</span></span><br><span class="line"><span class="string">                    如果右边的概率大于0.5,那么预测值为1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># # 将对测试集的概率预测的第二列(也就是结果为1)的概率装进dataset_blend_test_j中</span></span><br><span class="line">        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对于测试集,直接用这5个模型的预测值均值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = dataset_blend_test_j.mean(<span class="number">1</span>)     <span class="comment"># mean(1) - 求每行数的平均值(五折预测测试集的平均值)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>)    <span class="comment"># 次级学习器再次训练</span></span><br><span class="line">clf.fit(dataset_blend_train, y)     <span class="comment"># 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_blend_test)[:, <span class="number">1</span>]  <span class="comment"># 把第一层预测测试集的结果作为新特征,预测测试集的标签</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ROC曲线和AUC - 用来评价一个二值分类器(binary classifier)的优劣,用于衡量&#x27;二分类问题&#x27;机器学习算法性能(泛化能力)</span></span><br><span class="line"><span class="string">AUC - ROC曲线下的面积</span></span><br><span class="line"><span class="string">      AUC的取值范围在0.5和1之间</span></span><br><span class="line"><span class="string">      使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好</span></span><br><span class="line"><span class="string">      而作为一个数值,对应AUC更大的分类器效果更好</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure><h2 id="Blending-和Stacking类似-不同点在于"><a href="#Blending-和Stacking类似-不同点在于" class="headerlink" title="# Blending - 和Stacking类似,不同点在于:"></a># Blending - 和Stacking类似,不同点在于:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.Stacking - 把第一层得到训练集的预测结果作为新特征,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string">2.Blending - 把第一层得到训练集中的30%的验证集的结果作为新特征继续训练,把训练集的真实标签作为标签,进行第二层训练</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Blending优点 - 比stacking简单,因为不用进行k次的交叉验证来获得stacker feature</span></span><br><span class="line"><span class="string">                避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集</span></span><br><span class="line"><span class="string">Blending缺点- 使用了很少的数据,可能会过拟合,没有stacking使用多次的交叉验证来的稳健</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把训练数据分成d1(子训练集),d2(验证集)两部分 - 对半分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))   <span class="comment"># 35行5列的全0数组</span></span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs)))      <span class="comment"># 30行5列的全0数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment"># 用子训练集依次训练各个模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    <span class="comment"># 返回模型对验证集的预测值为1的概率</span></span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 结果装进dataset_d1中 - 表示用子训练集训练的模型预测验证集标签的结果 - 就是上文说的30%的数据</span></span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment"># 建立第二层模型的特征 - 用第一层模型预测测试集的结果作为新的特征</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 看一下预测的预测集标签和真实的预测集标签的roc_auc_score</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val auc Score: %f&#x27;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用第二层模型训练特征</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)   <span class="comment"># 用验证集的第一层模型预测结果作为特征,用验证集的真实标签作为标签,再次训练</span></span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, <span class="number">1</span>]  <span class="comment"># 用第一层模型预测测试集的结果作为特征,用第二层模型预测训练集返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Blending: %f&#x27;</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">Val auc Score of Blending: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure><h2 id="利用mlxtend进行分类的Stacking融合"><a href="#利用mlxtend进行分类的Stacking融合" class="headerlink" title="# 利用mlxtend进行分类的Stacking融合"></a># 利用mlxtend进行分类的Stacking融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, <span class="number">1</span>:<span class="number">3</span>], iris.target</span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=<span class="number">1</span>)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">StackingClassifier() - 快速Stacking融合的方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">classifiers - 一级分类器列表</span></span><br><span class="line"><span class="string">meta_classifier - 二级分类器(元分类器)</span></span><br><span class="line"><span class="string">use_probas - 如果为True,则基于预测的概率而不是类标签来训练元分类器,默认为False</span></span><br><span class="line"><span class="string">average_probas - 如果为真,将概率平均为元特征,默认为False</span></span><br><span class="line"><span class="string">verbose - 是否输出到日志</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                          meta_classifier=lr)</span><br><span class="line">label = [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>, <span class="string">&#x27;Stacking Classifier&#x27;</span>]</span><br><span class="line">clf_list = [clf1, clf2, clf3, sclf]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># 网格布局,每行2个,每列2个</span></span><br><span class="line">grid = itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)  <span class="comment"># 求多个可迭代对象的笛卡尔积,其实就是更加灵活调整网格的大小</span></span><br><span class="line"></span><br><span class="line">clf_cv_mean = []    <span class="comment"># 存放每个模型的准确率的均值</span></span><br><span class="line">clf_cv_std = []     <span class="comment"># 存放每个模型的准确率的标准差</span></span><br><span class="line"><span class="keyword">for</span> clf, label, grd <span class="keyword">in</span> <span class="built_in">zip</span>(clf_list, label, grid):</span><br><span class="line"></span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)   <span class="comment"># 3折交叉验证,评分标准为模型准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.2f (+/- %.2f) [%s]&#x27;</span> % (scores.mean(), scores.std(), label))</span><br><span class="line">    clf_cv_mean.append(scores.mean())</span><br><span class="line">    clf_cv_std.append(scores.std())</span><br><span class="line"></span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(label)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.02</span>) [Stacking Classifier]</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020040420565248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看出,融合后的曲线更加优秀</p><h2 id="一些其它方法"><a href="#一些其它方法" class="headerlink" title="# 一些其它方法"></a># 一些其它方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将特征放进模型中预测,并将预测结果变换并作为新的特征加入原有特征中,再经过模型预测结果(Stacking变化)</span></span><br><span class="line"><span class="string">可以反复预测多次将结果加入最后的特征中</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ensemble_add_feature</span>(<span class="params">train, test, target, clfs</span>):</span><br><span class="line">    <span class="comment"># n_folds = 5</span></span><br><span class="line">    <span class="comment"># skf = list(StratifiedKFold(y, n_folds=n_folds))</span></span><br><span class="line">    train_ = np.zeros((train.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    test_ = np.zeros((test.shape[<span class="number">0</span>], <span class="built_in">len</span>(clfs * <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="comment"># 依次训练单个模型</span></span><br><span class="line">        <span class="built_in">print</span>(j, clf)</span><br><span class="line">        <span class="comment"># 使用第1部分作为预测,第2部分来训练模型(第1部分预测的输出作为第2部分的新特征)</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train]</span></span><br><span class="line">        clf.fit(train, target)  <span class="comment"># 训练模型</span></span><br><span class="line">        y_train = clf.predict(train)    <span class="comment"># 模型在训练集中的预测值</span></span><br><span class="line">        y_test = clf.predict(test)      <span class="comment"># 模型在测试集中的预测值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成新特征</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        j 从0开始递增,构建新的特征集,特征为训练集和测试集各自的预测值的平方</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_[:, j*<span class="number">2</span>] = y_train ** <span class="number">2</span></span><br><span class="line">        test_[:, j*<span class="number">2</span>] = y_test ** <span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)    <span class="comment"># np.exp(a) - 返回e的a次方</span></span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method:&#x27;</span>, j)</span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf = LogisticRegression()  <span class="comment"># 次级模型</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.3</span>)</span><br><span class="line">x_train = pd.DataFrame(x_train)     <span class="comment"># 转换成DataFrame格式,方便后续构造新特征</span></span><br><span class="line">x_test = pd.DataFrame(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给出模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 新特征的构造 - 用上面的各个单模型预测训练集和测试集的结果,作为新特征</span></span><br><span class="line">New_train, New_test = ensemble_add_feature(x_train, x_test, y_train, clfs)</span><br><span class="line">clf.fit(New_train, y_train)     <span class="comment"># 用训练集的新特征和训练集的真实标签训练数据</span></span><br><span class="line">y_emb = clf.predict_proba(New_test)[:, <span class="number">1</span>]   <span class="comment"># 用训练好的模型得到新的测试集特征返回1的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val auc Score of Stacking: %f&#x27;</span> % (roc_auc_score(y_test, y_emb)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method: <span class="number">4</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure><h1 id="本赛题示例"><a href="#本赛题示例" class="headerlink" title="本赛题示例"></a>本赛题示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据读取</span></span><br><span class="line">Train_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\used_car_train_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">TestA_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\used_car_testA_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(TestA_data.shape)</span><br></pre></td></tr></table></figure><h2 id="回顾一下数据"><a href="#回顾一下数据" class="headerlink" title="# 回顾一下数据"></a># 回顾一下数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Train_data.head()</span><br><span class="line">Train_data.info()   <span class="comment"># 20个浮点类型字段,10个整数型字段,1个object字段</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过info(),训练集发现一共有4个字段有缺失值</span></span><br><span class="line"><span class="string">                          model字段有1个缺失值</span></span><br><span class="line"><span class="string">                          bodyType字段有4506个缺失值</span></span><br><span class="line"><span class="string">                          fuelType字段有8680个缺失值</span></span><br><span class="line"><span class="string">                          gearbox字段有5981个缺失值            </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们把数值型字段和非数值型字段做一个区分</span></span><br><span class="line">numerical_cols = Train_data.select_dtypes(exclude=<span class="string">&#x27;object&#x27;</span>).columns     <span class="comment"># 这是所有数值类型的字段</span></span><br><span class="line"><span class="built_in">print</span>(numerical_cols)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其中SaleID(合同号),name(汽车交易名称),regDate(汽车注册时间)三个字段看似没有用处,暂时抛弃这些特征</span></span><br><span class="line"><span class="comment"># price是标签字段,不放在特征集中</span></span><br><span class="line">feature_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> numerical_cols <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;SaleID&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;regDate&#x27;</span>, <span class="string">&#x27;price&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 区分特征集和标签集</span></span><br><span class="line">X_data = Train_data[feature_cols]</span><br><span class="line">Y_data = Train_data[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">X_test = TestA_data[feature_cols]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X train shape:&#x27;</span>, X_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X test shape:&#x27;</span>, X_test.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X train shape: (<span class="number">150000</span>, <span class="number">26</span>)</span><br><span class="line">X test shape: (<span class="number">50000</span>, <span class="number">26</span>)</span><br></pre></td></tr></table></figure><h2 id="创建一个统计函数-方便后续信息统计"><a href="#创建一个统计函数-方便后续信息统计" class="headerlink" title="# 创建一个统计函数,方便后续信息统计"></a># 创建一个统计函数,方便后续信息统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sta_inf</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_min:&#x27;</span>, np.<span class="built_in">min</span>(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_max:&#x27;</span>, np.<span class="built_in">max</span>(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_mean:&#x27;</span>, np.mean(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_ptp:&#x27;</span>, np.ptp(data))    <span class="comment"># ptp - 最大值和最小值的差值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_std:&#x27;</span>, np.std(data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;_var:&#x27;</span>, np.var(data))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sta of label&#x27;</span>, sta_inf(Y_data))      <span class="comment"># 看一下训练集标签的统计分布</span></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: <span class="number">11</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">99999</span></span><br><span class="line">_mean: <span class="number">5923.327333333334</span></span><br><span class="line">_ptp: <span class="number">99988</span></span><br><span class="line">_std: <span class="number">7501.973469876438</span></span><br><span class="line">_var: <span class="number">56279605.94272992</span></span><br></pre></td></tr></table></figure><h2 id="简单填补训练集和测试集的缺失值-用-1填充"><a href="#简单填补训练集和测试集的缺失值-用-1填充" class="headerlink" title="# 简单填补训练集和测试集的缺失值 - 用-1填充"></a># 简单填补训练集和测试集的缺失值 - 用-1填充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_data = X_data.fillna(-<span class="number">1</span>)</span><br><span class="line">X_test = X_test.fillna(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="为了代码简洁-创建好模型训练函数"><a href="#为了代码简洁-创建好模型训练函数" class="headerlink" title="# 为了代码简洁,创建好模型训练函数"></a># 为了代码简洁,创建好模型训练函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 线性回归</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_lr</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    reg_model = linear_model.LinearRegression()</span><br><span class="line">    reg_model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> reg_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## Ridge 岭回归 - 加入了l2正则化的线性回归</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">L2正则化 - 岭回归 - 模型被限制在圆形区域(二维区域下),损失函数的最小值因为圆形约束没有角,所以不会使得权重为0,但是可以使得权重</span></span><br><span class="line"><span class="string">                    都尽可能的小,最后得到一个所有参数都比较小的模型,这样模型比较简单,能适应不同数据集,一定程度上避免了过拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_ridge</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    reg_model = linear_model.Ridge(alpha=<span class="number">0.8</span>)    <span class="comment"># alpha - 正则化系数</span></span><br><span class="line">    reg_model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> reg_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## Lasso回归 - 加入了l1正则化的线性回归</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">L1正则化 - Lasso回归 - 模型被限制在正方形区域(二维区域下),损失函数的最小值往往在正方形(约束)的角上,很多权值为0(多维),所以可以</span></span><br><span class="line"><span class="string">                        实现模型的稀疏性(生成稀疏权值矩阵,进而用于特征选择</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_lasso</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    reg_model = linear_model.LassoCV()</span><br><span class="line">    reg_model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> reg_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## gbdt -梯度下降树 - 传统机器学习算法里面是对真实分布拟合的最好的几种算法之一</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Boosting算法思想 -  一堆弱分类器的组合就可以成为一个强分类器;</span></span><br><span class="line"><span class="string">                    不断地在错误中学习，迭代来降低犯错概率</span></span><br><span class="line"><span class="string">                    通过一系列的迭代来优化分类结果,每迭代一次引入一个弱分类器,来克服现在已经存在的弱分类器组合的短板</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Adaboost      - 整个训练集上维护一个分布权值向量W</span></span><br><span class="line"><span class="string">                        用赋予权重的训练集通过弱分类算法产生分类假设（基学习器）y(x)</span></span><br><span class="line"><span class="string">                        然后计算错误率,用得到的错误率去更新分布权值向量w</span></span><br><span class="line"><span class="string">                        对错误分类的样本分配更大的权值,正确分类的样本赋予更小的权值</span></span><br><span class="line"><span class="string">                        每次更新后用相同的弱分类算法产生新的分类假设,这些分类假设的序列构成多分类器</span></span><br><span class="line"><span class="string">                        对这些多分类器用加权的方法进行联合,最后得到决策结果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">参考资料:https://www.cnblogs.com/zhubinwang/p/5170087.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_gbdt</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    estimator = GradientBoostingRegressor(loss=<span class="string">&#x27;ls&#x27;</span>, subsample=<span class="number">0.85</span>, max_depth=<span class="number">5</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line">    param_grid = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.08</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;</span><br><span class="line">    gbdt = GridSearchCV(estimator, param_grid, cv=<span class="number">3</span>)    <span class="comment"># 网格搜索最佳参数,3折交叉检验</span></span><br><span class="line">    gbdt.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(gbdt.best_params_)    <span class="comment"># 输出最佳参数</span></span><br><span class="line">    <span class="built_in">print</span>(gbdt.best_estimator_)</span><br><span class="line">    <span class="keyword">return</span> gbdt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## xgb - 梯度提升决策树</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">XGBRegressor - 梯度提升回归树,也叫梯度提升机</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                采用连续的方式构造树,每棵树都试图纠正前一棵树的错误</span></span><br><span class="line"><span class="string">                与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝</span></span><br><span class="line"><span class="string">                从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快</span></span><br><span class="line"><span class="string">                </span></span><br><span class="line"><span class="string">                gamma - 定了节点分裂所需的最小损失函数下降值,这个参数的值越大,算法越保守</span></span><br><span class="line"><span class="string">                subsample - 这个参数控制对于每棵树随机采样的比例,减小这个参数的值,算法会更加保守,避免过拟合</span></span><br><span class="line"><span class="string">                colsample_bytree - 用来控制每棵随机采样的列数的占比</span></span><br><span class="line"><span class="string">                learning_rate - 学习速率,用于控制树的权重,xgb模型在进行完每一轮迭代之后,会将叶子节点的分数乘上该系数,</span></span><br><span class="line"><span class="string">                                以便于削弱各棵树的影响,避免过拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_xgb</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    model = xgb.XGBRegressor(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.08</span>, gamma=<span class="number">0</span>,</span><br><span class="line">                             subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">    model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## lgb - xgb加强版</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LightGBM - 使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低</span></span><br><span class="line"><span class="string">            思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram</span></span><br><span class="line"><span class="string">            然后遍历训练数据，统计每个离散值在直方图中的累计统计量</span></span><br><span class="line"><span class="string">            在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            LightGBM采用leaf-wise生长策略:</span></span><br><span class="line"><span class="string">            每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。</span></span><br><span class="line"><span class="string">            因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度</span></span><br><span class="line"><span class="string">            Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合</span></span><br><span class="line"><span class="string">            因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合</span></span><br><span class="line"><span class="string">参数:</span></span><br><span class="line"><span class="string">num_leaves - 控制了叶节点的数目,它是控制树模型复杂度的主要参数,取值应 &lt;= 2 ^（max_depth）</span></span><br><span class="line"><span class="string">bagging_fraction - 每次迭代时用的数据比例,用于加快训练速度和减小过拟合</span></span><br><span class="line"><span class="string">feature_fraction - 每次迭代时用的特征比例,例如为0.8时,意味着在每次迭代中随机选择80％的参数来建树,</span></span><br><span class="line"><span class="string">                    boosting为random forest时用</span></span><br><span class="line"><span class="string">min_data_in_leaf - 每个叶节点的最少样本数量。</span></span><br><span class="line"><span class="string">                    它是处理leaf-wise树的过拟合的重要参数</span></span><br><span class="line"><span class="string">                    将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合</span></span><br><span class="line"><span class="string">max_depth - 控制了树的最大深度,该参数可以显式的限制树的深度</span></span><br><span class="line"><span class="string">n_estimators - 分多少颗决策树(总共迭代的次数)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">objective - 问题类型</span></span><br><span class="line"><span class="string">            regression - 回归任务,使用L2损失函数</span></span><br><span class="line"><span class="string">            regression_l1 - 回归任务,使用L1损失函数</span></span><br><span class="line"><span class="string">            huber - 回归任务,使用huber损失函数</span></span><br><span class="line"><span class="string">            fair - 回归任务,使用fair损失函数</span></span><br><span class="line"><span class="string">            mape (mean_absolute_precentage_error) - 回归任务,使用MAPE损失函数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_lgb</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    estimator = lgb.LGBMRegressor(num_leaves=<span class="number">63</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line">    param_grid = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]&#125;</span><br><span class="line">    gbm = GridSearchCV(estimator, param_grid)</span><br><span class="line">    gbm.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(gbm.best_params_)</span><br><span class="line">    <span class="keyword">return</span> gbm</span><br></pre></td></tr></table></figure><h2 id="XGB的五折交叉回归验证-这里只是举例-别的模型也可以这么验证"><a href="#XGB的五折交叉回归验证-这里只是举例-别的模型也可以这么验证" class="headerlink" title="# XGB的五折交叉回归验证 - 这里只是举例,别的模型也可以这么验证"></a># XGB的五折交叉回归验证 - 这里只是举例,别的模型也可以这么验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">xgr = xgb.XGBRegressor(n_estimators=<span class="number">120</span>, learning_rate=<span class="number">0.1</span>, subsample=<span class="number">0.8</span>, colsample_bytree=<span class="number">0.9</span>, max_depth=<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scores_train = []   <span class="comment"># 每次模型训练训练集中子训练集的得分</span></span><br><span class="line">scores = []         <span class="comment"># 每次模型训练训练集中验证集的得分</span></span><br><span class="line"></span><br><span class="line">sk = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">0</span>)  <span class="comment"># shuffle判断是否在每次抽样时对样本进行清洗</span></span><br><span class="line"><span class="keyword">for</span> train_ind, val_ind <span class="keyword">in</span> sk.split(X_data, Y_data):</span><br><span class="line">    train_x = X_data.iloc[train_ind].values</span><br><span class="line">    train_y = Y_data.iloc[train_ind]</span><br><span class="line">    val_x = X_data.iloc[val_ind].values</span><br><span class="line">    val_y = Y_data.iloc[val_ind]</span><br><span class="line"></span><br><span class="line">    xgr.fit(train_x, train_y)</span><br><span class="line">    pred_train_xgb = xgr.predict(train_x)   <span class="comment"># 子训练集的预测值</span></span><br><span class="line">    pre_xgb = xgr.predict(val_x)            <span class="comment"># 验证集的预测值</span></span><br><span class="line"></span><br><span class="line">    score_train = mean_absolute_error(train_y, pred_train_xgb)</span><br><span class="line">    scores_train.append(score_train)       <span class="comment"># 统计子训练集的mae</span></span><br><span class="line"></span><br><span class="line">    score = mean_absolute_error(val_y, pre_xgb)</span><br><span class="line">    scores.append(score)                    <span class="comment"># 统计验证集的mae</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train mae:&#x27;</span>, np.mean(scores_train))  <span class="comment"># 统计mae均值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Val mae:&#x27;</span>, np.mean(scores))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为跑的很慢,所以记录一下这次的结果<br>Train mae: 596.3128886185606<br>Val mae: 693.382067947197</p><h2 id="划分数据集-并用多种方法训练和预测"><a href="#划分数据集-并用多种方法训练和预测" class="headerlink" title="# 划分数据集,并用多种方法训练和预测"></a># 划分数据集,并用多种方法训练和预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_val, y_train, y_val = train_test_split(X_data, Y_data, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 训练并预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict LR...&#x27;</span>)</span><br><span class="line">model_lr = build_model_lr(x_train, y_train)</span><br><span class="line">val_lr = model_lr.predict(x_val)    <span class="comment"># 得到验证集的预测值</span></span><br><span class="line">subA_lr = model_lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict Ridge...&#x27;</span>)</span><br><span class="line">model_ridge = build_model_ridge(x_train, y_train)</span><br><span class="line">val_ridge = model_ridge.predict(x_val)</span><br><span class="line">subA_ridge = model_ridge.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict Lasso...&#x27;</span>)</span><br><span class="line">model_lasso = build_model_lasso(x_train, y_train)</span><br><span class="line">val_lasso = model_lasso.predict(x_val)</span><br><span class="line">subA_lasso = model_lasso.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict GBDT...&#x27;</span>)</span><br><span class="line">model_gbdt = build_model_gbdt(x_train, y_train)</span><br><span class="line">val_gbdt = model_gbdt.predict(x_val)</span><br><span class="line">subA_gbdt = model_gbdt.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Predict LR...</span><br><span class="line">Predict Ridge...</span><br><span class="line">Predict Lasso...</span><br><span class="line">Predict GBDT...</span><br><span class="line">&#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.2</span>&#125;</span><br></pre></td></tr></table></figure><p>因为GBDT加了网格搜索调参,所以跑的特别慢…</p><h2 id="一般比赛中效果最为显著的两种方法-XGB-x2F-LGB"><a href="#一般比赛中效果最为显著的两种方法-XGB-x2F-LGB" class="headerlink" title="# 一般比赛中效果最为显著的两种方法 - XGB&#x2F;LGB"></a># 一般比赛中效果最为显著的两种方法 - XGB&#x2F;LGB</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict XGB...&#x27;</span>)</span><br><span class="line">model_xgb = build_model_xgb(x_train, y_train)</span><br><span class="line">val_xgb = model_xgb.predict(x_val)</span><br><span class="line">subA_xgb = model_xgb.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict LGB...&#x27;</span>)</span><br><span class="line">model_lgb = build_model_lgb(x_train, y_train)</span><br><span class="line">val_lgb = model_lgb.predict(x_val)</span><br><span class="line">subA_lgb = model_lgb.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 看一下lgb模型预测测试集的数据统计性分布</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sta inf of lgb:&#x27;</span>, sta_inf(subA_lgb))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: -<span class="number">76.40857341373585</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">88951.1857499891</span></span><br><span class="line">_mean: <span class="number">5926.870646241635</span></span><br><span class="line">_ptp: <span class="number">89027.59432340284</span></span><br><span class="line">_std: <span class="number">7379.389534056081</span></span><br><span class="line">_var: <span class="number">54455389.89533643</span></span><br></pre></td></tr></table></figure><h2 id="加权融合-简单加权平均"><a href="#加权融合-简单加权平均" class="headerlink" title="# 加权融合 - 简单加权平均"></a># 加权融合 - 简单加权平均</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 使用我们定义过的加权平均函数weighted_method</span></span><br><span class="line"><span class="comment"># ## 设置权重</span></span><br><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]</span><br><span class="line"><span class="comment"># ## 预测验证集的准确度 - 三个模型加权融合</span></span><br><span class="line">val_pre = weighted_method(val_lgb, val_xgb, val_gbdt, w)</span><br><span class="line">MAE_Weighted = mean_absolute_error(y_val, val_pre)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of Weighted of val:&#x27;</span>, MAE_Weighted)</span><br><span class="line"><span class="comment"># 预测测试集的加权融合结果</span></span><br><span class="line">subA = weighted_method(subA_lgb, subA_xgb, subA_gbdt, w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAE of Weighted of val: <span class="number">724.2198039000299</span></span><br><span class="line"><span class="comment"># ## 看一下预测测试集融合后的结果统计分布</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sta inf:&#x27;</span>, sta_inf(subA))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: -<span class="number">878.0418803904766</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">88761.70460429232</span></span><br><span class="line">_mean: <span class="number">5931.043037628969</span></span><br><span class="line">_ptp: <span class="number">89639.7464846828</span></span><br><span class="line">_std: <span class="number">7367.855733884833</span></span><br><span class="line">_var: <span class="number">54285298.11533961</span></span><br><span class="line"><span class="comment"># ## 生成提交文件</span></span><br><span class="line">sub = pd.DataFrame()</span><br><span class="line">sub[<span class="string">&#x27;SaleID&#x27;</span>] = X_test.index</span><br><span class="line">sub[<span class="string">&#x27;price&#x27;</span>] = subA</span><br><span class="line">sub.to_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\sub_Weighted.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ##与简单的lr(线性回归)进行对比</span></span><br><span class="line">val_lr_pred = model_lr.predict(x_val)</span><br><span class="line">MAE_lr = mean_absolute_error(y_val, val_lr_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of lr:&#x27;</span>, MAE_lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lr_MAE:<span class="number">2588.26</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure><p>weighted_MAE:724.84    —&gt; 能看出,对比lr,加权融合之后模型的精确度有了非常大的提高</p><h2 id="Stacking融合"><a href="#Stacking融合" class="headerlink" title="# Stacking融合"></a># Stacking融合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ## 第一层,得到各个单模型预测子训练集的预测值</span></span><br><span class="line">train_lgb_pred = model_lgb.predict(x_train)</span><br><span class="line">train_xgb_pred = model_xgb.predict(x_train)</span><br><span class="line">train_gbdt_pred = model_gbdt.predict(x_train)</span><br><span class="line"><span class="comment"># ## 得到各个单模型预测验证集的预测值 --&gt; 前面预测过了,直接拿来用</span></span><br><span class="line"><span class="comment"># ## 得到各个单模型预测测试集的预测值 --&gt; 前面也预测过了,直接拿来用</span></span><br><span class="line"><span class="comment"># ## 创建新特征</span></span><br><span class="line"></span><br><span class="line">Stark_X_train = pd.DataFrame()  <span class="comment"># 由子训练集的预测数据得到的新特征</span></span><br><span class="line">Stark_X_train[<span class="string">&#x27;Method_1&#x27;</span>] = train_lgb_pred</span><br><span class="line">Stark_X_train[<span class="string">&#x27;Method_2&#x27;</span>] = train_xgb_pred</span><br><span class="line">Stark_X_train[<span class="string">&#x27;Method_3&#x27;</span>] = train_gbdt_pred</span><br><span class="line"></span><br><span class="line">Stark_X_val = pd.DataFrame()    <span class="comment"># 由验证集的预测数据得到的新特征</span></span><br><span class="line">Stark_X_val[<span class="string">&#x27;Method_1&#x27;</span>] = val_lgb</span><br><span class="line">Stark_X_val[<span class="string">&#x27;Method_2&#x27;</span>] = val_xgb</span><br><span class="line">Stark_X_val[<span class="string">&#x27;Method_3&#x27;</span>] = val_gbdt</span><br><span class="line"></span><br><span class="line">Stark_X_test = pd.DataFrame()   <span class="comment"># 由测试集的预测数据得到的新特征</span></span><br><span class="line">Stark_X_test[<span class="string">&#x27;Method_1&#x27;</span>] = subA_lgb</span><br><span class="line">Stark_X_test[<span class="string">&#x27;Method_2&#x27;</span>] = subA_xgb</span><br><span class="line">Stark_X_test[<span class="string">&#x27;Method_3&#x27;</span>] = subA_gbdt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 第二层模型训练 - 元模型得简单一点,这里用线性回归</span></span><br><span class="line">model_lr_Stacking = build_model_lr(Stark_X_train, y_train)      <span class="comment"># 用子训练集的新特征和标签训练</span></span><br><span class="line"><span class="comment"># ## 看模型在子训练集上的表现</span></span><br><span class="line">train_pre_Stacking = model_lr_Stacking.predict(Stark_X_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of Stacking-LR&#x27;</span>, mean_absolute_error(y_train, train_pre_Stacking))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAE of Stacking-LR <span class="number">630.5897643683384</span></span><br><span class="line"><span class="comment"># ## 看模型在验证集上的表现</span></span><br><span class="line">val_pre_Stacking = model_lr_Stacking.predict(Stark_X_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE of Stacking-LR:&#x27;</span>, mean_absolute_error(y_val, val_pre_Stacking))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAE of Stacking-LR: <span class="number">722.2460136359339</span></span><br><span class="line"><span class="comment"># ## 预测测试集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict Stacking-LR...&#x27;</span>)</span><br><span class="line">subA_Stacking = model_lr_Stacking.predict(Stark_X_test)</span><br></pre></td></tr></table></figure><h2 id="我们看一下预测值的统计性分布"><a href="#我们看一下预测值的统计性分布" class="headerlink" title="# 我们看一下预测值的统计性分布"></a># 我们看一下预测值的统计性分布</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sta_inf(subA_Stacking)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_<span class="built_in">min</span>: -<span class="number">3638.174875620137</span></span><br><span class="line">_<span class="built_in">max</span>: <span class="number">90457.25117154507</span></span><br><span class="line">_mean: <span class="number">5926.428386169822</span></span><br><span class="line">_ptp: <span class="number">94095.42604716521</span></span><br><span class="line">_std: <span class="number">7417.358806265995</span></span><br><span class="line">_var: <span class="number">55017211.6608917</span></span><br><span class="line"><span class="comment"># 有负值,我们去掉过小的预测值 - 预测值小于10的替换成10</span></span><br><span class="line">subA_Stacking[subA_Stacking &lt; <span class="number">10</span>] = <span class="number">10</span></span><br><span class="line"><span class="comment"># 生成结果</span></span><br><span class="line">sub = pd.DataFrame()</span><br><span class="line">sub[<span class="string">&#x27;SaleID&#x27;</span>] = TestA_data.SaleID</span><br><span class="line">sub[<span class="string">&#x27;price&#x27;</span>] = subA_Stacking</span><br><span class="line">sub.to_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\sub_Stacking.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交两个结果之后,发现使用Stacking融合比使用加权融合效果好,但是还没有第一次baseline的分数理想</span></span><br></pre></td></tr></table></figure><p>最后完整的比赛打算尝试从这些方面着重入手:<br>0.调整数据类型,压缩数据大小<br>1.测试集和训练集都有缺失值<br>2.notRepairedDamage有异常值,处理完记得转换成数值型特征<br>3.训练集标签分布呈现长尾分布,截断或log变换<br>4.箱线图处理训练集特征的异常数据<br>5.构造特征:使用时间creatDate-regDate<br>errors&#x3D;’coerce’将不能转换的值变成nan,然后尝试用XGBOOST预测填补缺失值<br>6.邮编提取城市信息(二手车价格和城市可能有关)<br>7.根据品牌的销售统计量构造统计量特征<br>8.数据分桶 - 看下哪些特征是连续型的,变成离散型<br>9.使用SFS挑选最优特征<br>10.调参 - 贝叶斯调参<br>11.xgb,lgb加权融合</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;模型融合-代码示例部分&quot;&gt;&lt;a href=&quot;#模型融合-代码示例部分&quot; class=&quot;headerlink&quot; title=&quot;模型融合_代码示例部分&quot;&gt;&lt;/a&gt;模型融合_代码示例部分&lt;/h1&gt;&lt;h2 id=&quot;导入工具包&quot;&gt;&lt;a href=&quot;#导入工具包&quot; class</summary>
      
    
    
    
    <category term="二手车交易价格预测" scheme="http://example.com/categories/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>二手车交易价格预测_Task4_建模与调参</title>
    <link href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/"/>
    <id>http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task4_%E5%BB%BA%E6%A8%A1%E4%B8%8E%E8%B0%83%E5%8F%82/</id>
    <published>2023-04-24T08:03:04.807Z</published>
    <updated>2023-04-24T08:17:38.985Z</updated>
    
    <content type="html"><![CDATA[<h1 id="建模与调参-代码示例部分"><a href="#建模与调参-代码示例部分" class="headerlink" title="建模与调参_代码示例部分"></a>建模与调参_代码示例部分</h1><h2 id="导入工具包"><a href="#导入工具包" class="headerlink" title="# 导入工具包"></a># 导入工具包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)   <span class="comment"># 代码可以正常运行但是会提示警告,很烦人,有了这行代码就能忽略警告了</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)      <span class="comment"># 显示所有列</span></span><br></pre></td></tr></table></figure><h2 id="创建一个reduce-mem-usage函数-通过调整数据类型-减少数据在内存中占用的空间"><a href="#创建一个reduce-mem-usage函数-通过调整数据类型-减少数据在内存中占用的空间" class="headerlink" title="# 创建一个reduce_mem_usage函数,通过调整数据类型,减少数据在内存中占用的空间"></a># 创建一个reduce_mem_usage函数,通过调整数据类型,减少数据在内存中占用的空间</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem_usage</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>()     <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is&#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(start_mem))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="built_in">object</span>:      <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:      <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:                               <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>()   <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.<span class="built_in">format</span>(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * (start_mem - end_mem) / start_mem))   <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h2 id="读取我们为树模型准备的特征数据-并且将数据压缩"><a href="#读取我们为树模型准备的特征数据-并且将数据压缩" class="headerlink" title="# 读取我们为树模型准备的特征数据,并且将数据压缩"></a># 读取我们为树模型准备的特征数据,并且将数据压缩</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = reduce_mem_usage(pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\data_for_tree.csv&#x27;</span>))</span><br><span class="line"></span><br><span class="line">Memory usage of dataframe is60507376<span class="number">.00</span> MB</span><br><span class="line">Memory usage after optimization <span class="keyword">is</span> <span class="number">15724155.00</span> MB</span><br><span class="line">Decreased by <span class="number">74.0</span>%</span><br></pre></td></tr></table></figure><p>数据内存大小压缩了74%,看来效果拔群</p><h2 id="把我们需要用的特征挑出-仅仅是列名-用来分离特征集和标签集"><a href="#把我们需要用的特征挑出-仅仅是列名-用来分离特征集和标签集" class="headerlink" title="# 把我们需要用的特征挑出(仅仅是列名,用来分离特征集和标签集)"></a># 把我们需要用的特征挑出(仅仅是列名,用来分离特征集和标签集)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">continuous_feature_names = [x <span class="keyword">for</span> x <span class="keyword">in</span> sample_feature.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;brand&#x27;</span>, <span class="string">&#x27;model&#x27;</span>]]</span><br></pre></td></tr></table></figure><h2 id="处理一下sample-feature数据集的缺失值"><a href="#处理一下sample-feature数据集的缺失值" class="headerlink" title="# 处理一下sample_feature数据集的缺失值"></a># 处理一下sample_feature数据集的缺失值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = sample_feature.dropna().replace(<span class="string">&#x27;-&#x27;</span>, <span class="number">0</span>).reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>这里我比较疑惑为什么replace了’-‘?<br>然后我看了看数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature[sample_feature.isin([<span class="string">&#x27;-&#x27;</span>])] </span><br></pre></td></tr></table></figure><p>发现每非常多的样本都存在’-‘,说明有一个特征里面有’-‘</p><p>我第一时间考虑到了notRepairedDamage这个特征,因为它是唯一一个object类型的特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature[<span class="string">&#x27;notRepairedDamage&#x27;</span>]</span><br></pre></td></tr></table></figure><p>果不其然,它是个类别特征,Categories (3, object): [-, 0.0, 1.0],有三个类别:’-‘, ‘0’, ‘1’</p><h2 id="既然想起来了notRepairedDamage不是数值型特征-那么我们把它转化一下"><a href="#既然想起来了notRepairedDamage不是数值型特征-那么我们把它转化一下" class="headerlink" title="# 既然想起来了notRepairedDamage不是数值型特征,那么我们把它转化一下"></a># 既然想起来了notRepairedDamage不是数值型特征,那么我们把它转化一下</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature[<span class="string">&#x27;notRepairedDamage&#x27;</span>] = sample_feature[<span class="string">&#x27;notRepairedDamage&#x27;</span>].astype(np.float32)</span><br></pre></td></tr></table></figure><h2 id="处理完了之后-我们拿出我们挑选的特征和它的价格构造训练集"><a href="#处理完了之后-我们拿出我们挑选的特征和它的价格构造训练集" class="headerlink" title="# 处理完了之后,我们拿出我们挑选的特征和它的价格构造训练集"></a># 处理完了之后,我们拿出我们挑选的特征和它的价格构造训练集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train = sample_feature[continuous_feature_names + [<span class="string">&#x27;price&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y = train[<span class="string">&#x27;price&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="简单建模"><a href="#简单建模" class="headerlink" title="# 简单建模"></a># 简单建模</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression       <span class="comment"># 线性回归模型 y = wx + b</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">这里比较疑惑,不是说用树模型嘛?挑的也是树模型的数据,为啥这里建了LR,看看再说</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">model = LinearRegression(normalize=<span class="literal">True</span>)    <span class="comment"># normalize参数决定是否将数据归一化</span></span><br><span class="line">model = model.fit(train_X, train_y)</span><br></pre></td></tr></table></figure><h2 id="查看训练的lR模型的截距-intercept-与权重-coef"><a href="#查看训练的lR模型的截距-intercept-与权重-coef" class="headerlink" title="# 查看训练的lR模型的截距(intercept)与权重(coef)"></a># 查看训练的lR模型的截距(intercept)与权重(coef)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model.intercept_)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">zip() - 可以将两个可迭代的对象,组合返回成一个元组数据</span></span><br><span class="line"><span class="string">dict() - 使用元组数据构建字典</span></span><br><span class="line"><span class="string">items方法 - items() 函数以列表返回可遍历的(键, 值) 元组数组 </span></span><br><span class="line"><span class="string">sort(iterable, cmp, key, reverse) - 排序函数</span></span><br><span class="line"><span class="string">    iterable - 指定要排序的list或者iterable</span></span><br><span class="line"><span class="string">    key - 指定取待排序元素的哪一项进行排序 - 这里x[1]表示按照列表中第二个元素排序</span></span><br><span class="line"><span class="string">    reverse - 是一个bool变量，表示升序还是降序排列，默认为False(升序)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">sorted</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(continuous_feature_names, model.coef_)).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 这行代码返回了每个特征的权重,按照权重降序排列</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.random.randint() - 产生离散均匀分布的整数</span></span><br><span class="line"><span class="string">            取数范围:若high不为None时,取[low,high)之间随机整数,否则取值[0,low)之间随机整数</span></span><br><span class="line"><span class="string">            size - 54输出的大小，可以是整数也可以是元组</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">subsample_index = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(train_y), size=<span class="number">50</span>)  <span class="comment"># 随机生成0-50000之间的50个整数</span></span><br></pre></td></tr></table></figure><h2 id="绘制v-9的值与标签的散点图"><a href="#绘制v-9的值与标签的散点图" class="headerlink" title="# 绘制v_9的值与标签的散点图"></a># 绘制v_9的值与标签的散点图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], train_y[subsample_index], color=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], model.predict(train_X.loc[subsample_index]), color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;v_9&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;True Price&#x27;</span>, <span class="string">&#x27;Predicted&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The predicted price is obvious different from true price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020033109212229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们发现预测点和真实值差别较大,且有预测值出现了结果为负的情况,不符合实际情况,模型有问题</p><h2 id="我们看一下价格的分布图"><a href="#我们看一下价格的分布图" class="headerlink" title="# 我们看一下价格的分布图"></a># 我们看一下价格的分布图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;It is clear to see the price shows a typical exponential distribution&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331092245311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>价格呈长尾分布,不利于建模预测,因为很多模型都假设数据误差项符合正态分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.quantile(train_y, 0.9) - 求train_y 的90%的分位数</span></span><br><span class="line"><span class="string">下面这个代码是把价格大于90%分位数的部分截断了,就是长尾分布截断</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.distplot(train_y[train_y &lt; np.quantile(train_y, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020033109232833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>看上去好多了</p><h2 id="为了更加贴近正态分布-对price进行log-x-1-变换"><a href="#为了更加贴近正态分布-对price进行log-x-1-变换" class="headerlink" title="# 为了更加贴近正态分布,对price进行log(x+1)变换"></a># 为了更加贴近正态分布,对price进行log(x+1)变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_y_In = np.log(train_y + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The transformed price seems like normal distribution&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y_In)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">sns.distplot(train_y_In[train_y_In &lt; np.quantile(train_y_In, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331092504165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这回看上去像正态分布了,右边的图是做了长尾截断处理</p><h2 id="我们再训练一次"><a href="#我们再训练一次" class="headerlink" title="# 我们再训练一次"></a># 我们再训练一次</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = model.fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model.intercept_))</span><br><span class="line"><span class="built_in">sorted</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(continuous_feature_names, model.coef_)).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="可视化一波-还是看v-9"><a href="#可视化一波-还是看v-9" class="headerlink" title="# 可视化一波,还是看v_9"></a># 可视化一波,还是看v_9</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], train_y[subsample_index], color=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.exp() - 求e的幂次方,因为训练模型的时候log变换了,所以预测完了对比结果的时候得变回来</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">plt.scatter(train_X[<span class="string">&#x27;v_9&#x27;</span>][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;v_9&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;True Price&#x27;</span>, <span class="string">&#x27;Predicted&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331092718897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>看上去准多了,预测值和真实值更加贴合</p><h2 id="五折交叉验证"><a href="#五折交叉验证" class="headerlink" title="# 五折交叉验证"></a># 五折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error, make_scorer</span><br></pre></td></tr></table></figure><p>定义一个函数,用来处理预测值和真实值的log变换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">numpy.nan_to_num(x) - 使用0代替数组x中的nan元素，使用有限的数字代替inf元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_transfer</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">warpper</span>(<span class="params">y, yhat</span>):</span><br><span class="line">        result = func(np.log(y), np.nan_to_num(np.log(yhat)))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> warpper</span><br></pre></td></tr></table></figure><h2 id="我们使用线性回归模型-对未处理过标签的特征数据做5折交叉验证"><a href="#我们使用线性回归模型-对未处理过标签的特征数据做5折交叉验证" class="headerlink" title="# 我们使用线性回归模型,对未处理过标签的特征数据做5折交叉验证"></a># 我们使用线性回归模型,对未处理过标签的特征数据做5折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scores = cross_val_score(model, X=train_X, y=train_y, verbose=<span class="number">1</span>, cv=<span class="number">5</span>,</span><br><span class="line">                         scoring=make_scorer(log_transfer(mean_absolute_error)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">verbose - 日志显示</span></span><br><span class="line"><span class="string">            verbose = 0 为不在标准输出流输出日志信息</span></span><br><span class="line"><span class="string">            verbose = 1 为输出进度条记录</span></span><br><span class="line"><span class="string">            verbose = 2 为每个epoch输出一行记录</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">make_scorer() - 工厂函数,自己定评分标准</span></span><br><span class="line"><span class="string">这里的log_transfer()是返回log化的标签预测值和真实值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AVG:&#x27;</span>, np.mean(scores))</span><br></pre></td></tr></table></figure><p>— AVG: 1.3654295934396061  —&gt; 5次的MAE平均值</p><h2 id="我们再使用线性回归模型-对处理过标签的特征数据做5折交叉验证"><a href="#我们再使用线性回归模型-对处理过标签的特征数据做5折交叉验证" class="headerlink" title="# 我们再使用线性回归模型,对处理过标签的特征数据做5折交叉验证"></a># 我们再使用线性回归模型,对处理过标签的特征数据做5折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores_In = cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">1</span>, cv=<span class="number">5</span>,</span><br><span class="line">                         scoring=make_scorer(mean_absolute_error))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AVG:&#x27;</span>, np.mean(scores_In))</span><br></pre></td></tr></table></figure><p>— AVG: 0.1932330179438017<br>MAE从1.365降低到0.193,误差缩小了很多</p><h2 id="事实上-五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况"><a href="#事实上-五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况" class="headerlink" title="# 事实上,五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况"></a># 事实上,五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况</h2><p>用2018年的二手车价格预测2017年是不合理的,所以我们可以用时间靠前的4&#x2F;5样本当作训练集,靠后的1&#x2F;5当验证集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime     <span class="comment"># 这里我没看到datetime的作用,只能认为数据集是按照时间排列的</span></span><br><span class="line">sample_feature = sample_feature.reset_index(drop=<span class="literal">True</span>)      <span class="comment"># 重置索引</span></span><br><span class="line">split_point = <span class="built_in">len</span>(sample_feature) // <span class="number">5</span> * <span class="number">4</span>      <span class="comment"># 设置分割点</span></span><br><span class="line"></span><br><span class="line">train = sample_feature.loc[:split_point].dropna()</span><br><span class="line">val = sample_feature.loc[split_point:].dropna()</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y_In = np.log(train[<span class="string">&#x27;price&#x27;</span>] + <span class="number">1</span>)</span><br><span class="line">val_X = val[continuous_feature_names]</span><br><span class="line">val_y_In = np.log(val[<span class="string">&#x27;price&#x27;</span>] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model = model.fit(train_X, train_y_In)</span><br><span class="line">mean_absolute_error(val_y_In, model.predict(val_X))</span><br></pre></td></tr></table></figure><p>— MAE为0.196,和五折交叉验证差别不大</p><h2 id="绘制学习率曲线与验证曲线"><a href="#绘制学习率曲线与验证曲线" class="headerlink" title="# 绘制学习率曲线与验证曲线"></a># 绘制学习率曲线与验证曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve, validation_curve</span><br></pre></td></tr></table></figure><p>创建绘制学习率曲线的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_learning_curve</span>(<span class="params">estimator, title, X, y, ylim=<span class="literal">None</span>, cv=<span class="literal">None</span>, n_jobs=<span class="number">1</span>, train_sizes=np.linspace(<span class="params"><span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span></span>)</span>):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.ylim(*ylim)     <span class="comment"># 如果规定了ylim的值,那么ylim就用规定的值</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Training example&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;score&#x27;</span>)</span><br><span class="line">    train_size, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,</span><br><span class="line">                                                           train_sizes=train_sizes,</span><br><span class="line">                                                           scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    fill_between()</span></span><br><span class="line"><span class="string">            train_sizes - 第一个参数表示覆盖的区域</span></span><br><span class="line"><span class="string">            train_scores_mean - train_scores_std - 第二个参数表示覆盖的下限</span></span><br><span class="line"><span class="string">            train_scores_mean + train_scores_std - 第三个参数表示覆盖的上限</span></span><br><span class="line"><span class="string">            color - 表示覆盖区域的颜色</span></span><br><span class="line"><span class="string">            alpha - 覆盖区域的透明度,越大越不透明 [0,1]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">&#x27;o-&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Training score&#x27;</span>)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">&#x27;o-&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;Cross-validation score&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br><span class="line"></span><br><span class="line">plot_learning_curve(LinearRegression(), <span class="string">&#x27;Liner_model&#x27;</span>, train_X[:<span class="number">1000</span>], train_y_In[:<span class="number">1000</span>],</span><br><span class="line">                    ylim=(<span class="number">0.0</span>, <span class="number">0.5</span>), cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331093755291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>模型在训练集拟合的不错,在验证集中表现一般</p><h2 id="嵌入式特征选择-大部分情况下都是用嵌入式做特征选择"><a href="#嵌入式特征选择-大部分情况下都是用嵌入式做特征选择" class="headerlink" title="# 嵌入式特征选择 - 大部分情况下都是用嵌入式做特征选择"></a># 嵌入式特征选择 - 大部分情况下都是用嵌入式做特征选择</h2><p>1.L1正则化 - Lasso回归 -<br>        模型被限制在正方形区域(二维区域下),损失函数的最小值往往在正方形(约束)的角上,很多权值为0(多维),所以可以实现模型的稀疏性(生成稀疏权值矩阵,进而用于特征选择</p><p>2.L2正则化 - 岭回归 -<br>        模型被限制在圆形区域(二维区域下),损失函数的最小值因为圆形约束没有角,所以不会使得权重为0,但是可以使得权重都尽可能的小,最后得到一个所有参数都比较小的模型,这样模型比较简单,能适应不同数据集,一定程度上避免了过拟合</p><h2 id="我们看下三种模型的效果对比-线性回归-加入了L1的Lasso回归-加入了L2的岭回归"><a href="#我们看下三种模型的效果对比-线性回归-加入了L1的Lasso回归-加入了L2的岭回归" class="headerlink" title="# 我们看下三种模型的效果对比:线性回归; 加入了L1的Lasso回归; 加入了L2的岭回归"></a># 我们看下三种模型的效果对比:线性回归; 加入了L1的Lasso回归; 加入了L2的岭回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">models = [LinearRegression(), Ridge(), Lasso()]</span><br><span class="line">result = <span class="built_in">dict</span>()     <span class="comment"># 创建一个用来装结果的字典</span></span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = <span class="built_in">str</span>(model).split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">0</span>]   <span class="comment"># 把括号去掉,只保留名字</span></span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>, cv=<span class="number">5</span>,       <span class="comment"># 五折交叉验证</span></span><br><span class="line">                             scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    <span class="built_in">print</span>(model_name + <span class="string">&#x27; is finished&#x27;</span>)</span><br><span class="line"></span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">&#x27;cv&#x27;</span> + <span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br><span class="line"></span><br><span class="line">model_Lr = LinearRegression().fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model_Lr.intercept_))</span><br><span class="line">sns.barplot(<span class="built_in">abs</span>(model_Lr.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331094145139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>— 线性回归模型:发现v_6, v_8, v_9权重大</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_Ridge = Ridge().fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model_Ridge.intercept_))</span><br><span class="line">sns.barplot(<span class="built_in">abs</span>(model_Ridge.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331094216502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>— 岭回归:发现有更多的参数对模型起到影响,而且参数都比较小,一定程度上避免了过拟合现象,抗扰动能力强</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_Lasso = Lasso().fit(train_X, train_y_In)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;intercept:&#x27;</span> + <span class="built_in">str</span>(model_Lasso.intercept_))</span><br><span class="line">sns.barplot(<span class="built_in">abs</span>(model_Lasso.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331094310769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>— lasso回归:发现power和used_time这两个特征很重要,L1正则化有助于生成一个稀疏权值矩阵,进而用于特征选择</p><h2 id="看看常用的非线性模型-与线性模型的效果进行一个比对"><a href="#看看常用的非线性模型-与线性模型的效果进行一个比对" class="headerlink" title="# 看看常用的非线性模型,与线性模型的效果进行一个比对"></a># 看看常用的非线性模型,与线性模型的效果进行一个比对</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">SVM - 支持向量机 - 通过寻求结构化风险最小来提高学习机泛化能力,基本模型定义为特征空间上的间隔最大的线性分类器</span></span><br><span class="line"><span class="string">                    支持向量机的学习策略便是间隔最大化</span></span><br><span class="line"><span class="string">    SVR - 用于标签连续值的回归问题</span></span><br><span class="line"><span class="string">    SVC - 用于分类标签的分类问题</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC     <span class="comment"># 这里用SVR是不是好得多?</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor      <span class="comment"># 决策树回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor  <span class="comment"># 随机森林回归</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Boosting算法思想 -  一堆弱分类器的组合就可以成为一个强分类器;</span></span><br><span class="line"><span class="string">                    不断地在错误中学习，迭代来降低犯错概率</span></span><br><span class="line"><span class="string">                    通过一系列的迭代来优化分类结果,每迭代一次引入一个弱分类器,来克服现在已经存在的弱分类器组合的短板</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Adaboost      - 整个训练集上维护一个分布权值向量W</span></span><br><span class="line"><span class="string">                        用赋予权重的训练集通过弱分类算法产生分类假设（基学习器）y(x)</span></span><br><span class="line"><span class="string">                        然后计算错误率,用得到的错误率去更新分布权值向量w</span></span><br><span class="line"><span class="string">                        对错误分类的样本分配更大的权值,正确分类的样本赋予更小的权值</span></span><br><span class="line"><span class="string">                        每次更新后用相同的弱分类算法产生新的分类假设,这些分类假设的序列构成多分类器</span></span><br><span class="line"><span class="string">                        对这些多分类器用加权的方法进行联合,最后得到决策结果</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">        Gradient Boosting - 迭代的时候选择梯度下降的方向来保证最后的结果最好</span></span><br><span class="line"><span class="string">                            损失函数用来描述模型的&#x27;靠谱&#x27;程度,假设模型没有过拟合,损失函数越大,模型的错误率越高</span></span><br><span class="line"><span class="string">                            如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">                            GradientBoostingRegressor()</span></span><br><span class="line"><span class="string">                                    loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合</span></span><br><span class="line"><span class="string">                                    learning_rate - 学习率</span></span><br><span class="line"><span class="string">                                    n_estimators - 弱学习器的数目,默认值100</span></span><br><span class="line"><span class="string">                                    max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3</span></span><br><span class="line"><span class="string">                                    min_samples_split - 可以划分为内部节点的最小样本数,默认为2</span></span><br><span class="line"><span class="string">                                    min_samples_leaf - 叶节点所需的最小样本数,默认为1</span></span><br><span class="line"><span class="string">参考资料:https://www.cnblogs.com/zhubinwang/p/5170087.html</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">MLPRegressor  - 人工神经网络,了解的不多</span></span><br><span class="line"><span class="string">参数详解</span></span><br><span class="line"><span class="string">    hidden_layer_sizes - hidden_layer_sizes=(50, 50),表示有两层隐藏层,第一层隐藏层有50个神经元,第二层也有50个神经元</span></span><br><span class="line"><span class="string">    activation - 激活函数   &#123;‘identity’, ‘logistic’, ‘tanh’, ‘relu’&#125;,默认relu</span></span><br><span class="line"><span class="string">                identity - f(x) = x</span></span><br><span class="line"><span class="string">                logistic - 其实就是sigmod函数,f(x) = 1 / (1 + exp(-x))</span></span><br><span class="line"><span class="string">                tanh - f(x) = tanh(x)</span></span><br><span class="line"><span class="string">                relu - f(x) = max(0, x) </span></span><br><span class="line"><span class="string">    solver - 用来优化权重     &#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;,默认adam,</span></span><br><span class="line"><span class="string">                lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也更好</span></span><br><span class="line"><span class="string">                sgd - 随机梯度下降 </span></span><br><span class="line"><span class="string">                adam - 机遇随机梯度的优化器</span></span><br><span class="line"><span class="string">    alpha - 正则化项参数,可选的，默认0.0001</span></span><br><span class="line"><span class="string">    learning_rate - 学习率,用于权重更新,只有当solver为’sgd’时使用</span></span><br><span class="line"><span class="string">    max_iter - 最大迭代次数,默认200</span></span><br><span class="line"><span class="string">    shuffle - 判断是否在每次迭代时对样本进行清洗,默认True,只有当solver=’sgd’或者‘adam’时使用</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPRegressor</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">XGBRegressor - 梯度提升回归树,也叫梯度提升机</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">                采用连续的方式构造树,每棵树都试图纠正前一棵树的错误</span></span><br><span class="line"><span class="string">                与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝</span></span><br><span class="line"><span class="string">                从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> lightgbm.sklearn <span class="keyword">import</span> LGBMRegressor</span><br><span class="line"></span><br><span class="line">models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(),</span><br><span class="line">          GradientBoostingRegressor(), MLPRegressor(solver=<span class="string">&#x27;lbfgs&#x27;</span>, max_iter=<span class="number">100</span>),</span><br><span class="line">          XGBRegressor(n_estimators=<span class="number">100</span>, objective=<span class="string">&#x27;reg:squarederror&#x27;</span>),</span><br><span class="line">          LGBMRegressor(n_estimators=<span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line">result = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = <span class="built_in">str</span>(model).split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_In,</span><br><span class="line">                             verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    <span class="built_in">print</span>(model_name + <span class="string">&#x27; is finished&#x27;</span>)</span><br><span class="line"></span><br><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">&#x27;cv&#x27;</span> + <span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331101034898.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200331101048101.png" alt="在这里插入图片描述"><br>整体看来,随机森林在每一折的表现最好,XGB也不错,LGB最小值在0.143左右</p><h2 id="模型调参-以LGB-LGBMRegressor为例"><a href="#模型调参-以LGB-LGBMRegressor为例" class="headerlink" title="# 模型调参-以LGB - LGBMRegressor为例"></a># 模型调参-以LGB - LGBMRegressor为例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LightGBM - 使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低</span></span><br><span class="line"><span class="string">            思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram</span></span><br><span class="line"><span class="string">            然后遍历训练数据，统计每个离散值在直方图中的累计统计量</span></span><br><span class="line"><span class="string">            在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            LightGBM采用leaf-wise生长策略:</span></span><br><span class="line"><span class="string">            每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。</span></span><br><span class="line"><span class="string">            因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度</span></span><br><span class="line"><span class="string">            Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合</span></span><br><span class="line"><span class="string">            因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">参数:</span></span><br><span class="line"><span class="string">num_leaves - 控制了叶节点的数目,它是控制树模型复杂度的主要参数,取值应 &lt;= 2 ^（max_depth）</span></span><br><span class="line"><span class="string">bagging_fraction - 每次迭代时用的数据比例,用于加快训练速度和减小过拟合</span></span><br><span class="line"><span class="string">feature_fraction - 每次迭代时用的特征比例,例如为0.8时,意味着在每次迭代中随机选择80％的参数来建树,</span></span><br><span class="line"><span class="string">                    boosting为random forest时用</span></span><br><span class="line"><span class="string">min_data_in_leaf - 每个叶节点的最少样本数量。</span></span><br><span class="line"><span class="string">                    它是处理leaf-wise树的过拟合的重要参数</span></span><br><span class="line"><span class="string">                    将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合</span></span><br><span class="line"><span class="string">max_depth - 控制了树的最大深度,该参数可以显式的限制树的深度</span></span><br><span class="line"><span class="string">n_estimators - 分多少颗决策树(总共迭代的次数)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">objective - 问题类型</span></span><br><span class="line"><span class="string">            regression - 回归任务,使用L2损失函数</span></span><br><span class="line"><span class="string">            regression_l1 - 回归任务,使用L1损失函数</span></span><br><span class="line"><span class="string">            huber - 回归任务,使用huber损失函数</span></span><br><span class="line"><span class="string">            fair - 回归任务,使用fair损失函数</span></span><br><span class="line"><span class="string">            mape (mean_absolute_precentage_error) - 回归任务,使用MAPE损失函数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="贪心算法"><a href="#贪心算法" class="headerlink" title="## 贪心算法"></a>## 贪心算法</h3><h3 id="–-LGB的参数集合"><a href="#–-LGB的参数集合" class="headerlink" title="– LGB的参数集合"></a>– LGB的参数集合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">objective = [<span class="string">&#x27;regression&#x27;</span>, <span class="string">&#x27;regression_l1&#x27;</span>, <span class="string">&#x27;mape&#x27;</span>, <span class="string">&#x27;huber&#x27;</span>, <span class="string">&#x27;fair&#x27;</span>]</span><br><span class="line"></span><br><span class="line">num_leaves = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">max_depth = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">bagging_fraction = []</span><br><span class="line">feature_fraction = []</span><br><span class="line">drop_rate = []</span><br></pre></td></tr></table></figure><p>贪心调参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">建立数学模型来描述问题</span></span><br><span class="line"><span class="string">把求解的问题分成若干个子问题</span></span><br><span class="line"><span class="string">对每个子问题求解，得到子问题的局部最优解</span></span><br><span class="line"><span class="string">把子问题的解局部最优解合成原来问题的一个解</span></span><br><span class="line"><span class="string">总是做出在当前看来是最好的选择,也就是说,不从整体最优上加以考虑,它所做出的仅仅是在某种意义上的局部最优解</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对于一个具体问题,要确定它是否具有贪心选择性质,必须证明每一步所作的贪心选择最终导致问题的整体最优解</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 局部最优解 - objective</span></span><br><span class="line">best_obj = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>,</span><br><span class="line">                                    cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 局部最优解 - num_leaves    --- 此时得限制objective参数,得是mae值最小,即误差最小的那个问题类型</span></span><br><span class="line">best_leaves = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    best_obj.items() - 把best_obj字典中的元组装进列表</span></span><br><span class="line"><span class="string">    key=lambda x: x[1] - 取前一个对象的第二维的数据,即原字典的value值</span></span><br><span class="line"><span class="string">    min(best_obj.items(), key=lambda x: x[1]) - 选择best_obj中value值最小的那组元组</span></span><br><span class="line"><span class="string">    min(best_obj.items(), key=lambda x: x[1])[0] - 返回value值最小的key,即问题类型objective</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>,</span><br><span class="line">                                    cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 局部最优解 - max_depth    --- 此时得限制objective参数和num_leaves参数,都得是mae值的问题类型和叶节点数</span></span><br><span class="line">best_depth = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=<span class="built_in">min</span>(best_leaves.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>,</span><br><span class="line">                                    cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure><p>经过漫长的等待….</p><p>以上就是贪心算法的思想:局部求得最优解之后,固定条件再求别的局部最优解,最后得出所有局部最优的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(x=[<span class="string">&#x27;0_initial&#x27;</span>, <span class="string">&#x27;1_turning_obj&#x27;</span>, <span class="string">&#x27;2_turning_leaves&#x27;</span>, <span class="string">&#x27;3_turning_depth&#x27;</span>],</span><br><span class="line">             y=[<span class="number">0.143</span>, <span class="built_in">min</span>(best_obj.values()), <span class="built_in">min</span>(best_leaves.values()), <span class="built_in">min</span>(best_depth.values())])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200331101813468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>能看出,XGB模型在没有调参的情况下,MAE约0.143<br>局部选择最优objective参数后,MAE下降约为0.142<br>再选择了最优的best_leaves参数后,MAE为0.1354左右(优化了很多)<br>最后选择了best_depth参数后,MAE为0.1353左右,差别不大</p><h2 id="Grid-Search-网格搜索调参"><a href="#Grid-Search-网格搜索调参" class="headerlink" title="## Grid Search  - 网格搜索调参"></a>## Grid Search  - 网格搜索调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过循环遍历，尝试每一种参数组合，返回最好的得分值的参数组合</span></span><br><span class="line"><span class="string">GridSearchCV能够使我们找到范围内最优的参数，param_grid参数越多，组合越多，计算的时间也需要越多</span></span><br><span class="line"><span class="string">GridSearchCV适用于小数据集</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">parameters = &#123;<span class="string">&#x27;objective&#x27;</span>: objective, <span class="string">&#x27;num_leaves&#x27;</span>: num_leaves, <span class="string">&#x27;max_depth&#x27;</span>: max_depth&#125;     <span class="comment"># 给定参数取值范围</span></span><br><span class="line">model = LGBMRegressor()     <span class="comment"># 创建实例</span></span><br><span class="line">clf = GridSearchCV(model, parameters, cv=<span class="number">5</span>)     <span class="comment"># 网格搜索,遍历各种特征组合,五折交叉验证</span></span><br><span class="line">clf = clf.fit(train_X, train_y_In)      <span class="comment"># 训练的过程的确很缓慢,超级慢</span></span><br><span class="line">clf.best_params_</span><br><span class="line"><span class="comment"># 结果Out[10]: &#123;&#x27;max_depth&#x27;: 10, &#x27;num_leaves&#x27;: 55, &#x27;objective&#x27;: &#x27;huber&#x27;&#125;,和教材不一样</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LGBMRegressor(objective=<span class="string">&#x27;huber&#x27;</span>, num_leaves=<span class="number">55</span>, max_depth=<span class="number">15</span>)</span><br><span class="line">np.mean(cross_val_score(model, X=train_X, y=train_y_In, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Out[<span class="number">36</span>]: <span class="number">0.1370086509753757</span></span><br></pre></td></tr></table></figure><p>MAE为0.137<br>结果Out[10]: {‘max_depth’: 10, ‘num_leaves’: 55, ‘objective’: ‘huber’},和教材不一样</p><h2 id="贝叶斯调参"><a href="#贝叶斯调参" class="headerlink" title="## 贝叶斯调参"></a>## 贝叶斯调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">贝叶斯优化是一种用模型找到函数最小值方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">贝叶斯方法与随机或网格搜索的不同之处在于:它在尝试下一组超参数时,会参考之前的评估结果,因此可以省去很多无用功</span></span><br><span class="line"><span class="string">贝叶斯调参法使用不断更新的概率模型,通过推断过去的结果来&#x27;集中&#x27;有希望的超参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">贝叶斯优化问题的四个部分</span></span><br><span class="line"><span class="string">            1.目标函数 - 机器学习模型使用该组超参数在验证集上的损失</span></span><br><span class="line"><span class="string">                        它的输入为一组超参数,输出需要最小化的值(交叉验证损失)</span></span><br><span class="line"><span class="string">            2.域空间 - 要搜索的超参数的取值范围</span></span><br><span class="line"><span class="string">                        在搜索的每次迭代中,贝叶斯优化算法将从域空间为每个超参数选择一个值</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">                        当我们进行随机或网格搜索时,域空间是一个网格</span></span><br><span class="line"><span class="string">                        而在贝叶斯优化中,不是按照顺序()网格)或者随机选择一个超参数,而是按照每个超参数的概率分布选择</span></span><br><span class="line"><span class="string">            3.优化算法 - 构造替代函数并选择下一个超参数值进行评估的方法</span></span><br><span class="line"><span class="string">            4.来自目标函数评估的存储结果,包括超参数和验证集上的损失</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数,我们要这个目标函数输出的值最小</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rf_cv</span>(<span class="params">num_leaves, max_depth, subsample, min_child_samples</span>):</span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        LGBMRegressor(</span><br><span class="line">            objective=<span class="string">&#x27;regression_l1&#x27;</span>, num_leaves=<span class="built_in">int</span>(num_leaves), max_depth=<span class="built_in">int</span>(max_depth),</span><br><span class="line">            subsample=subsample, min_child_samples=<span class="built_in">int</span>(min_child_samples)),</span><br><span class="line">        X=train_X, y=train_y_In, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)</span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - val</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化参数,即域空间</span></span><br><span class="line">rf_bo = BayesianOptimization(rf_cv, &#123;<span class="string">&#x27;num_leaves&#x27;</span>: (<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">                                     <span class="string">&#x27;max_depth&#x27;</span>: (<span class="number">2</span>, <span class="number">100</span>), <span class="string">&#x27;subsample&#x27;</span>: (<span class="number">0.1</span>, <span class="number">1</span>),</span><br><span class="line">                                     <span class="string">&#x27;min_child_samples&#x27;</span>: (<span class="number">2</span>, <span class="number">100</span>)&#125;</span><br><span class="line">                             )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始优化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">rf_bo.maximize() - 最大化分数   这里的目标函数是1-MAE,应该是越大越好,所以用最大化分数</span></span><br><span class="line"><span class="string">rf_bo.minimize() - 最小化分数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rf_bo.maximize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最优目标函数对应的MAE值</span></span><br><span class="line"><span class="number">1</span> - rf_bo.<span class="built_in">max</span>[<span class="string">&#x27;target&#x27;</span>]</span><br></pre></td></tr></table></figure><p>Target值一旦出现新高,就会标记紫色<br>观察最高的值为0.8694<br>MAE值为0.1306;<br>目前是三种调参方法中得到最高精度的一种</p><p>不难发现,随着对参数的调整,模型的精度在一点点提高</p><hr><p>后续Task5 模型融合,最后的时刻到了~</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;建模与调参-代码示例部分&quot;&gt;&lt;a href=&quot;#建模与调参-代码示例部分&quot; class=&quot;headerlink&quot; title=&quot;建模与调参_代码示例部分&quot;&gt;&lt;/a&gt;建模与调参_代码示例部分&lt;/h1&gt;&lt;h2 id=&quot;导入工具包&quot;&gt;&lt;a href=&quot;#导入工具包&quot; c</summary>
      
    
    
    
    <category term="二手车交易价格预测" scheme="http://example.com/categories/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>二手车交易价格预测_Task3_特征工程</title>
    <link href="http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task3_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>http://example.com/2023/04/24/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task3_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</id>
    <published>2023-04-24T08:02:16.147Z</published>
    <updated>2023-04-24T08:17:33.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征工程-代码示例部分"><a href="#特征工程-代码示例部分" class="headerlink" title="特征工程_代码示例部分"></a>特征工程_代码示例部分</h1><h2 id="1-导入数据"><a href="#1-导入数据" class="headerlink" title="1.导入数据"></a>1.导入数据</h2><h3 id="导入第三方工具包"><a href="#导入第三方工具包" class="headerlink" title="# 导入第三方工具包"></a># 导入第三方工具包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter  <span class="comment"># 用于获取对象的位置</span></span><br></pre></td></tr></table></figure><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="# 导入数据"></a># 导入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\used_car_train_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\used_car_testA_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="看下数据形状"><a href="#看下数据形状" class="headerlink" title="# 看下数据形状"></a># 看下数据形状</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_data.shape, test_data.shape)</span><br></pre></td></tr></table></figure><p>发现训练集中有150000个样本,31列(30列特征+1列标签)<br>        测试集中有50000个样本,30列(30列特征)</p><hr><h2 id="2-查找异常值并删除"><a href="#2-查找异常值并删除" class="headerlink" title="2.查找异常值并删除"></a>2.查找异常值并删除</h2><p>这里阿泽大大包装了一个异常之处理的代码,可以随时调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outliers_proc</span>(<span class="params">data, col_name, scale=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    用于清洗异常值,默认用box_plot(scale=3)进行清洗 - 箱线图处理异常值</span></span><br><span class="line"><span class="string">    :param data: 接收pandas数据格式</span></span><br><span class="line"><span class="string">    :param col_name: pandas列名</span></span><br><span class="line"><span class="string">    :param scale: 尺度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">box_plot_outliers</span>(<span class="params">data_ser, box_scale</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        利用箱线图去除异常值</span></span><br><span class="line"><span class="string">        :param data_ser: 接收pandas.Series数据格式</span></span><br><span class="line"><span class="string">        :param box_scale: 箱线图尺度</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># quantile(0.75) - 求数据的上四分位数 - Q3</span></span><br><span class="line">        <span class="comment"># quantile(0.25) - 求数据的下四分位数 - Q1</span></span><br><span class="line">        <span class="comment"># data_ser.quantile(0.75) - data_ser.quantile(0.25) = Q3 - Q1 = ΔQ --&gt; 四分位距</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        boxplot默认的上边缘到上四分位数的间距是1.5ΔQ,即 scale=1.5</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        这里设定的为3ΔQ:</span></span><br><span class="line"><span class="string">        超过了上边缘Q3+3ΔQ和下边缘Q1-3ΔQ的部分视为异常值</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        iqr = box_scale * (data_ser.quantile(<span class="number">0.75</span>) - data_ser.quantile(<span class="number">0.25</span>))  <span class="comment"># iqr - 上边缘到上四分位数的间距,即3ΔQ</span></span><br><span class="line">        val_low = data_ser.quantile(<span class="number">0.25</span>) - iqr  <span class="comment"># 下边缘 Q1-3ΔQ</span></span><br><span class="line">        val_up = data_ser.quantile(<span class="number">0.75</span>) + iqr  <span class="comment"># 上边缘 Q3+3ΔQ</span></span><br><span class="line">        rule_low = (data_ser &lt; val_low)  <span class="comment"># 低于下边缘 Q1-3ΔQ的为异常值</span></span><br><span class="line">        rule_up = (data_ser &gt; val_up)  <span class="comment"># 高于上边缘 Q3+3ΔQ的为异常值</span></span><br><span class="line">        <span class="keyword">return</span> (rule_low, rule_up), (val_low, val_up)  <span class="comment"># 得到异常值 / 上边缘与下边缘之间的值</span></span><br><span class="line"></span><br><span class="line">    data_n = data.copy()  <span class="comment"># 拷贝一份数据的副本</span></span><br><span class="line">    data_series = data_n[col_name]  <span class="comment"># 转化成pandas.Series数据格式</span></span><br><span class="line">    rule, value = box_plot_outliers(data_series, box_scale=scale)</span><br><span class="line">    <span class="comment"># data_series.shape[0] - 看data_series这个一维数组有几行,即原数据集的总列数</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    np.arange() - 函数返回一个有终点和起点的固定步长的排列</span></span><br><span class="line"><span class="string">                    一个参数时:参数值为终点,起点取默认值0,步长取默认值1</span></span><br><span class="line"><span class="string">                    两个参数时:第一个参数为起点,第二个参数为终点,步长取默认值1</span></span><br><span class="line"><span class="string">                    三个参数时:第一个参数为起点,第二个参数为终点,第三个参数为步长,其中步长支持小数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># np.arange(data_series.shape[0]) - 取N个数,N为数据集字段数,步长为1  --&gt; 生成的是列表</span></span><br><span class="line">    index = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>] | rule[<span class="number">1</span>]]  <span class="comment"># 挑出位于异常值区间的序号,放进标记为index的列表中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Delete number is: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(index)))  <span class="comment"># 输出要删除多少个异常值</span></span><br><span class="line"></span><br><span class="line">    data_n = data_n.drop(index)  <span class="comment"># 按索引查找并删除</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    reset_index() - 重塑索引 (因为有时候对dataframe做处理后索引可能是乱的,就像上面删除了异常值一样)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数详解:</span></span><br><span class="line"><span class="string">    drop - True:把原来的索引index列去掉,重置index      False:保留原来的索引，添加重置的index</span></span><br><span class="line"><span class="string">    inplace - True:原数组不变，对数据进行修改之后结果给新的数组     False:直接在原数组上对数据进行修改</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    data_n.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Now column number is: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data_n.shape[<span class="number">0</span>]))  <span class="comment"># 打印出现在的行数,即正常值的个数</span></span><br><span class="line"></span><br><span class="line">    index_low = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">0</span>]]  <span class="comment"># 挑出位于下异常值区间的序号,放进标记为index_low的列表中</span></span><br><span class="line">    outliers_low = data_series.iloc[index_low]  <span class="comment"># 把位于下异常值区间的数据放进outliers中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Description of data less than the lower bound is: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pd.Series(outliers_low).describe())  <span class="comment"># 对于位于下异常值区间的数据,做一个统计描述</span></span><br><span class="line"></span><br><span class="line">    index_up = np.arange(data_series.shape[<span class="number">0</span>])[rule[<span class="number">1</span>]]  <span class="comment"># 挑出位于上异常值区间的序号,放进标记为index_up的列表中</span></span><br><span class="line">    outliers_up = data_series.iloc[index_up]  <span class="comment"># 把位于上异常值区间的数据放进outliers_up中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Description of data larger than the lower bound is: &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pd.Series(outliers_up).describe())  <span class="comment"># 对于位于上异常值区间的数据,再做一个统计描述</span></span><br><span class="line"></span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    sns.boxplot - 箱线图</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数详解:</span></span><br><span class="line"><span class="string">    x, y, hue - 数据或向量数据的变量名称</span></span><br><span class="line"><span class="string">    data - 用于绘图的数据集</span></span><br><span class="line"><span class="string">    palette - 调色板名称</span></span><br><span class="line"><span class="string">    ax - 绘图时使用的matplotlib轴对象</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    sns.boxplot(y=data[col_name], data=data, palette=<span class="string">&#x27;Set1&#x27;</span>, ax=ax[<span class="number">0</span>])</span><br><span class="line">    sns.boxplot(y=data_n[col_name], data=data_n, palette=<span class="string">&#x27;Set1&#x27;</span>, ax=ax[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> data_n</span><br></pre></td></tr></table></figure><p>我们可以删掉一些异常数据,以power为例 - 注意:test的数据不能删除,train删哪些可以自行判断</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data = outliers_proc(train_data, <span class="string">&#x27;power&#x27;</span>, scale=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200327155223826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们发现,处理前的异常值是很多的,处理之后可以看到箱线图的形状了,数据好了很多,但是还是有部分异常值,可以考虑将尺度scale再调小些;</p><hr><h2 id="3-特征构造"><a href="#3-特征构造" class="headerlink" title="3.特征构造"></a>3.特征构造</h2><p>我们把训练集和测试集放在一起,方便构造特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把两个数据集都加一列&#x27;train&#x27;标识,合并之后用来区分数据来源</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">train_data[<span class="string">&#x27;train&#x27;</span>] = <span class="number">1</span></span><br><span class="line">test_data[<span class="string">&#x27;train&#x27;</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">pd.concat()参数说明:</span></span><br><span class="line"><span class="string">ignore_index - 如果两个表的index都没有实际含义,可以令 ignore_index = True</span></span><br><span class="line"><span class="string">                合并的两个表根据列字段对齐,然后重塑新的索引</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sort - 默认为False;</span></span><br><span class="line"><span class="string">        设置为True时表示合并时会根据给定的列值来进行排序后再输出</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data = pd.concat([train_data, test_data], ignore_index=<span class="literal">True</span>, sort=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="看一下数据有哪些列"><a href="#看一下数据有哪些列" class="headerlink" title="# 看一下数据有哪些列"></a># 看一下数据有哪些列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">data.columns</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">特征解释:</span></span><br><span class="line"><span class="string">SaleID - 交易ID,唯一编码</span></span><br><span class="line"><span class="string">name - 汽车交易名称,已脱敏</span></span><br><span class="line"><span class="string">regDate - 汽车注册日期,例如20160101,2016年01月01日</span></span><br><span class="line"><span class="string">model - 车型编码,已脱敏</span></span><br><span class="line"><span class="string">brand - 汽车品牌,已脱敏</span></span><br><span class="line"><span class="string">bodyType - 车身类型-豪华轿车:0; 微型车:1;  厢型车:2;  大巴车:3;  敞篷车:4;  双门汽车:5; 商务车:6;  搅拌车:7</span></span><br><span class="line"><span class="string">fuelType - 燃油类型-汽油:0;   柴油:1;   液化石油气:2;   天然气:3;   混合动力:4;   其他:5;   电动:6</span></span><br><span class="line"><span class="string">gearbox - 变速箱-手动:0;   自动:1</span></span><br><span class="line"><span class="string">power - 发动机功率:范围 [ 0, 600 ]</span></span><br><span class="line"><span class="string">kilometer - 汽车已行驶公里,单位万km</span></span><br><span class="line"><span class="string">notRepairedDamage - 汽车有尚未修复的损坏-是:0; 否:1</span></span><br><span class="line"><span class="string">regionCode - 地区编码,已脱敏</span></span><br><span class="line"><span class="string">seller - 销售方-个体:0;  非个体:1</span></span><br><span class="line"><span class="string">offerType - 报价类型-提供:0;  请求:1</span></span><br><span class="line"><span class="string">creatDate - 汽车上线时间-即开始售卖时间</span></span><br><span class="line"><span class="string">price - 二手车交易价格(预测目标)</span></span><br><span class="line"><span class="string">v系列特征 - 匿名特征:包含v0-14在内15个匿名特征(根据汽车的评论,标签等大量信息得到的embedding向量)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="我们构造一个车辆使用时间-一般来说-使用时间和价格会呈反比"><a href="#我们构造一个车辆使用时间-一般来说-使用时间和价格会呈反比" class="headerlink" title="# 我们构造一个车辆使用时间 - 一般来说:使用时间和价格会呈反比"></a># 我们构造一个车辆使用时间 - 一般来说:使用时间和价格会呈反比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">车辆使用时间 = 汽车上线时间 - 汽车注册时间</span></span><br><span class="line"><span class="string">            这里的汽车注册时间应该是二手车交易过户后在登记本上的注册时间,而不是新车首次注册的时间,不然不合逻辑</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">注意:数据的时间有出错的格式,我们需要设置参数 errors = &#x27;coerce&#x27;</span></span><br><span class="line"><span class="string">    errors=&#x27;coerce&#x27; Pandas - 遇到不能转换的数据就会赋值为 NaN</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">datetime标准形式:   xxxx - xx - xx</span></span><br><span class="line"><span class="string">dt.days - 求datetime数据的天数</span></span><br><span class="line"><span class="string">dt.years - 求datetime数据的年</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;used_Time&#x27;</span>] = (pd.to_datetime(data[<span class="string">&#x27;creatDate&#x27;</span>], <span class="built_in">format</span>=<span class="string">&#x27;%Y%m%d&#x27;</span>, errors=<span class="string">&#x27;coerce&#x27;</span>) -</span><br><span class="line">                     pd.to_datetime(data[<span class="string">&#x27;regDate&#x27;</span>], <span class="built_in">format</span>=<span class="string">&#x27;%Y%m%d&#x27;</span>, errors=<span class="string">&#x27;coerce&#x27;</span>)).dt.days</span><br></pre></td></tr></table></figure><h3 id="看一下新建的特征’used-time’有多少nan"><a href="#看一下新建的特征’used-time’有多少nan" class="headerlink" title="# 看一下新建的特征’used_time’有多少nan"></a># 看一下新建的特征’used_time’有多少nan</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;used_Time&#x27;</span>].isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>1.空值有15072个,总数居有199037个,缺失比例约为7.6%<br>2.缺失数据占总样本大,不建议删除<br>3.可以尝试使用XGBoost之类的决策树来处理缺失值</p><h3 id="从邮编中提取城市信息"><a href="#从邮编中提取城市信息" class="headerlink" title="# 从邮编中提取城市信息"></a># 从邮编中提取城市信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据来源德国,因此参考德国的邮编 - 先验知识</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;city&#x27;</span>] = data[<span class="string">&#x27;regionCode&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x)[:-<span class="number">3</span>])  <span class="comment"># [:-3] --&gt; [0:-3] - 左闭右开:取第一个到倒数第4个</span></span><br></pre></td></tr></table></figure><h3 id="计算某品牌的销售统计量-计算统计量的时候得以训练集的数据进行统计"><a href="#计算某品牌的销售统计量-计算统计量的时候得以训练集的数据进行统计" class="headerlink" title="# 计算某品牌的销售统计量 - 计算统计量的时候得以训练集的数据进行统计"></a># 计算某品牌的销售统计量 - 计算统计量的时候得以训练集的数据进行统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_gb = train_data.groupby(<span class="string">&#x27;brand&#x27;</span>)</span><br><span class="line"></span><br><span class="line">all_info = &#123;&#125;  <span class="comment"># 新建一个字典,用来存放统计量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> kind, kind_data <span class="keyword">in</span> train_gb:  <span class="comment"># kind - 品牌;        # kind_data - 所属品牌对应的数据</span></span><br><span class="line">    info = &#123;&#125;</span><br><span class="line">    kind_data = kind_data[kind_data[<span class="string">&#x27;price&#x27;</span>] &gt; <span class="number">0</span>]  <span class="comment"># 去掉不合理的价格 - 售价不会小于0</span></span><br><span class="line">    info[<span class="string">&#x27;brand_amount&#x27;</span>] = <span class="built_in">len</span>(kind_data)  <span class="comment"># 所属品牌的数据长度即为该品牌的销售数量</span></span><br><span class="line">    info[<span class="string">&#x27;brand_price_max&#x27;</span>] = kind_data.price.<span class="built_in">max</span>()  <span class="comment"># 找出该品牌的销售最高价</span></span><br><span class="line">    info[<span class="string">&#x27;brand_price_median&#x27;</span>] = kind_data.price.median()  <span class="comment"># 找出该品牌的销售价的中位数</span></span><br><span class="line">    info[<span class="string">&#x27;brand_price_min&#x27;</span>] = kind_data.price.<span class="built_in">min</span>()  <span class="comment"># 找出该品牌的销售最低价</span></span><br><span class="line">    info[<span class="string">&#x27;brand_price_sum&#x27;</span>] = kind_data.price.<span class="built_in">sum</span>()  <span class="comment"># 统计该品牌的销售总额</span></span><br><span class="line">    info[<span class="string">&#x27;brand_price_std&#x27;</span>] = kind_data.price.std()  <span class="comment"># 统计该品牌的销售价的标准差</span></span><br><span class="line">    info[<span class="string">&#x27;brand_price_average&#x27;</span>] = <span class="built_in">round</span>(kind_data.price.<span class="built_in">sum</span>() / (<span class="built_in">len</span>(kind_data) + <span class="number">1</span>), <span class="number">2</span>)  <span class="comment"># 这里数量加了1,防止分母为0</span></span><br><span class="line">    all_info[kind] = info</span><br></pre></td></tr></table></figure><p>这里的平均值average我卡了很久,一直在想为什么分母要加上1,找了很多资料都没有找到原因,后面看直播以后才知道+1是为了防止出现分母为0的情况(虽然说一般不会出现这种情况,但还是习惯性的加了1,对数据影响很小又能保证不会因为这个原因报错,学习到了!)</p><h3 id="把做出来的新的统计特征加入原表中"><a href="#把做出来的新的统计特征加入原表中" class="headerlink" title="# 把做出来的新的统计特征加入原表中"></a># 把做出来的新的统计特征加入原表中</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns=&#123;<span class="string">&#x27;index&#x27;</span>: <span class="string">&#x27;brand&#x27;</span>&#125;)  <span class="comment"># 转置后重塑索引</span></span><br><span class="line">data = data.merge(brand_fe, how=<span class="string">&#x27;left&#x27;</span>, on=<span class="string">&#x27;brand&#x27;</span>)  <span class="comment"># 相当于excel中的 vlookup</span></span><br></pre></td></tr></table></figure><h3 id="数据分桶-以power为例"><a href="#数据分桶-以power为例" class="headerlink" title="# 数据分桶 - 以power为例"></a># 数据分桶 - 以power为例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据分桶的原因:</span></span><br><span class="line"><span class="string">LightGBM在改进 XGBoost 时就增加了数据分桶,增强了模型的泛化性</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1.分桶就是离散化,离散后,2333333333333333稀疏向量内积乘法运算速度会更快,计算结果也方便储存     # 这是onehot的好处</span></span><br><span class="line"><span class="string">2.离散后,特征对异常值更具鲁棒性 - 系统或组织有抵御或克服不利条件的能力 - 常被用来描述可以面对复杂适应系统的能力</span></span><br><span class="line"><span class="string">3.LR(线性回归)属于广义线性模型,表达能力有限</span></span><br><span class="line"><span class="string">                经过离散化后,每个变量有单独的权重,这相当于引入了非线性,能够提升模型的表达能力,加大拟合</span></span><br><span class="line"><span class="string">4.离散后特征可以进行特征交叉(one-hot编码等),由 M+N 个变量编程 M*N 个变量,进一步引入非线形,提升了表达能力</span></span><br><span class="line"><span class="string">5.特征离散后模型更稳定,如用户年龄区间不会因为用户年龄长了一岁就变化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">bin</span> = [i * <span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">31</span>)]</span><br><span class="line">data[<span class="string">&#x27;power_bin&#x27;</span>] = pd.cut(data[<span class="string">&#x27;power&#x27;</span>], <span class="built_in">bin</span>, labels=<span class="literal">False</span>)  <span class="comment"># 按值切分,bin为区间</span></span><br><span class="line">data[[<span class="string">&#x27;power&#x27;</span>, <span class="string">&#x27;power_bin&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>这里针对第一个优点:‘稀疏向量内积乘法运算速度会更快,计算结果也方便储存’是onehot编码的优点,不是分桶的好处</p><h3 id="以上特征处理完了-我们可以删掉原始多余的特征数据了"><a href="#以上特征处理完了-我们可以删掉原始多余的特征数据了" class="headerlink" title="# 以上特征处理完了,我们可以删掉原始多余的特征数据了"></a># 以上特征处理完了,我们可以删掉原始多余的特征数据了</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = data.drop([<span class="string">&#x27;SaleID&#x27;</span>, <span class="string">&#x27;creatDate&#x27;</span>, <span class="string">&#x27;regDate&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>], axis=<span class="number">1</span>)  <span class="comment"># 资料上没有SaleID列,我在这里一起删除了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>教材里是没有SaleID列的,应该是省略了,在这一步我一起做了删除的动作</p><h3 id="目前的数据已经可以给树模型使用了-我们导出"><a href="#目前的数据已经可以给树模型使用了-我们导出" class="headerlink" title="# 目前的数据已经可以给树模型使用了,我们导出"></a># 目前的数据已经可以给树模型使用了,我们导出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.to_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\data_for_tree.csv&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="不同模型对数据集的要求不同-我们再构造一份特征给LR-Logistic-regression-逻辑回归-NN-近邻-如k近邻-之类的模型用"><a href="#不同模型对数据集的要求不同-我们再构造一份特征给LR-Logistic-regression-逻辑回归-NN-近邻-如k近邻-之类的模型用" class="headerlink" title="# 不同模型对数据集的要求不同,我们再构造一份特征给LR(Logistic regression-逻辑回归), NN(近邻,如k近邻)之类的模型用"></a># 不同模型对数据集的要求不同,我们再构造一份特征给LR(Logistic regression-逻辑回归), NN(近邻,如k近邻)之类的模型用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 我们看下数据分布</span></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>].plot.hist()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">分布非常不均匀,因为我们对train进行了异常值处理,但是test还有异常值</span></span><br><span class="line"><span class="string">所以我们其实还是不删除train中的power的异常值比较好,改用长尾分布截断代替</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">train_data[<span class="string">&#x27;power&#x27;</span>].plot.hist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们对其取log 再做归一化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">归一化的理由:</span></span><br><span class="line"><span class="string">1.数据存在不同的评价指标,其量纲或量纲单位不同,处于不同的数量级</span></span><br><span class="line"><span class="string">    解决特征指标之间的可比性,经过归一化处理后,各指标处于同一数量级,便于综合对比</span></span><br><span class="line"><span class="string">2.求最优解的过程会变得平缓,更容易正确收敛-即能提高梯度下降求最优解时的速度</span></span><br><span class="line"><span class="string">3.提高计算精度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scaling-将变化幅度较大的特征化到[-1,1]之内</span></span><br><span class="line"><span class="string">minmax_scale - 将每个特征放缩到给定范围内(默认范围0-1)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># min_max_scaler = preprocessing.MinMaxScaler()  # 创建该方法的实例</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">为什么要取对数 - 数据集中有负数就不能取对数了 - 实践中,取对数的一般是水平量,而不是比例数据</span></span><br><span class="line"><span class="string">1.缩小数据的绝对数值,方便计算</span></span><br><span class="line"><span class="string">2.取对数后,可以将乘法计算转换称加法计算</span></span><br><span class="line"><span class="string">3.对数值小的部分差异的敏感程度比数值大的部分的差异敏感程度更高</span></span><br><span class="line"><span class="string">4.取对数之后不会改变数据的性质和相关关系,但压缩了变量的尺度</span></span><br><span class="line"><span class="string">5.所得到的数据易消除异方差问题</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>] = np.log(data[<span class="string">&#x27;power&#x27;</span>] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x = (x - xmin) / (xmax - xmin)    --&gt; 将X归一化为[0,1]</span></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>] = ((data[<span class="string">&#x27;power&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;power&#x27;</span>])) / (np.<span class="built_in">max</span>(data[<span class="string">&#x27;power&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;power&#x27;</span>])))</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>].plot.hist()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200327161510556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="这里其实适合长尾分布截断"><br>这里其实适合长尾分布截断</p><h3 id="我们对其取log-再做归一化"><a href="#我们对其取log-再做归一化" class="headerlink" title="# 我们对其取log 再做归一化"></a># 我们对其取log 再做归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">归一化的理由:</span></span><br><span class="line"><span class="string">1.数据存在不同的评价指标,其量纲或量纲单位不同,处于不同的数量级</span></span><br><span class="line"><span class="string">    解决特征指标之间的可比性,经过归一化处理后,各指标处于同一数量级,便于综合对比</span></span><br><span class="line"><span class="string">2.求最优解的过程会变得平缓,更容易正确收敛-即能提高梯度下降求最优解时的速度</span></span><br><span class="line"><span class="string">3.提高计算精度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scaling-将变化幅度较大的特征化到[-1,1]之内</span></span><br><span class="line"><span class="string">minmax_scale - 将每个特征放缩到给定范围内(默认范围0-1)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># min_max_scaler = preprocessing.MinMaxScaler()  # 创建该方法的实例</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">为什么要取对数 - 数据集中有负数就不能取对数了 - 实践中,取对数的一般是水平量,而不是比例数据</span></span><br><span class="line"><span class="string">1.缩小数据的绝对数值,方便计算</span></span><br><span class="line"><span class="string">2.取对数后,可以将乘法计算转换称加法计算</span></span><br><span class="line"><span class="string">3.对数值小的部分差异的敏感程度比数值大的部分的差异敏感程度更高</span></span><br><span class="line"><span class="string">4.取对数之后不会改变数据的性质和相关关系,但压缩了变量的尺度</span></span><br><span class="line"><span class="string">5.所得到的数据易消除异方差问题</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>] = np.log(data[<span class="string">&#x27;power&#x27;</span>] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x = (x - xmin) / (xmax - xmin)    --&gt; 将X归一化为[0,1]</span></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>] = ((data[<span class="string">&#x27;power&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;power&#x27;</span>])) / (np.<span class="built_in">max</span>(data[<span class="string">&#x27;power&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;power&#x27;</span>])))</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;power&#x27;</span>].plot.hist()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200327161756501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="看一下特征’kilometer’-汽车行驶公里数"><a href="#看一下特征’kilometer’-汽车行驶公里数" class="headerlink" title="# 看一下特征’kilometer’- 汽车行驶公里数"></a># 看一下特征’kilometer’- 汽车行驶公里数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;kilometer&#x27;</span>].plot.hist()       <span class="comment"># 没有负的公里数,而且符合二手车大多数公里数都比较高的现象,应该已经做过了分桶</span></span><br><span class="line"><span class="comment"># 直接做归一化</span></span><br><span class="line">data[<span class="string">&#x27;kilometer&#x27;</span>] = ((data[<span class="string">&#x27;kilometer&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;kilometer&#x27;</span>]))</span><br><span class="line">                     / (np.<span class="built_in">max</span>(data[<span class="string">&#x27;kilometer&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;kilometer&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;kilometer&#x27;</span>].plot.hist()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200327161908693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="将构造的关于brand特征的统计量特征做变换"><a href="#将构造的关于brand特征的统计量特征做变换" class="headerlink" title="# 将构造的关于brand特征的统计量特征做变换"></a># 将构造的关于brand特征的统计量特征做变换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_min</span>(<span class="params">x</span>):     <span class="comment"># 这个函数实现的就是下面的公式</span></span><br><span class="line">    <span class="keyword">return</span> (x - np.<span class="built_in">min</span>(x)) / (np.<span class="built_in">max</span>(x) - np.<span class="built_in">min</span>(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;brand_amount&#x27;</span>] = ((data[<span class="string">&#x27;brand_amount&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_amount&#x27;</span>])) /</span><br><span class="line">                        (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_amount&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_amount&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;brand_price_average&#x27;</span>] = ((data[<span class="string">&#x27;brand_price_average&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_average&#x27;</span>])) /</span><br><span class="line">                               (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_price_average&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_average&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;brand_price_max&#x27;</span>] = ((data[<span class="string">&#x27;brand_price_max&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_max&#x27;</span>])) /</span><br><span class="line">                           (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_price_max&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_max&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;brand_price_median&#x27;</span>] = ((data[<span class="string">&#x27;brand_price_median&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_median&#x27;</span>])) /</span><br><span class="line">                              (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_price_median&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_median&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;brand_price_min&#x27;</span>] = ((data[<span class="string">&#x27;brand_price_min&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_min&#x27;</span>])) /</span><br><span class="line">                           (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_price_min&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_min&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;brand_price_std&#x27;</span>] = ((data[<span class="string">&#x27;brand_price_std&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_std&#x27;</span>])) /</span><br><span class="line">                           (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_price_std&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_std&#x27;</span>])))</span><br><span class="line">data[<span class="string">&#x27;brand_price_sum&#x27;</span>] = ((data[<span class="string">&#x27;brand_price_sum&#x27;</span>] - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_sum&#x27;</span>])) /</span><br><span class="line">                           (np.<span class="built_in">max</span>(data[<span class="string">&#x27;brand_price_sum&#x27;</span>]) - np.<span class="built_in">min</span>(data[<span class="string">&#x27;brand_price_sum&#x27;</span>])))</span><br></pre></td></tr></table></figure><h3 id="对所有类别特征进行oneEncoder编码"><a href="#对所有类别特征进行oneEncoder编码" class="headerlink" title="# 对所有类别特征进行oneEncoder编码"></a># 对所有类别特征进行oneEncoder编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">pandas --&gt; get_dummies()方法: 对数据进行one-hot编码(因子化)</span></span><br><span class="line"><span class="string">逻辑回归建模时,需要输入的特征都是数值型特征,我们通常会先对类目型的特征因子化;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">以male(男性)为例:原本一个属性维度,因为其取值可以是yes或no,而将其平展开为’male_yes’,&#x27;male_no’两个属性;  -是不是男性</span></span><br><span class="line"><span class="string">1.原本male取值为yes的，在此处的&quot;male_yes&quot;下取值为1，在&quot;male_no&quot;下取值为0;</span></span><br><span class="line"><span class="string">2.原本male取值为no的，在此处的&quot;male_yes&quot;下取值为0，在&quot;male_no&quot;下取值为1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">data = pd.get_dummies(data, columns=[<span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;brand&#x27;</span>, <span class="string">&#x27;bodyType&#x27;</span>, <span class="string">&#x27;fuelType&#x27;</span>,</span><br><span class="line">                                     <span class="string">&#x27;gearbox&#x27;</span>, <span class="string">&#x27;notRepairedDamage&#x27;</span>, <span class="string">&#x27;power_bin&#x27;</span>])</span><br><span class="line"><span class="comment"># 可以发现,原来进行onehot编码的特征被覆盖了</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">也可以单独转化特征,通过x_new = pd.get_dummies(data[x], prefix=&#x27;x&#x27;) --&gt; prefix - 转化后列名的前缀,这样子原来的特征不会被覆盖</span></span><br><span class="line"><span class="string">然后通过pd.concat()连接转换后的特征</span></span><br><span class="line"><span class="string">原先的特征想删除也可以用drop方法</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br></pre></td></tr></table></figure><p>这份数据给 LR 用 - 逻辑回归模型,因为逻辑回归建模时,需要输入的特征都是数值型特征(其实这里的city特征有问题,后面会说)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.to_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\ML_Used_car\data\data_for_lr.csv&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="4-特征筛选"><a href="#4-特征筛选" class="headerlink" title="4.特征筛选"></a>4.特征筛选</h2><h3 id="过滤式"><a href="#过滤式" class="headerlink" title="#过滤式"></a>#过滤式</h3><h5 id="相关性分析-我们挑选的都是数值型的特征"><a href="#相关性分析-我们挑选的都是数值型的特征" class="headerlink" title="#相关性分析 - 我们挑选的都是数值型的特征"></a>#相关性分析 - 我们挑选的都是数值型的特征</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DataFrame.corr(method=&#x27;pearson&#x27;, min_periods=1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">method - 分析方法: pearson, kendall, spearman</span></span><br><span class="line"><span class="string">           pearson -  衡量两个数据集合是否在一条线上面 - 即针对线性数据的相关系数计算</span></span><br><span class="line"><span class="string">           kendall - 反映分类变量相关性的指标 - 即针对无序序列的相关系数,非正态分布的数据</span></span><br><span class="line"><span class="string">           spearman - 非线性的,非正态分布的数据的相关系数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;power&#x27;</span>].corr(data[<span class="string">&#x27;price&#x27;</span>], method=<span class="string">&#x27;spearman&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;kilometer&#x27;</span>].corr(data[<span class="string">&#x27;price&#x27;</span>], method=<span class="string">&#x27;spearman&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;brand_amount&#x27;</span>].corr(data[<span class="string">&#x27;price&#x27;</span>], method=<span class="string">&#x27;spearman&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;brand_price_average&#x27;</span>].corr(data[<span class="string">&#x27;price&#x27;</span>], method=<span class="string">&#x27;spearman&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;brand_price_max&#x27;</span>].corr(data[<span class="string">&#x27;price&#x27;</span>], method=<span class="string">&#x27;spearman&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;brand_price_median&#x27;</span>].corr(data[<span class="string">&#x27;price&#x27;</span>], method=<span class="string">&#x27;spearman&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="也可以直接看图-下面是看两两特征之间的联系"><a href="#也可以直接看图-下面是看两两特征之间的联系" class="headerlink" title="# 也可以直接看图 - 下面是看两两特征之间的联系"></a># 也可以直接看图 - 下面是看两两特征之间的联系</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_numeric = data[[<span class="string">&#x27;power&#x27;</span>, <span class="string">&#x27;kilometer&#x27;</span>, <span class="string">&#x27;brand_amount&#x27;</span>,</span><br><span class="line">                     <span class="string">&#x27;brand_price_average&#x27;</span>, <span class="string">&#x27;brand_price_max&#x27;</span>, <span class="string">&#x27;brand_price_median&#x27;</span>]]</span><br><span class="line">correlation = data_numeric.corr()</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Correlation of Numeric Features with Price&#x27;</span>, y=<span class="number">1</span>, size=<span class="number">16</span>)</span><br><span class="line">sns.heatmap(correlation, square=<span class="literal">True</span>, vmax=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200327162330732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>没啥用,就是好看</p><h3 id="使用顺序向前选择SFS挑选最优特征"><a href="#使用顺序向前选择SFS挑选最优特征" class="headerlink" title="# 使用顺序向前选择SFS挑选最优特征"></a># 使用顺序向前选择SFS挑选最优特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.feature_selection <span class="keyword">import</span> SequentialFeatureSelector <span class="keyword">as</span> SFS</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">SequentialFeatureSelector是顺序特征选择算法,是一种贪心算法</span></span><br><span class="line"><span class="string">SFA会根据分类器的性能同时删除或添加一个特征，直到达到所需大小k的特征子集为止</span></span><br><span class="line"><span class="string">可通过以下四种方式获得SFA</span></span><br><span class="line"><span class="string">顺序前向选择(SFS) #这里就是,所以forward参数选择了True</span></span><br><span class="line"><span class="string">顺序向后选择（SBS）</span></span><br><span class="line"><span class="string">顺序正向浮动选择（SFFS）</span></span><br><span class="line"><span class="string">顺序向后浮动选择（SBFS）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sfs = SequentialFeatureSelector(model,k_features=feature_n,forward=True,verbose=2,scoring=&#x27;accuracy&#x27;,cv=cv)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">model - 导入要对特征求职的算法</span></span><br><span class="line"><span class="string">k_features - 要挑选的特征数</span></span><br><span class="line"><span class="string">forward=True - 顺序向前,如果是False就是向后</span></span><br><span class="line"><span class="string">floating - 是否浮动搜索</span></span><br><span class="line"><span class="string">            浮动算法具有附加的排除或包含步骤,可在特征被包含（或排除）后删除特征,以便可以对大量特征子集组合进行采样。</span></span><br><span class="line"><span class="string">            此步骤是有条件的,仅当在移除（或添加）特定特征后,通过标准函数将结果特征子集评估为“更好”时，才会发生此步骤</span></span><br><span class="line"><span class="string">cv - 是否进行交叉验证</span></span><br><span class="line"><span class="string">scoring - str,可调用或无（默认值：无）</span></span><br><span class="line"><span class="string">        如果为None（默认），则对sklearn分类器使用“准确性”，对于sklearn回归变量使用“ r2”</span></span><br><span class="line"><span class="string">        如果为str，则使用sklearn评分指标字符串标识符:如accuracy，f1，precision，recall，roc_auc</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">参考网址: https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="将训练集和测试集剥离-这里得用训练集的数据-因为测试集没有price标签"><a href="#将训练集和测试集剥离-这里得用训练集的数据-因为测试集没有price标签" class="headerlink" title="# 将训练集和测试集剥离,这里得用训练集的数据,因为测试集没有price标签"></a># 将训练集和测试集剥离,这里得用训练集的数据,因为测试集没有price标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sfs = SFS(LinearRegression(), k_features=<span class="number">10</span>, forward=<span class="literal">True</span>, floating=<span class="literal">False</span>, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_data_new = data[data[<span class="string">&#x27;train&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line">test_data_new = data[data[<span class="string">&#x27;train&#x27;</span>] == <span class="number">0</span>]</span><br><span class="line">x = train_data_new.drop([<span class="string">&#x27;price&#x27;</span>], axis=<span class="number">1</span>)    <span class="comment"># 去掉标签,只保留特征</span></span><br><span class="line">x = x.fillna(<span class="number">0</span>)</span><br><span class="line">y = train_data_new[<span class="string">&#x27;price&#x27;</span>]</span><br></pre></td></tr></table></figure><p>这时候,如果训练模型就会报错!!!!!!说无法将字符串转化成浮点数,说明数据里有字符串类型的数据!!!<br>我们看一下是哪里的数据出了问题?!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们把训练特征转化成DataFrame类型</span></span><br><span class="line">df_x = pd.DataFrame(x)</span><br><span class="line"><span class="comment"># 找到df_x中的object类型的特征</span></span><br><span class="line">df_x_obj = df_x.columns[df_x.dtypes == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(df_x_obj)     <span class="comment"># 发现是city列,在提取邮编的时候数据类型变成了object</span></span><br><span class="line"><span class="comment"># 我们将city列做一下处理</span></span><br><span class="line">data[<span class="string">&#x27;city&#x27;</span>] = pd.to_numeric(data.city, errors=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line"><span class="comment"># 我们再看一下city的数据类型</span></span><br><span class="line">data[<span class="string">&#x27;city&#x27;</span>]</span><br><span class="line"><span class="comment"># 处理之后我们再重复上面的特征顺序选择算法</span></span><br><span class="line"></span><br><span class="line">train_data_new = data[data[<span class="string">&#x27;train&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line">test_data_new = data[data[<span class="string">&#x27;train&#x27;</span>] == <span class="number">0</span>]</span><br><span class="line">x = train_data_new.drop([<span class="string">&#x27;price&#x27;</span>], axis=<span class="number">1</span>)    <span class="comment"># 去掉标签,只保留特征</span></span><br><span class="line">x = x.fillna(<span class="number">0</span>)</span><br><span class="line">y = train_data_new[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">sfs.fit(x, y)   <span class="comment"># 不报错了</span></span><br><span class="line"></span><br><span class="line">sfs.k_feature_names_    <span class="comment"># 我们输出挑选出来的特征,结果和教材上的好像不大一样</span></span><br></pre></td></tr></table></figure><p>Out[98]:<br>(‘kilometer’,<br> ‘v_0’,<br> ‘v_3’,<br> ‘v_7’,<br> ‘used_Time’,<br> ‘brand_price_std’,<br> ‘brand_price_average’,<br> ‘model_167.0’,<br> ‘brand_24’,<br> ‘gearbox_1.0’)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们也试着画一画,看下边际效益</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_sequential_feature_selection <span class="keyword">as</span> plot_sfs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">figl = plot_sfs(sfs.get_metric_dict(), kind=<span class="string">&#x27;std_dev&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200327163244461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们可以发现随着特征的增大,表现增长幅度大,随后呈现平稳缓慢增长的趋势;</p><p>—–等待Task4</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;特征工程-代码示例部分&quot;&gt;&lt;a href=&quot;#特征工程-代码示例部分&quot; class=&quot;headerlink&quot; title=&quot;特征工程_代码示例部分&quot;&gt;&lt;/a&gt;特征工程_代码示例部分&lt;/h1&gt;&lt;h2 id=&quot;1-导入数据&quot;&gt;&lt;a href=&quot;#1-导入数据&quot; cla</summary>
      
    
    
    
    <category term="二手车交易价格预测" scheme="http://example.com/categories/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>二手车交易价格预测_Task2_EDA - 数据探索性分析</title>
    <link href="http://example.com/2023/04/18/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task2_EDA%20-%20%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2023/04/18/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B_Task2_EDA%20-%20%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/</id>
    <published>2023-04-18T09:07:21.476Z</published>
    <updated>2023-04-24T08:17:21.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EDA-数据探索性分析-代码示例部分"><a href="#EDA-数据探索性分析-代码示例部分" class="headerlink" title="EDA - 数据探索性分析_代码示例部分"></a>EDA - 数据探索性分析_代码示例部分</h1><h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><h3 id="1-载入各种数据科学以及可视化库"><a href="#1-载入各种数据科学以及可视化库" class="headerlink" title="1.载入各种数据科学以及可视化库"></a>1.载入各种数据科学以及可视化库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings     <span class="comment"># 利用过滤器实现忽略警告语句</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> st</span><br><span class="line"><span class="keyword">import</span> pandas_profiling</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno    <span class="comment"># 缺失值可视化的库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-载入数据"><a href="#2-载入数据" class="headerlink" title="2.载入数据"></a>2.载入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Train_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\used_car\data\used_car_train_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">Test_data = pd.read_csv(<span class="string">r&#x27;F:\Users\TreeFei\文档\PyC\used_car\data\used_car_testA_20200313.csv&#x27;</span>, sep=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="comment"># #在kaggle上看一下已知特征的解释</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">SaleID - 交易ID,唯一编码</span></span><br><span class="line"><span class="string">name - 汽车交易名称,已脱敏</span></span><br><span class="line"><span class="string">regDate - 汽车注册日期,例如20160101,2016年01月01日</span></span><br><span class="line"><span class="string">model - 车型编码,已脱敏</span></span><br><span class="line"><span class="string">brand - 汽车品牌,已脱敏</span></span><br><span class="line"><span class="string">bodyType - 车身类型-豪华轿车:0; 微型车:1;  厢型车:2;  大巴车:3;  敞篷车:4;  双门汽车:5; 商务车:6;  搅拌车:7</span></span><br><span class="line"><span class="string">fuelType - 燃油类型-汽油:0;   柴油:1;   液化石油气:2;   天然气:3;   混合动力:4;   其他:5;   电动:6</span></span><br><span class="line"><span class="string">gearbox - 变速箱-手动:0;   自动:1</span></span><br><span class="line"><span class="string">power - 发动机功率:范围 [ 0, 600 ]</span></span><br><span class="line"><span class="string">kilometer - 汽车已行驶公里,单位万km</span></span><br><span class="line"><span class="string">notRepairedDamage - 汽车有尚未修复的损坏-是:0; 否:1</span></span><br><span class="line"><span class="string">regionCode - 地区编码,已脱敏</span></span><br><span class="line"><span class="string">seller - 销售方-个体:0;  非个体:1</span></span><br><span class="line"><span class="string">offerType - 报价类型-提供:0;  请求:1</span></span><br><span class="line"><span class="string">creatDate - 汽车上线时间-即开始售卖时间</span></span><br><span class="line"><span class="string">price - 二手车交易价格(预测目标)</span></span><br><span class="line"><span class="string">v系列特征 - 匿名特征:包含v0-14在内15个匿名特征(根据汽车的评论,标签等大量信息得到的embedding向量)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="3-简略观察数据"><a href="#3-简略观察数据" class="headerlink" title="3.简略观察数据"></a>3.简略观察数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Train_data.head().append(Train_data.tail())     <span class="comment"># 显示开头5行和最后5行</span></span><br><span class="line">Train_data.shape</span><br><span class="line"></span><br><span class="line">Test_data.head().append(Test_data.tail())</span><br><span class="line">Test_data.shape</span><br></pre></td></tr></table></figure><h3 id="4-数据总览概况"><a href="#4-数据总览概况" class="headerlink" title="4.数据总览概况"></a>4.数据总览概况</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">describe - 包括每列的统计量,个数count,平均值mean,方差std,最小值min,中位数25% 50% 75%及最大值</span></span><br><span class="line"><span class="string">            看这个信息可以瞬间掌握数据的大概范围以及每个值的异常值得判断</span></span><br><span class="line"><span class="string">            有时会发现999, 9999, -1等值,这些其实可能都是nan得另一种表达方式</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">info - 了解每列数据得type,有助于了解是否存在除了nan以外得特殊符号异常</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">Train_data.describe()</span><br><span class="line">Test_data.describe()</span><br><span class="line"></span><br><span class="line">Train_data.info()   <span class="comment"># 发现model, bodyType, fuelType-缺失的最多, gearbox 这四个特征有缺失值</span></span><br><span class="line">Test_data.info()    <span class="comment"># 发现bodyType, fuelType, gearbox 这三个特征有缺失值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们看下每列存在nan的情况</span></span><br><span class="line">Train_data.isnull().<span class="built_in">sum</span>()</span><br><span class="line">Test_data.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><h3 id="5-我们将训练集的缺失数据可视化"><a href="#5-我们将训练集的缺失数据可视化" class="headerlink" title="5.我们将训练集的缺失数据可视化"></a>5.我们将训练集的缺失数据可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">missing = Train_data.isnull().<span class="built_in">sum</span>()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]  <span class="comment"># 将有缺失值的列选出</span></span><br><span class="line">missing.sort_values(inplace=<span class="literal">True</span>)   <span class="comment"># 升序排序</span></span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020032415402229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可视化看下缺省值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">msno.matrix(Train_data.sample(<span class="number">250</span>))     <span class="comment"># sample(250) - 随机抽取250个样本,再以列表的形式返回</span></span><br><span class="line">msno.bar(Train_data.sample(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324154141745.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="6-异常值检测"><a href="#6-异常值检测" class="headerlink" title="6.异常值检测"></a>6.异常值检测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Train_data.info()   <span class="comment"># 发现除了notRepairedDamage是object类型,其余特征都是数值型数据</span></span><br><span class="line"><span class="comment"># 我们看一下notRepairedDamage里面有什么</span></span><br><span class="line">Train_data[<span class="string">&#x27;notRepairedDamage&#x27;</span>].value_counts()      <span class="comment"># 发现了有&#x27;-&#x27;的数据,明显是缺失值,我们将它统一替换成nan</span></span><br><span class="line">Train_data[<span class="string">&#x27;notRepairedDamage&#x27;</span>].replace(<span class="string">&#x27;-&#x27;</span>, np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line">Train_data[<span class="string">&#x27;notRepairedDamage&#x27;</span>].value_counts()      <span class="comment"># 现在都是数值型数据了</span></span><br></pre></td></tr></table></figure><p>重新看一下缺失值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Train_data.isnull().<span class="built_in">sum</span>()      <span class="comment"># 可以发现notRepairedDamage缺失的数据最多,有24324个</span></span><br></pre></td></tr></table></figure><p>把测试集中notRepairedDamage的’-‘值也做一下处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Test_data[<span class="string">&#x27;notRepairedDamage&#x27;</span>].replace(<span class="string">&#x27;-&#x27;</span>, np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line">Test_data[<span class="string">&#x27;notRepairedDamage&#x27;</span>].value_counts()</span><br><span class="line">Test_data.isnull().<span class="built_in">sum</span>()        <span class="comment"># 发现缺失值最多的也是notRepairedDamage</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过这种方法,我们可以发现&#x27;seller-销售方&#x27; 和&#x27;offerType-报价类型&#x27; 这两个特征严重倾斜,一般不会对预测有帮助,可以删掉</span></span><br><span class="line">Train_data[<span class="string">&#x27;seller&#x27;</span>].value_counts()</span><br><span class="line">Train_data[<span class="string">&#x27;offerType&#x27;</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> Train_data[<span class="string">&#x27;seller&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> Train_data[<span class="string">&#x27;offerType&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> Test_data[<span class="string">&#x27;seller&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> Test_data[<span class="string">&#x27;offerType&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="7-了解预测值的分布"><a href="#7-了解预测值的分布" class="headerlink" title="7.了解预测值的分布"></a>7.了解预测值的分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Train_data[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">Train_data[<span class="string">&#x27;price&#x27;</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一下price和哪一种分布最贴近</span></span><br><span class="line">y = Train_data[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Johnson SU&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">st.johnsonsu - 约翰逊分布,是一种经过约翰变换后服从正态分布概率的随机变量的概率分布</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.johnsonsu)    <span class="comment"># kde - bool型变量,控制是否绘制核密度估计曲线,默认为True</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Normal&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">st.norm - 正态分布</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.norm)</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Log Normal&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">st.lognorm - 对数正态分布,从短期来看,与正态分布非常接近;但长期来看,对数正态分布向上分布的数值更多一些</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.lognorm)</span><br></pre></td></tr></table></figure><p>约翰逊分布拟合效果:<br><img src="https://img-blog.csdnimg.cn/20200324154515241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>正态分布拟合效果:<br><img src="https://img-blog.csdnimg.cn/20200324154600444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>对数正态分布拟合效果:<br><img src="https://img-blog.csdnimg.cn/20200324154655402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="8-查看skewness和kurtosis"><a href="#8-查看skewness和kurtosis" class="headerlink" title="8.查看skewness和kurtosis"></a>8.查看skewness和kurtosis</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">skewness - 偏度,也称为偏态/偏态系数,是统计数据分布偏斜方向和程度的度量,是统计数据分布非对称程度的数字特征</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">kurtosis - 峰度,与偏度类似,是描述总体中所有取值分布形态陡缓程度的统计量</span></span><br><span class="line"><span class="string">            这个统计量需要与正态分布相比较:</span></span><br><span class="line"><span class="string">            1.峰度为0表示该总体数据分布与正态分布的陡缓程度相同</span></span><br><span class="line"><span class="string">            2.峰度大于0表示该总体数据分布与正态分布相比较为陡峭,为尖顶峰</span></span><br><span class="line"><span class="string">            3.峰度小于0表示该总体数据分布与正态分布相比较为平坦,为平顶峰</span></span><br><span class="line"><span class="string">            4.峰度的绝对值数值越大表示其分布形态的陡缓程度与正态分布的差异程度越大</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.distplot(Train_data[<span class="string">&#x27;price&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Skewness: %f&#x27;</span> % Train_data[<span class="string">&#x27;price&#x27;</span>].skew())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Kurtosis: %f&#x27;</span> % Train_data[<span class="string">&#x27;price&#x27;</span>].kurt())</span><br><span class="line"><span class="comment"># 看一下整个训练集的偏度和峰度</span></span><br><span class="line">Train_data.skew()</span><br><span class="line">Train_data.kurt()</span><br></pre></td></tr></table></figure><p>可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(Train_data.skew(), color=<span class="string">&#x27;blue&#x27;</span>, axlabel=<span class="string">&#x27;Skewness&#x27;</span>)</span><br><span class="line">sns.distplot(Train_data.kurt(), color=<span class="string">&#x27;orange&#x27;</span>, axlabel=<span class="string">&#x27;Kurtness&#x27;</span>)</span><br></pre></td></tr></table></figure><p>偏度:<br><img src="https://img-blog.csdnimg.cn/20200324154857313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>峰度:<br><img src="https://img-blog.csdnimg.cn/20200324154933259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="9-查看预测值的频数"><a href="#9-查看预测值的频数" class="headerlink" title="9.查看预测值的频数"></a>9.查看预测值的频数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(Train_data[<span class="string">&#x27;price&#x27;</span>], orientation=<span class="string">&#x27;vertical&#x27;</span>, histtype=<span class="string">&#x27;bar&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">参数解释</span></span><br><span class="line"><span class="string">orientation - 直方图方向</span></span><br><span class="line"><span class="string">                horizontal - 水平方向</span></span><br><span class="line"><span class="string">                vertical - 垂直方向</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 发现价格大于20000的样本极少(其实可以直接填充或者删掉,当作异常值处理)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 试着log变换,看一下价格分布</span></span><br><span class="line">plt.hist(np.log(Train_data[<span class="string">&#x27;price&#x27;</span>]), orientation=<span class="string">&#x27;vertical&#x27;</span>, histtype=<span class="string">&#x27;bar&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 发现分布显示变均匀了</span></span><br></pre></td></tr></table></figure><p>log变换之前:<br><img src="https://img-blog.csdnimg.cn/20200324155040891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>log变换之后<br><img src="https://img-blog.csdnimg.cn/20200324155108941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="10-将特征分为类别特征和数字特征-并对类别特征查看unique分布"><a href="#10-将特征分为类别特征和数字特征-并对类别特征查看unique分布" class="headerlink" title="10.将特征分为类别特征和数字特征,并对类别特征查看unique分布"></a>10.将特征分为类别特征和数字特征,并对类别特征查看unique分布</h3><p>本案例需要人为根据实际含义区分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">numeric_features = Train_data.select_dtypes(include=[np.number])</span></span><br><span class="line"><span class="string">numeric_features.columns</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">numeric_features = [<span class="string">&#x27;power&#x27;</span>, <span class="string">&#x27;kilometer&#x27;</span>, <span class="string">&#x27;v_0&#x27;</span>, <span class="string">&#x27;v_1&#x27;</span>, <span class="string">&#x27;v_2&#x27;</span>, <span class="string">&#x27;v_3&#x27;</span>, <span class="string">&#x27;v_4&#x27;</span>, <span class="string">&#x27;v_5&#x27;</span>, <span class="string">&#x27;v_6&#x27;</span>, <span class="string">&#x27;v_7&#x27;</span>, <span class="string">&#x27;v_8&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;v_9&#x27;</span>, <span class="string">&#x27;v_10&#x27;</span>, <span class="string">&#x27;v_11&#x27;</span>, <span class="string">&#x27;v_12&#x27;</span>, <span class="string">&#x27;v_13&#x27;</span>, <span class="string">&#x27;v_14&#x27;</span>]</span><br><span class="line">categorical_features = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;brand&#x27;</span>, <span class="string">&#x27;bodyType&#x27;</span>, <span class="string">&#x27;fuelType&#x27;</span>,</span><br><span class="line">                        <span class="string">&#x27;gearbox&#x27;</span>, <span class="string">&#x27;notRepairedDamage&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>]</span><br><span class="line"><span class="comment"># 在这里,两个日期没有分类</span></span><br></pre></td></tr></table></figure><h3 id="11-看一下训练集类型特征的特征分布"><a href="#11-看一下训练集类型特征的特征分布" class="headerlink" title="11.看一下训练集类型特征的特征分布"></a>11.看一下训练集类型特征的特征分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> cat_fea <span class="keyword">in</span> categorical_features:</span><br><span class="line">    <span class="built_in">print</span>(cat_fea + <span class="string">&#x27;的特征分布如下:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125;特征有&#123;&#125;个不同值&#x27;</span>.<span class="built_in">format</span>(cat_fea, Train_data[cat_fea].nunique()))</span><br><span class="line">    <span class="built_in">print</span>(Train_data[cat_fea].value_counts())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一下测试集类型特征的特征分布</span></span><br><span class="line"><span class="keyword">for</span> cat_fea <span class="keyword">in</span> categorical_features:</span><br><span class="line">    <span class="built_in">print</span>(cat_fea + <span class="string">&#x27;的特征分布如下:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125;特征有&#123;&#125;个不同值&#x27;</span>.<span class="built_in">format</span>(cat_fea, Test_data[cat_fea].nunique()))</span><br><span class="line">    <span class="built_in">print</span>(Test_data[cat_fea].value_counts())</span><br></pre></td></tr></table></figure><h3 id="12-数字特征分析"><a href="#12-数字特征分析" class="headerlink" title="12.数字特征分析"></a>12.数字特征分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #把price加入数字特征中</span></span><br><span class="line">numeric_features.append(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">numeric_features</span><br><span class="line"></span><br><span class="line"><span class="comment"># #相关性分析</span></span><br><span class="line">price_numeric = Train_data[numeric_features]    <span class="comment"># 把训练集中样本的价格和数字型特征提取出来</span></span><br><span class="line">correlation = price_numeric.corr()      <span class="comment"># corr() - 给出任意两个变量之间的相关系数</span></span><br><span class="line"><span class="built_in">print</span>(correlation[<span class="string">&#x27;price&#x27;</span>].sort_values(ascending=<span class="literal">False</span>), <span class="string">&#x27;\n&#x27;</span>)      <span class="comment"># 把其他变量和价格的相关系数降序排列</span></span><br><span class="line"><span class="comment"># #将这个结果可视化</span></span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Correlation of Numeric Feature with Price&#x27;</span>, y=<span class="number">1</span>, size=<span class="number">16</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sns.heatmap - 热力图</span></span><br><span class="line"><span class="string">            square - 正方形</span></span><br><span class="line"><span class="string">            vmax - 热力图颜色取值的最大值,默认会从data中推导</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sns.heatmap(correlation, square=<span class="literal">True</span>, vmax=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324155406320.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对数字特征数据集进行偏度和峰度分析</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> numeric_features:</span><br><span class="line">    <span class="built_in">print</span>(col)</span><br><span class="line">    <span class="built_in">print</span>(Train_data[col].skew())</span><br><span class="line">    <span class="built_in">print</span>(Train_data[col].kurt())</span><br></pre></td></tr></table></figure><p>每个数字特征的分布可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个数字特征的分布可视化</span></span><br><span class="line">f = pd.melt(Train_data, value_vars=numeric_features)    <span class="comment"># melt() - 将列名转化为列数据; value_vars参数 - 需要转换的列名</span></span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">&#x27;variable&#x27;</span>, col_wrap=<span class="number">2</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>)</span><br><span class="line">g = g.<span class="built_in">map</span>(sns.distplot, <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sns.FaceGrid - 构建结构化多绘图网格</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">data - 处理后的dataframe数据,其中每一列都是一个变量（特征),每一行都是一个样本</span></span><br><span class="line"><span class="string">row/col/hue - 定义数据子集的变量,这些变量将在网格的不同方面绘制</span></span><br><span class="line"><span class="string">col_wrap - 图网格列维度限制</span></span><br><span class="line"><span class="string">share&#123;x,y&#125; - 是否共享x轴或者y轴</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以看出匿名特征相对分布均匀</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324155611622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="13-数字特征相互之间的关系可视化"><a href="#13-数字特征相互之间的关系可视化" class="headerlink" title="13.数字特征相互之间的关系可视化"></a>13.数字特征相互之间的关系可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sns.<span class="built_in">set</span>()</span><br><span class="line">columns = [<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;v_12&#x27;</span>, <span class="string">&#x27;v_8&#x27;</span>, <span class="string">&#x27;v_0&#x27;</span>, <span class="string">&#x27;power&#x27;</span>, <span class="string">&#x27;v_5&#x27;</span>, <span class="string">&#x27;v_2&#x27;</span>, <span class="string">&#x27;v_6&#x27;</span>, <span class="string">&#x27;v_1&#x27;</span>, <span class="string">&#x27;v_14&#x27;</span>]</span><br><span class="line">sns.pairplot(Train_data[columns], size=<span class="number">2</span>, kind=<span class="string">&#x27;scatter&#x27;</span>, diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sns.pairplot - 绘制多变量网格图</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">size - 图的尺寸大小,默认6</span></span><br><span class="line"><span class="string">kind - 绘图样式    kind=&#x27;scatter&#x27; --&gt; 散点图</span></span><br><span class="line"><span class="string">diag_kind - 对角线子块的绘图方式      diag_kind=&#x27;kde&#x27; --&gt; 显示核密度分布</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324160155922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="14-多变量互相关系回归可视化"><a href="#14-多变量互相关系回归可视化" class="headerlink" title="14.多变量互相关系回归可视化"></a>14.多变量互相关系回归可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">此处设定画布规格 - 5行2列</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(nrows=<span class="number">5</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">24</span>, <span class="number">20</span>))</span><br><span class="line"><span class="comment"># [&#x27;v_12&#x27;, &#x27;v_8&#x27; , &#x27;v_0&#x27;, &#x27;power&#x27;, &#x27;v_5&#x27;, &#x27;v_2&#x27;, &#x27;v_6&#x27;, &#x27;v_1&#x27;, &#x27;v_14&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sns.regplot - 用于线性关系可视化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数详解:</span></span><br><span class="line"><span class="string">x/y - 横纵坐标</span></span><br><span class="line"><span class="string">data - 数据集</span></span><br><span class="line"><span class="string">fig_reg - 回归线 --&gt; True:显示    False:不显示</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">v_12_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_12&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_12&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_12_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax1)</span><br><span class="line"></span><br><span class="line">v_8_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_8&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_8&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_8_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax2)</span><br><span class="line">v_0_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_0&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_0&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_0_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax3)</span><br><span class="line">power_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;power&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;power&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=power_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax4)</span><br><span class="line">v_5_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_5&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_5&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_5_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax5)</span><br><span class="line">v_2_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_2&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_2&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_2_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax6)</span><br><span class="line">v_6_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_6&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_6&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_6_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax7)</span><br><span class="line">v_1_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_1&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_1&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_1_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax8)</span><br><span class="line">v_14_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_14&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_14&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_14_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax9)</span><br><span class="line">v_13_scatter_plot = pd.concat([Y_train, Train_data[<span class="string">&#x27;v_13&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">&#x27;v_13&#x27;</span>, y=<span class="string">&#x27;price&#x27;</span>, data=v_13_scatter_plot, scatter=<span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax10)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324160741429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="15-类别特征分析"><a href="#15-类别特征分析" class="headerlink" title="15.类别特征分析"></a>15.类别特征分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #unique分布</span></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> categorical_features:</span><br><span class="line">    <span class="built_in">print</span>(Train_data[fea].nunique())</span><br><span class="line"></span><br><span class="line"><span class="comment"># #类别特征可视化 - 由于name和regionCode特征过于稀疏，所以选取其他类别特征进行可视化</span></span><br><span class="line">categorical_features_box = [<span class="string">&#x27;model&#x27;</span>, <span class="string">&#x27;brand&#x27;</span>, <span class="string">&#x27;bodyType&#x27;</span>, <span class="string">&#x27;fuelType&#x27;</span>, <span class="string">&#x27;gearbox&#x27;</span>, <span class="string">&#x27;notRepairedDamage&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> categorical_features_box:</span><br><span class="line">    Train_data[c] = Train_data[c].astype(<span class="string">&#x27;category&#x27;</span>)    <span class="comment"># 转化成类型数据的格式</span></span><br><span class="line">    <span class="comment"># any() - 判断给定的可迭代参数iterable:如果全部为 False,则返回 False;如果有一个为 True,则返回 True</span></span><br><span class="line">    <span class="keyword">if</span> Train_data[c].isnull().<span class="built_in">any</span>():</span><br><span class="line">        Train_data[c] = Train_data[c].cat.add_categories([<span class="string">&#x27;MISSING&#x27;</span>])   <span class="comment"># 如果有缺失值,那么添加新类别&#x27;MISSING&#x27;</span></span><br><span class="line">        Train_data[c] = Train_data[c].fillna(<span class="string">&#x27;MISSING&#x27;</span>)     <span class="comment"># 缺失值填充为MISSING</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># #定义箱型图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">boxplot</span>(<span class="params">x, y, **kwargs</span>):</span><br><span class="line">    sns.boxplot(x=x, y=y)</span><br><span class="line">    x = plt.xticks(rotation=<span class="number">90</span>)     <span class="comment"># otation - label显示的旋转角度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f = pd.melt(Train_data, id_vars=[<span class="string">&#x27;price&#x27;</span>], value_vars=categorical_features_box)     <span class="comment"># id_vars - 不需要转化的列名</span></span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">&#x27;variable&#x27;</span>, col_wrap=<span class="number">2</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.<span class="built_in">map</span>(boxplot, <span class="string">&#x27;value&#x27;</span>, <span class="string">&#x27;price&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020032416101646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="16-类别特征的小提琴图可视化"><a href="#16-类别特征的小提琴图可视化" class="headerlink" title="16.类别特征的小提琴图可视化"></a>16.类别特征的小提琴图可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">catg_list = categorical_features_box</span><br><span class="line">target = <span class="string">&#x27;price&#x27;</span></span><br><span class="line"><span class="keyword">for</span> catg <span class="keyword">in</span> catg_list:</span><br><span class="line">    sns.violinplot(x=catg, y=target, data=Train_data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324161144416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="17-类别特征的柱形图可视化"><a href="#17-类别特征的柱形图可视化" class="headerlink" title="17.类别特征的柱形图可视化"></a>17.类别特征的柱形图可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bar_plot</span>(<span class="params">x, y, **kwargs</span>):</span><br><span class="line">    sns.barplot(x=x, y=y)</span><br><span class="line">    x = plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f = pd.melt(Train_data, id_vars=[<span class="string">&#x27;price&#x27;</span>], value_vars=categorical_features_box)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">&#x27;variable&#x27;</span>, col_wrap=<span class="number">2</span>, sharey=<span class="literal">False</span>, sharex=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.<span class="built_in">map</span>(bar_plot, <span class="string">&#x27;value&#x27;</span>, <span class="string">&#x27;price&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200324161301579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JhYnkxNjAxdHJlZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;EDA-数据探索性分析-代码示例部分&quot;&gt;&lt;a href=&quot;#EDA-数据探索性分析-代码示例部分&quot; class=&quot;headerlink&quot; title=&quot;EDA - 数据探索性分析_代码示例部分&quot;&gt;&lt;/a&gt;EDA - 数据探索性分析_代码示例部分&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="二手车交易价格预测" scheme="http://example.com/categories/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="金融风控" scheme="http://example.com/tags/%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
</feed>
